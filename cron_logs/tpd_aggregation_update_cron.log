2025-01-08 05:00:01 - Starting stat_type_update job
2025-01-08 05:00:01 - Environment variables loaded from /home/exx/myCode/horse-racing/FoxRiverAIRacing/config/.env
2025-01-08 05:00:01 - Running Python script: /home/exx/myCode/horse-racing/FoxRiverAIRacing/src/data_preprocessing/tpd_aggregation_update.py
25/01/08 05:00:03 WARN Utils: Your hostname, mail.relufox.ai resolves to a loopback address: 127.0.0.1; using 192.168.4.25 instead (on interface eno2)
25/01/08 05:00:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
25/01/08 05:00:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/01/08 05:00:03 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
[Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 1:>                                                          (0 + 1) / 1]                                                                                [Stage 2:>    (0 + 1) / 1][Stage 3:>    (0 + 1) / 1][Stage 4:>    (0 + 1) / 1][Stage 3:>                  (0 + 1) / 1][Stage 4:>                  (0 + 1) / 1][Stage 3:>                                                          (0 + 1) / 1][Stage 8:>                                                      (0 + 128) / 200][Stage 8:>                                                      (2 + 128) / 200][Stage 8:=>                                                     (7 + 128) / 200][Stage 8:=========>                                            (34 + 128) / 200][Stage 8:==================================>                   (128 + 72) / 200][Stage 8:==================================>                   (129 + 71) / 200]                                                                                [Stage 9:>                  (0 + 1) / 1][Stage 10:>                 (0 + 1) / 1][Stage 9:>                                                          (0 + 1) / 1][Stage 13:>                                                       (0 + 40) / 40][Stage 13:=>                                                      (1 + 39) / 40]                                                                                Spark session created successfully.
DataFrame 'gpspoint' Schema:
DataFrame 'sectionals' Schema:
2025-01-08 05:04:28 - Python script failed with exit code 1
2025-01-09 05:00:01 - Starting stat_type_update job
2025-01-09 05:00:01 - Environment variables loaded from /home/exx/myCode/horse-racing/FoxRiverAIRacing/config/.env
2025-01-09 05:00:01 - Running Python script: /home/exx/myCode/horse-racing/FoxRiverAIRacing/src/data_preprocessing/tpd_aggregation_update.py
25/01/09 05:00:03 WARN Utils: Your hostname, mail.relufox.ai resolves to a loopback address: 127.0.0.1; using 192.168.4.25 instead (on interface eno2)
25/01/09 05:00:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
25/01/09 05:00:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/01/09 05:00:03 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
25/01/09 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 1:>                                                          (0 + 1) / 1]                                                                                [Stage 2:>    (0 + 1) / 1][Stage 3:>    (0 + 1) / 1][Stage 4:>    (0 + 1) / 1][Stage 3:>                  (0 + 1) / 1][Stage 4:>                  (0 + 1) / 1][Stage 3:>                                                          (0 + 1) / 1][Stage 8:>                                                      (0 + 128) / 200][Stage 8:>                                                      (1 + 128) / 200][Stage 8:>                                                      (2 + 128) / 200][Stage 8:=>                                                     (5 + 130) / 200][Stage 8:==============>                                       (52 + 131) / 200][Stage 8:===================================>                  (130 + 70) / 200][Stage 8:=======================================>              (146 + 54) / 200][Stage 8:============================================>         (165 + 35) / 200][Stage 8:=================================================>    (182 + 18) / 200]                                                                                [Stage 9:>                  (0 + 1) / 1][Stage 10:>                 (0 + 1) / 1][Stage 9:>                                                          (0 + 1) / 1][Stage 13:=========>                                              (7 + 33) / 40]                                                                                Spark session created successfully.
DataFrame 'gpspoint' Schema:
DataFrame 'sectionals' Schema:
2025-01-09 05:04:29 - Python script failed with exit code 1
2025-01-10 05:00:01 - Starting stat_type_update job
2025-01-10 05:00:01 - Environment variables loaded from /home/exx/myCode/horse-racing/FoxRiverAIRacing/config/.env
2025-01-10 05:00:01 - Running Python script: /home/exx/myCode/horse-racing/FoxRiverAIRacing/src/data_preprocessing/tpd_aggregation_update.py
25/01/10 05:00:02 WARN Utils: Your hostname, mail.relufox.ai resolves to a loopback address: 127.0.0.1; using 192.168.4.25 instead (on interface eno2)
25/01/10 05:00:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/01/10 05:00:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/01/10 05:00:02 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
[Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 1:>                                                          (0 + 1) / 1]                                                                                [Stage 2:>    (0 + 1) / 1][Stage 3:>    (0 + 1) / 1][Stage 4:>    (0 + 1) / 1][Stage 3:>                  (0 + 1) / 1][Stage 4:>                  (0 + 1) / 1][Stage 3:>                                                          (0 + 1) / 1][Stage 8:>                                                          (0 + 8) / 8][Stage 8:=======>                                                   (1 + 7) / 8]                                                                                [Stage 9:>                  (0 + 1) / 1][Stage 10:>                 (0 + 1) / 1][Stage 9:>                                                          (0 + 1) / 1][Stage 13:>                                                         (0 + 8) / 8][Stage 13:==============>                                           (2 + 6) / 8]                                                                                Spark session created successfully.
DataFrame 'gpspoint' Schema:
DataFrame 'sectionals' Schema:
2025-01-10 05:04:35 - Python script failed with exit code 1
2025-01-11 05:00:01 - Starting stat_type_update job
2025-01-11 05:00:01 - Environment variables loaded from /home/exx/myCode/horse-racing/FoxRiverAIRacing/config/.env
2025-01-11 05:00:01 - Running Python script: /home/exx/myCode/horse-racing/FoxRiverAIRacing/src/data_preprocessing/tpd_aggregation_update.py
25/01/11 05:00:02 WARN Utils: Your hostname, mail.relufox.ai resolves to a loopback address: 127.0.0.1; using 192.168.4.25 instead (on interface eno2)
25/01/11 05:00:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
25/01/11 05:00:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/01/11 05:00:03 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
[Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 1:>                                                          (0 + 1) / 1]                                                                                [Stage 2:>    (0 + 1) / 1][Stage 3:>    (0 + 1) / 1][Stage 4:>    (0 + 1) / 1][Stage 3:>                  (0 + 1) / 1][Stage 4:>                  (0 + 1) / 1][Stage 3:>                                                          (0 + 1) / 1][Stage 8:>                                                      (0 + 128) / 200][Stage 8:>                                                      (0 + 129) / 200][Stage 8:=>                                                     (5 + 128) / 200][Stage 8:=======>                                              (28 + 129) / 200][Stage 8:==================================>                   (128 + 72) / 200][Stage 8:==================================>                   (129 + 71) / 200][Stage 8:===========================================>          (160 + 40) / 200]                                                                                [Stage 9:>                  (0 + 1) / 1][Stage 10:>                 (0 + 1) / 1][Stage 9:>                                                          (0 + 1) / 1][Stage 13:==>                                                     (2 + 38) / 40]                                                                                Spark session created successfully.
DataFrame 'gpspoint' Schema:
DataFrame 'sectionals' Schema:
2025-01-11 05:04:22 - Python script failed with exit code 1
2025-01-12 05:00:01 - Starting stat_type_update job
2025-01-12 05:00:01 - Environment variables loaded from /home/exx/myCode/horse-racing/FoxRiverAIRacing/config/.env
2025-01-12 05:00:01 - Running Python script: /home/exx/myCode/horse-racing/FoxRiverAIRacing/src/data_preprocessing/tpd_aggregation_update.py
25/01/12 05:00:03 WARN Utils: Your hostname, mail.relufox.ai resolves to a loopback address: 127.0.0.1; using 192.168.4.25 instead (on interface eno2)
25/01/12 05:00:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
25/01/12 05:00:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/01/12 05:00:03 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
25/01/12 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/01/12 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/01/12 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/01/12 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 1:>                                                          (0 + 1) / 1]                                                                                [Stage 2:>    (0 + 1) / 1][Stage 3:>    (0 + 1) / 1][Stage 4:>    (0 + 1) / 1][Stage 3:>                  (0 + 1) / 1][Stage 4:>                  (0 + 1) / 1][Stage 3:>                                                          (0 + 1) / 1][Stage 8:>                                                      (0 + 128) / 200][Stage 8:>                                                      (3 + 128) / 200][Stage 8:=>                                                     (5 + 128) / 200][Stage 8:=>                                                     (7 + 128) / 200][Stage 8:===>                                                  (12 + 129) / 200][Stage 8:==================================>                   (128 + 72) / 200][Stage 8:===================================>                  (133 + 67) / 200][Stage 8:=======================================>              (146 + 54) / 200][Stage 8:========================================>             (150 + 50) / 200][Stage 8:===========================================>          (162 + 38) / 200]                                                                                [Stage 9:>                  (0 + 1) / 1][Stage 10:>                 (0 + 1) / 1][Stage 9:>                                                          (0 + 1) / 1][Stage 13:=>                                                      (1 + 39) / 40]                                                                                Spark session created successfully.
DataFrame 'gpspoint' Schema:
DataFrame 'sectionals' Schema:
2025-01-12 05:04:23 - Python script failed with exit code 1
2025-01-13 05:00:01 - Starting stat_type_update job
2025-01-13 05:00:01 - Environment variables loaded from /home/exx/myCode/horse-racing/FoxRiverAIRacing/config/.env
2025-01-13 05:00:01 - Running Python script: /home/exx/myCode/horse-racing/FoxRiverAIRacing/src/data_preprocessing/tpd_aggregation_update.py
25/01/13 05:00:02 WARN Utils: Your hostname, mail.relufox.ai resolves to a loopback address: 127.0.0.1; using 192.168.4.25 instead (on interface eno2)
25/01/13 05:00:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
25/01/13 05:00:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/01/13 05:00:03 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
25/01/13 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/01/13 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/01/13 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/01/13 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/01/13 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
[Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 1:>                                                          (0 + 1) / 1]                                                                                [Stage 2:>    (0 + 1) / 1][Stage 3:>    (0 + 1) / 1][Stage 4:>    (0 + 1) / 1][Stage 3:>                  (0 + 1) / 1][Stage 4:>                  (0 + 1) / 1][Stage 3:>                                                          (0 + 1) / 1][Stage 8:>                                                      (0 + 128) / 200][Stage 8:>                                                      (2 + 128) / 200][Stage 8:===>                                                  (12 + 128) / 200][Stage 8:=======>                                              (27 + 128) / 200][Stage 8:==================================>                   (128 + 72) / 200][Stage 8:===================================>                  (130 + 70) / 200]                                                                                [Stage 9:>                  (0 + 1) / 1][Stage 10:>                 (0 + 1) / 1][Stage 9:>                                                          (0 + 1) / 1][Stage 13:======================>                                (16 + 24) / 40]                                                                                Spark session created successfully.
DataFrame 'gpspoint' Schema:
DataFrame 'sectionals' Schema:
2025-01-13 05:04:25 - Python script failed with exit code 1
2025-01-14 05:00:01 - Starting stat_type_update job
2025-01-14 05:00:01 - Environment variables loaded from /home/exx/myCode/horse-racing/FoxRiverAIRacing/config/.env
2025-01-14 05:00:01 - Running Python script: /home/exx/myCode/horse-racing/FoxRiverAIRacing/src/data_preprocessing/tpd_aggregation_update.py
25/01/14 05:00:02 WARN Utils: Your hostname, mail.relufox.ai resolves to a loopback address: 127.0.0.1; using 192.168.4.25 instead (on interface eno2)
25/01/14 05:00:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
25/01/14 05:00:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/01/14 05:00:02 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
25/01/14 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/01/14 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/01/14 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/01/14 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/01/14 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/01/14 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 1:>                                                          (0 + 1) / 1]                                                                                [Stage 2:>    (0 + 1) / 1][Stage 3:>    (0 + 1) / 1][Stage 4:>    (0 + 1) / 1][Stage 3:>                  (0 + 1) / 1][Stage 4:>                  (0 + 1) / 1][Stage 3:>                                                          (0 + 1) / 1][Stage 8:>                                                      (0 + 128) / 200][Stage 8:>                                                      (2 + 128) / 200][Stage 8:>                                                      (3 + 128) / 200][Stage 8:==>                                                    (9 + 128) / 200][Stage 8:=================>                                    (66 + 128) / 200][Stage 8:==================================>                   (128 + 72) / 200][Stage 8:===================================>                  (130 + 70) / 200][Stage 8:========================================>             (150 + 50) / 200]                                                                                [Stage 9:>                                                          (0 + 1) / 1][Stage 9:>                  (0 + 1) / 1][Stage 10:>                 (0 + 1) / 1][Stage 9:>                                                          (0 + 1) / 1][Stage 13:=====================================>                 (27 + 13) / 40]                                                                                Spark session created successfully.
DataFrame 'gpspoint' Schema:
DataFrame 'sectionals' Schema:
2025-01-14 05:04:23 - Python script failed with exit code 1
2025-01-15 05:00:01 - Starting stat_type_update job
2025-01-15 05:00:01 - Environment variables loaded from /home/exx/myCode/horse-racing/FoxRiverAIRacing/config/.env
2025-01-15 05:00:01 - Running Python script: /home/exx/myCode/horse-racing/FoxRiverAIRacing/src/data_preprocessing/tpd_aggregation_update.py
25/01/15 05:00:02 WARN Utils: Your hostname, mail.relufox.ai resolves to a loopback address: 127.0.0.1; using 192.168.4.25 instead (on interface eno2)
25/01/15 05:00:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
25/01/15 05:00:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/01/15 05:00:03 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
25/01/15 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/01/15 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/01/15 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/01/15 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/01/15 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/01/15 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 1:>                                                          (0 + 1) / 1]                                                                                [Stage 2:>    (0 + 1) / 1][Stage 3:>    (0 + 1) / 1][Stage 4:>    (0 + 1) / 1][Stage 3:>                  (0 + 1) / 1][Stage 4:>                  (0 + 1) / 1][Stage 3:>                                                          (0 + 1) / 1][Stage 8:>                                                      (0 + 128) / 200][Stage 8:>                                                      (1 + 128) / 200][Stage 8:>                                                      (3 + 128) / 200][Stage 8:=>                                                     (4 + 128) / 200][Stage 8:==>                                                    (9 + 128) / 200][Stage 8:======>                                               (23 + 128) / 200][Stage 8:============>                                         (47 + 128) / 200][Stage 8:==================>                                   (67 + 128) / 200][Stage 8:======================================>               (142 + 58) / 200][Stage 8:==============================================>       (171 + 29) / 200][Stage 8:=================================================>    (183 + 17) / 200]                                                                                [Stage 9:>                  (0 + 1) / 1][Stage 10:>                 (0 + 1) / 1][Stage 9:>                                                          (0 + 1) / 1][Stage 13:>                                                       (0 + 40) / 40][Stage 13:=>                                                      (1 + 39) / 40][Stage 13:===========>                                            (8 + 32) / 40]                                                                                [Stage 27:>   (0 + 1) / 1][Stage 28:>   (0 + 1) / 1][Stage 29:>   (0 + 1) / 1][Stage 27:>                 (0 + 1) / 1][Stage 29:>                 (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1][Stage 33:>                                                     (0 + 100) / 100][Stage 33:>                                                      (1 + 99) / 100][Stage 33:=>                                                     (2 + 98) / 100][Stage 33:===========================>                          (50 + 50) / 100]                                                                                Spark session created successfully.
DataFrame 'gpspoint' Schema:
DataFrame 'sectionals' Schema:
2025-01-15 05:05:39 - Python script succeeded
2025-01-15 05:05:39 - Job completed
2025-01-16 05:00:01 - Starting stat_type_update job
2025-01-16 05:00:01 - Environment variables loaded from /home/exx/myCode/horse-racing/FoxRiverAIRacing/config/.env
2025-01-16 05:00:01 - Running Python script: /home/exx/myCode/horse-racing/FoxRiverAIRacing/src/data_preprocessing/tpd_aggregation_update.py
Traceback (most recent call last):
  File "/home/exx/myCode/horse-racing/FoxRiverAIRacing/src/data_preprocessing/tpd_aggregation_update.py", line 678, in <module>
    main()
  File "/home/exx/myCode/horse-racing/FoxRiverAIRacing/src/data_preprocessing/tpd_aggregation_update.py", line 673, in main
    spark.stop()
    ^^^^^
NameError: name 'spark' is not defined
2025-01-16 05:00:37 - Python script failed with exit code 1
2025-01-17 05:00:01 - Starting stat_type_update job
2025-01-17 05:00:01 - Environment variables loaded from /home/exx/myCode/horse-racing/FoxRiverAIRacing/config/.env
2025-01-17 05:00:01 - Running Python script: /home/exx/myCode/horse-racing/FoxRiverAIRacing/src/data_preprocessing/tpd_aggregation_update.py
25/01/17 05:00:02 WARN Utils: Your hostname, mail.relufox.ai resolves to a loopback address: 127.0.0.1; using 192.168.4.25 instead (on interface eno2)
25/01/17 05:00:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
25/01/17 05:00:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/01/17 05:00:03 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
25/01/17 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/01/17 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/01/17 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/01/17 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/01/17 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/01/17 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 1:>                                                          (0 + 1) / 1]                                                                                [Stage 2:>    (0 + 1) / 1][Stage 3:>    (0 + 1) / 1][Stage 4:>    (0 + 1) / 1][Stage 3:>                  (0 + 1) / 1][Stage 4:>                  (0 + 1) / 1][Stage 3:>                                                          (0 + 1) / 1][Stage 8:>                                                      (0 + 128) / 200][Stage 8:>                                                      (1 + 128) / 200][Stage 8:>                                                      (2 + 128) / 200][Stage 8:=>                                                     (7 + 128) / 200][Stage 8:===================================>                  (130 + 70) / 200][Stage 8:===================================>                  (132 + 68) / 200][Stage 8:=========================================>            (152 + 48) / 200]                                                                                [Stage 9:>                  (0 + 1) / 1][Stage 10:>                 (0 + 1) / 1][Stage 9:>                                                          (0 + 1) / 1][Stage 13:==============================>                        (22 + 18) / 40]                                                                                [Stage 27:>   (0 + 1) / 1][Stage 28:>   (0 + 1) / 1][Stage 29:>   (0 + 1) / 1][Stage 27:>                 (0 + 1) / 1][Stage 29:>                 (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1][Stage 33:>                                                     (0 + 100) / 100][Stage 33:>                                                      (1 + 99) / 100][Stage 33:=>                                                     (3 + 97) / 100]                                                                                Spark session created successfully.
DataFrame 'gpspoint' Schema:
DataFrame 'sectionals' Schema:
2025-01-17 05:05:14 - Python script succeeded
2025-01-17 05:05:14 - Job completed
2025-01-18 05:00:01 - Starting stat_type_update job
2025-01-18 05:00:01 - Environment variables loaded from /home/exx/myCode/horse-racing/FoxRiverAIRacing/config/.env
2025-01-18 05:00:01 - Running Python script: /home/exx/myCode/horse-racing/FoxRiverAIRacing/src/data_preprocessing/tpd_aggregation_update.py
25/01/18 05:00:02 WARN Utils: Your hostname, mail.relufox.ai resolves to a loopback address: 127.0.0.1; using 192.168.4.25 instead (on interface eno2)
25/01/18 05:00:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
25/01/18 05:00:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/01/18 05:00:03 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
25/01/18 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/01/18 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/01/18 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/01/18 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/01/18 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/01/18 05:00:03 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 1:>                                                          (0 + 1) / 1]                                                                                [Stage 2:>                                                          (0 + 1) / 1][Stage 2:>    (0 + 1) / 1][Stage 3:>    (0 + 1) / 1][Stage 4:>    (0 + 1) / 1][Stage 3:>                  (0 + 1) / 1][Stage 4:>                  (0 + 1) / 1][Stage 3:>                                                          (0 + 1) / 1][Stage 8:>                                                      (0 + 128) / 200][Stage 8:>                                                      (1 + 128) / 200][Stage 8:=======================>                              (87 + 113) / 200][Stage 8:==================================>                   (128 + 72) / 200]                                                                                [Stage 9:>                  (0 + 1) / 1][Stage 10:>                 (0 + 1) / 1][Stage 9:>                                                          (0 + 1) / 1][Stage 13:==============================================>         (33 + 7) / 40]                                                                                [Stage 27:>   (0 + 1) / 1][Stage 28:>   (0 + 1) / 1][Stage 29:>   (0 + 1) / 1][Stage 27:>                 (0 + 1) / 1][Stage 29:>                 (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1][Stage 33:>                                                     (0 + 100) / 100][Stage 33:>                                                      (1 + 99) / 100][Stage 33:====>                                                  (9 + 91) / 100]                                                                                Spark session created successfully.
DataFrame 'gpspoint' Schema:
DataFrame 'sectionals' Schema:
2025-01-18 05:05:05 - Python script succeeded
2025-01-18 05:05:05 - Job completed
2025-01-18 11:42:08 - Starting stat_type_update job
2025-01-18 11:42:08 - Environment variables loaded from /home/exx/myCode/horse-racing/FoxRiverAIRacing/config/.env
2025-01-18 11:42:08 - Running Python script: /home/exx/myCode/horse-racing/FoxRiverAIRacing/src/data_preprocessing/tpd_aggregation_update.py
25/01/18 11:42:09 WARN Utils: Your hostname, mail.relufox.ai resolves to a loopback address: 127.0.0.1; using 192.168.4.25 instead (on interface eno2)
25/01/18 11:42:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/opt/spark-3.4.4-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /home/exx/.ivy2/cache
The jars for the packages stored in: /home/exx/.ivy2/jars
ml.dmlc#xgboost4j-spark_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-fa9bf972-d37c-436c-b2db-f88713d12f70;1.0
	confs: [default]
	found ml.dmlc#xgboost4j-spark_2.12;1.7.5 in central
	found ml.dmlc#xgboost4j_2.12;1.7.5 in central
	found com.typesafe.akka#akka-actor_2.12;2.5.23 in central
	found com.typesafe#config;1.3.3 in central
	found org.scala-lang.modules#scala-java8-compat_2.12;0.8.0 in central
	found com.esotericsoftware#kryo;4.0.2 in central
	found com.esotericsoftware#reflectasm;1.11.3 in central
	found org.ow2.asm#asm;5.0.4 in central
	found com.esotericsoftware#minlog;1.3.0 in central
	found org.objenesis#objenesis;2.5.1 in central
	found org.scala-lang#scala-reflect;2.12.8 in central
	found commons-logging#commons-logging;1.2 in central
:: resolution report :: resolve 138ms :: artifacts dl 5ms
	:: modules in use:
	com.esotericsoftware#kryo;4.0.2 from central in [default]
	com.esotericsoftware#minlog;1.3.0 from central in [default]
	com.esotericsoftware#reflectasm;1.11.3 from central in [default]
	com.typesafe#config;1.3.3 from central in [default]
	com.typesafe.akka#akka-actor_2.12;2.5.23 from central in [default]
	commons-logging#commons-logging;1.2 from central in [default]
	ml.dmlc#xgboost4j-spark_2.12;1.7.5 from central in [default]
	ml.dmlc#xgboost4j_2.12;1.7.5 from central in [default]
	org.objenesis#objenesis;2.5.1 from central in [default]
	org.ow2.asm#asm;5.0.4 from central in [default]
	org.scala-lang#scala-reflect;2.12.8 from central in [default]
	org.scala-lang.modules#scala-java8-compat_2.12;0.8.0 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-fa9bf972-d37c-436c-b2db-f88713d12f70
	confs: [default]
	0 artifacts copied, 12 already retrieved (0kB/4ms)
25/01/18 11:42:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/01/18 11:42:10 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
25/01/18 11:42:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
25/01/18 11:42:10 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
25/01/18 11:42:10 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
25/01/18 11:42:10 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
25/01/18 11:42:10 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
25/01/18 11:42:10 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
[Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1][Stage 0:>                                                          (0 + 1) / 1]                                                                                [Stage 1:>                                                          (0 + 1) / 1]                                                                                [Stage 2:>    (0 + 1) / 1][Stage 3:>    (0 + 1) / 1][Stage 4:>    (0 + 1) / 1][Stage 3:>                  (0 + 1) / 1][Stage 4:>                  (0 + 1) / 1][Stage 3:>                                                          (0 + 1) / 1][Stage 8:>                                                      (0 + 128) / 200][Stage 8:>                                                      (3 + 128) / 200][Stage 8:=>                                                     (6 + 128) / 200][Stage 8:==>                                                    (9 + 128) / 200][Stage 8:=======>                                              (27 + 129) / 200][Stage 8:=============>                                        (50 + 128) / 200][Stage 8:====================================>                 (136 + 64) / 200][Stage 8:===========================================>          (161 + 39) / 200]                                                                                [Stage 9:>                  (0 + 1) / 1][Stage 10:>                 (0 + 1) / 1][Stage 9:>                                                          (0 + 1) / 1][Stage 13:=======>                                                (5 + 35) / 40]                                                                                [Stage 27:>   (0 + 1) / 1][Stage 28:>   (0 + 1) / 1][Stage 29:>   (0 + 1) / 1][Stage 27:>                 (0 + 1) / 1][Stage 29:>                 (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1][Stage 33:>                                                     (0 + 100) / 100][Stage 33:>                                                      (1 + 99) / 100][Stage 33:=========>                                            (17 + 83) / 100]                                                                                Spark session created successfully.
DataFrame 'gpspoint' Schema:
DataFrame 'sectionals' Schema:
2025-01-18 11:47:19 - Python script succeeded
2025-01-18 11:47:19 - Job completed
