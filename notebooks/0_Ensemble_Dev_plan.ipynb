{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43428b1c",
   "metadata": {},
   "source": [
    "# Ensemble Dev Plan\n",
    "\n",
    "Here are several models you can use with your current data to generate probability rankings for an ensemble. Each model supports probabilistic predictions and can be integrated into an ensemble:\n",
    "\n",
    "1. Logistic Regression\n",
    "\n",
    "Why Use It:\n",
    "\t•\tSimple, interpretable, and efficient for binary and multiclass classification.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=500, random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "probs_log_reg = log_reg.predict_proba(X_test_scaled)  # Probabilities\n",
    "\n",
    "2. XGBoost (already used)\n",
    "\n",
    "Why Use It:\n",
    "\t•\tHigh accuracy and feature importance.\n",
    "\n",
    "Implementation:\n",
    "(You already have this, but include it in your ensemble.)\n",
    "\n",
    "3. Gradient Boosting Classifier (LightGBM)\n",
    "\n",
    "Why Use It:\n",
    "\t•\tSimilar to XGBoost but faster and uses less memory.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n",
    "lgb_model.fit(X_train_scaled, y_train)\n",
    "probs_lgb = lgb_model.predict_proba(X_test_scaled)  # Probabilities\n",
    "\n",
    "4. Support Vector Machines (SVM) with Probabilistic Outputs\n",
    "\n",
    "Why Use It:\n",
    "\t•\tEffective for high-dimensional spaces and non-linear classification.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(probability=True, kernel=\"rbf\", random_state=42)\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "probs_svc = svc.predict_proba(X_test_scaled)  # Probabilities\n",
    "\n",
    "5. k-Nearest Neighbors (k-NN)\n",
    "\n",
    "Why Use It:\n",
    "\t•\tSimple, interpretable, and non-parametric.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "probs_knn = knn.predict_proba(X_test_scaled)  # Probabilities\n",
    "\n",
    "6. Multi-Layer Perceptron (MLP) Classifier\n",
    "\n",
    "Why Use It:\n",
    "\t•\tNeural network-based approach; good for non-linear relationships.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=300, random_state=42)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "probs_mlp = mlp.predict_proba(X_test_scaled)  # Probabilities\n",
    "\n",
    "7. CatBoost\n",
    "\n",
    "Why Use It:\n",
    "\t•\tFast and works well with categorical data without much preprocessing.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cat_model = CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, random_state=42, verbose=0)\n",
    "cat_model.fit(X_train_scaled, y_train)\n",
    "probs_cat = cat_model.predict_proba(X_test_scaled)  # Probabilities\n",
    "\n",
    "Ensemble Creation\n",
    "\n",
    "Once you have probability outputs from all models:\n",
    "\t1.\tCombine the probabilities using an averaging or weighted averaging strategy.\n",
    "\t2.\tOptionally, use a meta-model (like Logistic Regression or XGBoost) to learn the optimal combination of the predictions.\n",
    "\n",
    "Example: Averaging Probabilities\n",
    "\n",
    "# Combine probabilities (equal weights)\n",
    "combined_probs = (probs_log_reg + probs_lgb + probs_svc + probs_knn + probs_mlp + probs_cat) / 6\n",
    "\n",
    "# Get final predictions\n",
    "final_predictions = combined_probs.argmax(axis=1)  # For multiclass\n",
    "\n",
    "Example: Stacking with Meta-Model\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stack = StackingClassifier(\n",
    "    estimators=[\n",
    "        (\"log_reg\", log_reg),\n",
    "        (\"lgb\", lgb_model),\n",
    "        (\"svc\", svc),\n",
    "        (\"knn\", knn),\n",
    "        (\"mlp\", mlp),\n",
    "        (\"cat\", cat_model),\n",
    "    ],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "stack.fit(X_train_scaled, y_train)\n",
    "stack_probs = stack.predict_proba(X_test_scaled)\n",
    "\n",
    "Models Summary\n",
    "\t•\tLogistic Regression: A simple baseline model.\n",
    "\t•\tXGBoost: Powerful gradient boosting.\n",
    "\t•\tLightGBM: Lightweight and faster alternative to XGBoost.\n",
    "\t•\tSVM: Works well for complex decision boundaries.\n",
    "\t•\tk-NN: Non-parametric and interpretable.\n",
    "\t•\tMLP: Neural network-based model for non-linear relationships.\n",
    "\t•\tCatBoost: Efficient handling of categorical data.\n",
    "\n",
    "By combining these models into an ensemble, you can leverage their strengths to improve overall predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3287070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
