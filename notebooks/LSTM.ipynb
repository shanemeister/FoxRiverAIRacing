{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277e9121-0ed4-44f3-b848-ba98dc6ce795",
   "metadata": {},
   "source": [
    "# Proposed Ensemble Models\n",
    "\n",
    "Given the constraints and objectives, I recommend considering the following models for the ensemble:\n",
    "\t\n",
    "    1.\tModel 1: LSTM Network on Raw GPS Data\n",
    "    \n",
    ">•\tInput Data: Sequences of raw GPS data (speed, progress, stride_frequency, etc.).\n",
    "\n",
    ">•\tArchitecture: An LSTM network designed to capture temporal dependencies and patterns in the sequential data.\n",
    "\n",
    ">•\tAdvantage: LSTMs are well-suited for time-series data and can learn complex temporal dynamics without the need for hand-engineered features like acceleration.\n",
    "\n",
    "    2.\tModel 2: 1D Convolutional Neural Network (1D-CNN)\n",
    "\t\n",
    ">•\tInput Data: The same raw GPS sequences as in Model 1.\n",
    "\n",
    ">•\tArchitecture: A 1D-CNN that applies convolutional filters across the time dimension to detect local patterns.\n",
    "\n",
    ">•\tAdvantage: CNNs can capture spatial hierarchies and are effective in recognizing patterns in sequences, potentially identifying features like sudden changes in speed or stride frequency.\n",
    "\n",
    "    3.\tModel 3: Transformer-based Model\n",
    "\t\n",
    ">•\tInput Data: Raw GPS sequences and possibly sectionals data.\n",
    "\n",
    ">•\tArchitecture: A Transformer model that uses self-attention mechanisms to weigh the importance of different parts of the sequence.\n",
    "\n",
    ">•\tAdvantage: Transformers can model long-range dependencies and focus on the most relevant parts of the sequence for prediction.\n",
    "\n",
    "## Additional Models (Optional):\n",
    "\n",
    "    4.\tModel 4: Gated Recurrent Unit (GRU) Network\n",
    "\n",
    ">•\tSimilar to LSTMs but with a simpler architecture, GRUs can be more efficient and may perform better on certain datasets.\n",
    "\n",
    ">•\tModel 5: Temporal Convolutional Network (TCN)\n",
    "\n",
    ">•\tTCNs are designed for sequential data and can capture long-term dependencies using causal convolutions and residual connections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c90ab6",
   "metadata": {},
   "source": [
    "### Validate GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14e5426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3eb8b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 2\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available:\", len(physical_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee863758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "for device in physical_devices:\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afcdb84",
   "metadata": {},
   "source": [
    "#### Make sure JAVA HOME is set to version 11 -- source .zshrc in same folder as you start jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e351e3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/java-11-openjdk\n",
      "openjdk 11.0.25 2024-10-15 LTS\n",
      "OpenJDK Runtime Environment (Red_Hat-11.0.25.0.9-1) (build 11.0.25+9-LTS)\n",
      "OpenJDK 64-Bit Server VM (Red_Hat-11.0.25.0.9-1) (build 11.0.25+9-LTS, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!echo $JAVA_HOME\n",
    "!java --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144788c4-1601-45af-97bd-2832f1d4d22d",
   "metadata": {},
   "source": [
    "# The LSTM Network on Raw GPS Data\n",
    "\n",
    "Initially I desired to merge the GPS data with Sectionals, but the timestamp and gate_name intervals of each respectively made it difficult to align the data in sequences -- something that is needed for Long-Short Term Memory models. Therefore, it was decided to go with an ensemble approach. There will be additional models that incorporate Equibase data as well, but for the time being, the focus will be on Total Performance GPS data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0647b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 20:45:51,531 - INFO - Environment setup initialized.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/06 20:45:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2024-12-06 20:45:53,440 - INFO - Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min as spark_min, sum as spark_sum, from_utc_timestamp, expr\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('/home/exx/myCode/horse-racing/FoxRiverAIRacing/config.ini')\n",
    "\n",
    "# Database credentials from config\n",
    "db_host = config['database']['host']\n",
    "db_port = config['database']['port']\n",
    "db_name = config['database']['dbname']\n",
    "db_user = config['database']['user']\n",
    "db_password = os.getenv(\"DB_PASSWORD\", \"SparkPy24!\")  # Ensure DB_PASSWORD is set\n",
    "\n",
    "# Validate database password\n",
    "if not db_password:\n",
    "    raise ValueError(\"Database password is missing. Set it in the DB_PASSWORD environment variable.\")\n",
    "\n",
    "# JDBC URL and properties\n",
    "jdbc_url = f\"jdbc:postgresql://{db_host}:{db_port}/{db_name}\"\n",
    "jdbc_properties = {\n",
    "    \"user\": db_user,\n",
    "    \"password\": db_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Path to JDBC driver\n",
    "jdbc_driver_path = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/jdbc/postgresql-42.7.4.jar\"\n",
    "\n",
    "# Configure logging\n",
    "log_file = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/logs/SparkPy_load.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logging.info(\"Environment setup initialized.\")\n",
    "\n",
    "# Initialize Spark session\n",
    "def initialize_spark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Horse Racing Data Processing\") \\\n",
    "        .config(\"spark.driver.extraClassPath\", jdbc_driver_path) \\\n",
    "        .config(\"spark.executor.extraClassPath\", jdbc_driver_path) \\\n",
    "        .config(\"spark.driver.memory\", \"64g\") \\\n",
    "        .config(\"spark.executor.memory\", \"32g\") \\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"8g\") \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", \"1000\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"LEGACY\") \\\n",
    "        .config(\"spark.sql.legacy.parquet.int96RebaseModeInWrite\", \"LEGACY\") \\\n",
    "        .getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    logging.info(\"Spark session created successfully.\")\n",
    "    return spark\n",
    "\n",
    "# Initialize Spark\n",
    "spark = initialize_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518deb19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 20:45:54,482 - INFO - Loading results data from PostgreSQL...\n",
      "2024-12-06 20:45:55,637 - INFO - Saving results DataFrame to Parquet at /home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/results.parquet...\n",
      "2024-12-06 20:45:58,430 - INFO - results data loaded and saved successfully.    \n",
      "2024-12-06 20:45:58,431 - INFO - Loading sectionals data from PostgreSQL...\n",
      "2024-12-06 20:45:58,451 - INFO - Saving sectionals DataFrame to Parquet at /home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/sectionals.parquet...\n",
      "2024-12-06 20:46:07,526 - INFO - sectionals data loaded and saved successfully. \n",
      "2024-12-06 20:46:07,527 - INFO - Loading gpspoint data from PostgreSQL...\n",
      "2024-12-06 20:46:07,547 - INFO - Saving gpspoint DataFrame to Parquet at /home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/gpspoint.parquet...\n",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Data Loading and Transformation\n",
    "\n",
    "from pyspark.sql.functions import col, min as spark_min, sum as spark_sum, unix_timestamp, to_timestamp, expr\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define SQL queries without trailing semicolons\n",
    "queries = {\n",
    "    \"results\": \"\"\"\n",
    "        SELECT vre.course_cd, vre.race_date, vre.race_number, vre.program_num AS saddle_cloth_number, vre.post_pos,\n",
    "               h.horse_id, vre.official_fin, vre.finish_time, vre.speed_rating, vr.todays_cls,\n",
    "               vr.previous_surface, vr.previous_class, vr.net_sentiment\n",
    "        FROM v_results_entries vre\n",
    "        JOIN v_runners vr \n",
    "            ON vre.course_cd = vr.course_cd\n",
    "            AND vre.race_date = vr.race_date\n",
    "            AND vre.race_number = vr.race_number\n",
    "            AND vre.program_num = vr.saddle_cloth_number\n",
    "        JOIN horse h \n",
    "            ON vre.axciskey = h.axciskey\n",
    "        WHERE vre.breed = 'TB'\n",
    "        GROUP BY vre.course_cd, vre.race_date, vre.race_number, vre.program_num, vre.post_pos,\n",
    "                 h.horse_id, vre.official_fin, vre.finish_time, vre.speed_rating, vr.todays_cls,\n",
    "                 vr.previous_surface, vr.previous_class, vr.net_sentiment\n",
    "    \"\"\",\n",
    "    \"sectionals\": \"\"\"\n",
    "        SELECT course_cd, race_date, race_number, saddle_cloth_number, gate_name, \n",
    "               gate_numeric, length_to_finish, sectional_time, running_time, \n",
    "               distance_back, distance_ran, number_of_strides\n",
    "        FROM v_sectionals\n",
    "    \"\"\",\n",
    "    \"gpspoint\": \"\"\"\n",
    "        SELECT course_cd, race_date, race_number, saddle_cloth_number, time_stamp, \n",
    "               longitude, latitude, speed, progress, stride_frequency, post_time, location\n",
    "        FROM v_gpspoint\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Path to save Parquet files\n",
    "parquet_dir = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/\"\n",
    "os.makedirs(parquet_dir, exist_ok=True)\n",
    "\n",
    "# Load data directly from PostgreSQL into Spark DataFrames and save as Parquet\n",
    "dfs = {}\n",
    "for name, query in queries.items():\n",
    "    logging.info(f\"Loading {name} data from PostgreSQL...\")\n",
    "    try:\n",
    "        df = spark.read.jdbc(url=jdbc_url, table=f\"({query}) AS subquery\", properties=jdbc_properties)\n",
    "        output_path = os.path.join(parquet_dir, f\"{name}.parquet\")\n",
    "        logging.info(f\"Saving {name} DataFrame to Parquet at {output_path}...\")\n",
    "        df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        dfs[name] = df\n",
    "        logging.info(f\"{name} data loaded and saved successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {name} data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Reload Parquet files into Spark DataFrames for processing\n",
    "logging.info(\"Reloading Parquet files into Spark DataFrames for transformation...\")\n",
    "results_df = spark.read.parquet(os.path.join(parquet_dir, \"results.parquet\"))\n",
    "sectionals_df = spark.read.parquet(os.path.join(parquet_dir, \"sectionals.parquet\"))\n",
    "gps_df = spark.read.parquet(os.path.join(parquet_dir, \"gpspoint.parquet\"))\n",
    "logging.info(\"Parquet files reloaded successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe02de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import timedelta\n",
    "\n",
    "# Define the UDF to add seconds (including fractional seconds) to a timestamp\n",
    "def add_seconds(ts, seconds):\n",
    "    if ts is None or seconds is None:\n",
    "        return None\n",
    "    return ts + timedelta(seconds=seconds)\n",
    "\n",
    "# Register the UDF\n",
    "add_seconds_udf = udf(add_seconds, TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    unix_timestamp,\n",
    "    expr,\n",
    "    min as spark_min,\n",
    "    sum as spark_sum,\n",
    "    date_format\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import udf\n",
    "from datetime import timedelta\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HorseRacingDataProcessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Clear all cached data\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Reload the DataFrames from Parquet files\n",
    "gps_df = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/gpspoint.parquet\")\n",
    "sectionals_df = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/sectionals.parquet\")\n",
    "\n",
    "# Convert time_stamp to timestamp type\n",
    "gps_df = gps_df.withColumn(\"time_stamp\", col(\"time_stamp\").cast(\"timestamp\"))\n",
    "\n",
    "# Print a sample of the time_stamp column to check for millisecond precision\n",
    "print(\"Sample of gps_df time_stamp column:\")\n",
    "gps_df.select(\"time_stamp\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the earliest 'time_stamp' for each race\n",
    "race_id_cols = [\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"]\n",
    "\n",
    "first_time_df = gps_df.groupBy(*race_id_cols).agg(\n",
    "    spark_min(\"time_stamp\").alias(\"earliest_time_stamp\")\n",
    ")\n",
    "\n",
    "# Step 2: Join 'first_time_df' with 'sectionals_df' to associate each sectional with the race's start time\n",
    "sectionals_df = sectionals_df.join(\n",
    "    first_time_df,\n",
    "    on=race_id_cols,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Step 3: Sort 'sectionals_df' by 'gate_numeric' to ensure correct order of gates\n",
    "sectionals_df = sectionals_df.orderBy(*race_id_cols, \"gate_numeric\")\n",
    "\n",
    "# Step 4: Define the window specification for cumulative sum\n",
    "window_spec = Window.partitionBy(*race_id_cols).orderBy(\"gate_numeric\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "# Step 5: Compute cumulative sum of 'sectional_time' for each race\n",
    "sectionals_df = sectionals_df.withColumn(\n",
    "    \"cumulative_sectional_time\",\n",
    "    spark_sum(\"sectional_time\").over(window_spec)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d4211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Define the UDF to add seconds (including fractional seconds) to a timestamp\n",
    "def add_seconds(ts, seconds):\n",
    "    if ts is None or seconds is None:\n",
    "        return None\n",
    "    return ts + timedelta(seconds=seconds)\n",
    "\n",
    "# Register the UDF\n",
    "add_seconds_udf = udf(add_seconds, TimestampType())\n",
    "\n",
    "# Step 7: Create 'sec_time_stamp' by adding 'cumulative_sectional_time' to 'earliest_time_stamp' using the UDF\n",
    "sectionals_df = sectionals_df.withColumn(\n",
    "    \"sec_time_stamp\",\n",
    "    add_seconds_udf(col(\"earliest_time_stamp\"), col(\"cumulative_sectional_time\"))\n",
    ")\n",
    "\n",
    "# Step 8: Drop intermediate columns if no longer needed\n",
    "sectionals_df = sectionals_df.drop(\"earliest_time_stamp\", \"cumulative_sectional_time\")\n",
    "\n",
    "# Show a sample of the results\n",
    "print(\"Sample of sectionals_df with sec_time_stamp:\")\n",
    "sectionals_df.select(\n",
    "    \"course_cd\",\n",
    "    \"race_date\",\n",
    "    \"race_number\",\n",
    "    \"saddle_cloth_number\",\n",
    "    \"gate_numeric\",\n",
    "    \"gate_name\",\n",
    "    \"sectional_time\",\n",
    "    \"sec_time_stamp\"\n",
    ").show(10, truncate=False)\n",
    "\n",
    "# Now, proceed with the join to create matched_df\n",
    "\n",
    "from pyspark.sql.functions import abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8865462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Convert 'time_stamp' and 'sec_time_stamp' to milliseconds since epoch to preserve sub-second precision\n",
    "gps_with_ms = gps_df.withColumn(\n",
    "    \"time_stamp_ms\",\n",
    "    (col(\"time_stamp\").cast(\"double\") * 1000).cast(\"long\")\n",
    ")\n",
    "\n",
    "sectionals_with_ms = sectionals_df.withColumn(\n",
    "    \"sec_time_stamp_ms\",\n",
    "    (col(\"sec_time_stamp\").cast(\"double\") * 1000).cast(\"long\")\n",
    ")\n",
    "\n",
    "# Step 10: Define the join condition with time window (±1000 milliseconds)\n",
    "join_condition = (\n",
    "    (gps_with_ms.course_cd == sectionals_with_ms.course_cd) &\n",
    "    (gps_with_ms.race_date == sectionals_with_ms.race_date) &\n",
    "    (gps_with_ms.race_number == sectionals_with_ms.race_number) &\n",
    "    (gps_with_ms.saddle_cloth_number == sectionals_with_ms.saddle_cloth_number) &\n",
    "    (abs(gps_with_ms.time_stamp_ms - sectionals_with_ms.sec_time_stamp_ms) <= 1000)\n",
    ")\n",
    "\n",
    "# Step 11: Perform the left join based on the join condition\n",
    "matched_df = gps_with_ms.join(\n",
    "    sectionals_with_ms,\n",
    "    on=join_condition,\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    gps_with_ms[\"*\"],\n",
    "    sectionals_with_ms[\"sec_time_stamp\"],\n",
    "    sectionals_with_ms[\"gate_numeric\"],\n",
    "    sectionals_with_ms[\"gate_name\"],\n",
    "    sectionals_with_ms[\"sectional_time\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd60cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Verify the matched records\n",
    "print(\"Sample of matched_df (All gps_df records with sectional data where available):\")\n",
    "matched_df.select(\n",
    "    *race_id_cols,\n",
    "    \"time_stamp\",\n",
    "    \"sec_time_stamp\",\n",
    "    \"gate_numeric\",\n",
    "    \"gate_name\",\n",
    "    \"sectional_time\"\n",
    ").show(10, truncate=False)\n",
    "\n",
    "# Step 13: Show the total number of rows in matched_df, gps_df, and sectionals_df\n",
    "matched_count = matched_df.count()\n",
    "print(f\"Total number of rows in matched_df: {matched_count}\")\n",
    "\n",
    "gps_count = gps_df.count()\n",
    "print(f\"Total number of rows in gps_df: {gps_count}\")\n",
    "\n",
    "sectionals_count = sectionals_df.count()\n",
    "print(f\"Total number of rows in sectionals_df: {sectionals_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c12b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: Show the number of unmatched sectional records\n",
    "\n",
    "# Extract matched sectional records\n",
    "matched_sectionals = matched_df.select(\n",
    "    *race_id_cols,\n",
    "    \"sec_time_stamp\"\n",
    ").distinct()\n",
    "\n",
    "# Perform a left anti join to find sectionals not present in matched_sectionals\n",
    "unmatched_sectionals_df = sectionals_df.join(\n",
    "    matched_sectionals,\n",
    "    on=race_id_cols + [\"sec_time_stamp\"],\n",
    "    how=\"leftanti\"\n",
    ")\n",
    "\n",
    "# Count the number of unmatched sectional records\n",
    "unmatched_sectionals_count = unmatched_sectionals_df.count()\n",
    "print(f\"Number of unmatched sectional records: {unmatched_sectionals_count}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c67ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab259b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617dde1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b9b91a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5949df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cab1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385e158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ebf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sectionals_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc0d301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0f26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6182eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06936ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e3511e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301f64a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae1c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc5a16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151633ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cc612b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8b42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b0ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40feb882",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1c: Compute Time Zone Offset (delta_time_seconds) as the difference between 'first_time_stamp' and 'post_time'\n",
    "# Assuming 'post_time' is in local time and 'first_time_stamp' is in UTC\n",
    "# delta_time_seconds = first_time_stamp (UTC) - post_time (local) => time_zone_offset in seconds\n",
    "\n",
    "logging.info(\"Computing time zone offset (delta_time_seconds) for each race...\")\n",
    "time_zone_offset_df = first_time_with_post_df.withColumn(\n",
    "    \"delta_time_seconds\",\n",
    "    (unix_timestamp(\"first_time_stamp\") - unix_timestamp(\"post_time\"))\n",
    ")\n",
    "\n",
    "# Step 1d: Join 'time_zone_offset_df' back to 'gps_df' and 'sectionals_df'\n",
    "logging.info(\"Joining 'time_zone_offset_df' back to 'gps_df' and 'sectionals_df'...\")\n",
    "gps_df = gps_df.join(\n",
    "    time_zone_offset_df.select(\n",
    "        \"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\", \"delta_time_seconds\"\n",
    "    ),\n",
    "    on=[\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "sectionals_df = sectionals_df.join(\n",
    "    time_zone_offset_df.select(\n",
    "        \"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\", \"delta_time_seconds\"\n",
    "    ),\n",
    "    on=[\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Step 2: Compute 'time_stamp_local' by adjusting 'time_stamp' with 'delta_time_seconds'\n",
    "# 'time_stamp_local' = 'time_stamp' - 'delta_time_seconds'\n",
    "\n",
    "logging.info(\"Computing 'time_stamp_local' by adjusting 'time_stamp' with 'delta_time_seconds'...\")\n",
    "gps_df = gps_df.withColumn(\n",
    "    \"time_stamp_local\",\n",
    "    expr(\"cast((unix_timestamp(time_stamp) - delta_time_seconds) as timestamp)\")\n",
    ")\n",
    "\n",
    "# Step 3: Calculate the minimum 'time_stamp_local' for each race\n",
    "logging.info(\"Calculating minimum 'time_stamp_local' for each race...\")\n",
    "min_time_df = gps_df.groupBy(\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\") \\\n",
    "    .agg(spark_min(\"time_stamp_local\").alias(\"min_time_stamp_local\"))\n",
    "\n",
    "# Step 4: Join 'min_time_df' with 'sectionals_df' to associate each sectional with the race's start time\n",
    "logging.info(\"Joining 'min_time_df' with 'sectionals_df' to associate each sectional with the race's start time...\")\n",
    "sectionals_df = sectionals_df.join(\n",
    "    min_time_df,\n",
    "    on=[\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Step 5: Sort 'sectionals_df' within each race by 'gate_numeric' to maintain sectional order\n",
    "# (Assuming 'gate_numeric' determines the sequence of sectionals)\n",
    "\n",
    "# Step 6: Compute cumulative 'sectional_time' within each race to determine when each sectional occurs\n",
    "logging.info(\"Computing cumulative 'sectional_time' for each race...\")\n",
    "window_spec = Window.partitionBy(\n",
    "    \"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"\n",
    ").orderBy(\"gate_numeric\") \\\n",
    " .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "sectionals_df = sectionals_df.withColumn(\n",
    "    \"sectional_time_cumulative\",\n",
    "    spark_sum(\"sectional_time\").over(window_spec)\n",
    ")\n",
    "\n",
    "# Step 7: Create 'sec_time_stamp' by adding cumulative 'sectional_time' to 'min_time_stamp_local'\n",
    "# Assuming 'sectional_time_cumulative' is in seconds\n",
    "\n",
    "logging.info(\"Creating 'sec_time_stamp' by adding cumulative 'sectional_time' to 'min_time_stamp_local'...\")\n",
    "sectionals_df = sectionals_df.withColumn(\n",
    "    \"sec_time_stamp\",\n",
    "    expr(\"cast((unix_timestamp(min_time_stamp_local) + sectional_time_cumulative) as timestamp)\")\n",
    ")\n",
    "\n",
    "# Step 8: Drop intermediate columns if not needed\n",
    "sectionals_df = sectionals_df.drop(\"min_time_stamp_local\", \"sectional_time_cumulative\", \"delta_time_seconds\")\n",
    "\n",
    "logging.info(\"'sec_time_stamp' created successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca25f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea57dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9051d974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa894c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84796b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b3dcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Display sample data to verify 'sec_time_stamp'\n",
    "logging.info(\"Displaying sample 'sec_time_stamp' values:\")\n",
    "sectionals_df.select(\n",
    "    \"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\",\n",
    "    \"gate_numeric\", \"sectional_time\", \"sec_time_stamp\"\n",
    ").show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba599e5",
   "metadata": {},
   "source": [
    "### Explanation of the Corrected Steps\n",
    "\n",
    "1.\tCalculate Time Zone Offset (delta_time_seconds):\n",
    "\n",
    ">    •\tObjective: Determine the difference between the earliest GPS time_stamp (in UTC) and the post_time (scheduled start time in local time) for each race.\n",
    "\n",
    ">.   •\tImplementation:\n",
    "\n",
    ">.   •\tStep 1a: Group gps_df by race identifiers and find the earliest time_stamp (first_time_stamp) for each race.\n",
    "\n",
    ">    •\tStep 1b: Join first_time_df with gps_df to retrieve the corresponding post_time for each race.\n",
    "\n",
    ">.   •\tStep 1c: Compute delta_time_seconds as the difference between first_time_stamp and post_time in seconds using unix_timestamp.\n",
    "\n",
    "2.\tAdjust time_stamp to time_stamp_local:\n",
    "\n",
    ">.   •\tObjective: Convert GPS time_stamp from UTC to local time using the computed delta_time_seconds.\n",
    "\n",
    ">.   •\tImplementation: Subtract delta_time_seconds from time_stamp to obtain time_stamp_local.\n",
    "\t\n",
    "3.\tCompute sec_time_stamp:\n",
    "\n",
    ">    •\tObjective: Assign a local timestamp to each sectional by adding cumulative sectional_time to the race’s start time (min_time_stamp_local).\n",
    "\n",
    ">    •\tImplementation:\n",
    "\t\n",
    ">    •\tStep 3a: Group gps_df by race identifiers and compute the minimum time_stamp_local (min_time_stamp_local) for each race.\n",
    "\n",
    ">    •\tStep 3b: Join min_time_df with sectionals_df to associate each sectional with the race’s start time.\n",
    "\n",
    ">    •\tStep 3c: Define a window specification to sort sectionals_df by gate_numeric within each race.\n",
    "\n",
    ">    •\tStep 3d: Compute the cumulative sum of sectional_time (sectional_time_cumulative) ordered by gate_numeric.\n",
    "\n",
    ">    •\tStep 3e: Create sec_time_stamp by adding sectional_time_cumulative to min_time_stamp_local.\n",
    "\n",
    "4.\tCleanup:\n",
    "\n",
    ">    •\tObjective: Remove intermediate columns that are no longer needed to maintain a clean DataFrame.\n",
    "\n",
    ">    •\tImplementation: Drop columns such as min_time_stamp_local, sectional_time_cumulative, and delta_time_seconds.\n",
    "\n",
    "5.\tValidation:\n",
    "\n",
    ">    •\tObjective: Ensure that sec_time_stamp has been correctly computed.\n",
    "\n",
    ">    •\tImplementation: Display a sample of the sec_time_stamp values to verify correctness.\n",
    "\n",
    "Recap of What Was Done\n",
    "\n",
    "\t1.\tGrouped and Sorted DataFrames:\n",
    "\t•\tBoth gps_df and sectionals_df were grouped by race identifiers (course_cd, race_date, race_number, saddle_cloth_number).\n",
    "\t•\tgps_df was sorted by time_stamp to identify the earliest timestamp per race.\n",
    "\t2.\tCalculated Time Zone Offset (delta_time_seconds):\n",
    "\t•\tComputed the difference between the earliest GPS time_stamp (in UTC) and post_time (scheduled race start time in local time) for each race.\n",
    "\t•\tThis difference represents the time zone offset in seconds.\n",
    "\t3.\tAdjusted time_stamp to time_stamp_local:\n",
    "\t•\tSubtracted delta_time_seconds from each time_stamp to obtain time_stamp_local, representing the local time.\n",
    "\t4.\tComputed sec_time_stamp:\n",
    "\t•\tCalculated the cumulative sum of sectional_time ordered by gate_numeric within each race.\n",
    "\t•\tAdded this cumulative sectional_time to the race’s start time (min_time_stamp_local) to assign a precise local timestamp (sec_time_stamp) to each sectional.\n",
    "\t5.\tCleaned Up DataFrame:\n",
    "\t•\tRemoved intermediate columns to maintain a focused and clean sectionals_df.\n",
    "\t6.\tValidation:\n",
    "\t•\tDisplayed a sample of sec_time_stamp values to ensure correctness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df28848",
   "metadata": {},
   "source": [
    "### Next Steps: Merging the Datasets\n",
    "\n",
    "#### Implementation Steps\n",
    "\n",
    "We’ll implement this strategy using Spark’s DataFrame operations and window functions.\n",
    "\n",
    "1. Prepare Sectionals DataFrame\n",
    "\n",
    "2. Ensure that each sectional has a unique identifier. This helps in managing joins and window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690d5977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Add a unique identifier to each sectional\n",
    "sectionals_df = sectionals_df.withColumn(\"sectional_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c9693",
   "metadata": {},
   "source": [
    "### Join Sectionals with GPS Points Within ±1 Second\n",
    "\n",
    "We’ll perform a range join where GPS points are within ±1 second of the sectional’s sec_time_stamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a43cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import timedelta\n",
    "\n",
    "# Define the UDF to subtract hours from a timestamp\n",
    "def subtract_hours(ts, dh):\n",
    "    if ts is None or dh is None:\n",
    "        return None\n",
    "    return ts - timedelta(hours=dh)\n",
    "\n",
    "# Register the UDF\n",
    "subtract_hours_udf = udf(subtract_hours, TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ec4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Subtract 'delta_hours' hours from 'time_stamp' to get 'time_stamp_local' using the UDF\n",
    "gps_df = gps_df.withColumn(\n",
    "    \"time_stamp_local\",\n",
    "    subtract_hours_udf(col(\"time_stamp\"), col(\"delta_hours\"))\n",
    ").drop(\"delta_seconds\", \"delta_hours\")\n",
    "\n",
    "# Optional: Select relevant columns to verify the transformation\n",
    "gps_df.select(\"time_stamp\", \"post_time\", \"time_stamp_local\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c1099d",
   "metadata": {},
   "source": [
    "### Removed all time stamps other than time_stamp which is now converted to local time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9626a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Step 5: Set 'time_stamp' to 'time_stamp_local' and remove 'post_time' and 'time_stamp_local'\n",
    "gps_df = gps_df.withColumn(\"time_stamp\", col(\"time_stamp_local\")) \\\n",
    "               .drop(\"post_time\", \"time_stamp_local\")\n",
    "\n",
    "# Optional: Verify the transformation\n",
    "gps_df.select(\"time_stamp\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b49e22",
   "metadata": {},
   "source": [
    "### Group and Sort gps_df and sectionals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70fe0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define race identifier columns\n",
    "race_id_cols = [\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"]\n",
    "\n",
    "# Step 1: Group and Sort `gps_df` by race and `time_stamp`\n",
    "gps_sorted_df = gps_df.orderBy(\n",
    "    *[col(col_name).asc() for col_name in race_id_cols],\n",
    "    col(\"time_stamp\").asc()\n",
    ")\n",
    "\n",
    "# Optional: Verify the sorting\n",
    "gps_sorted_df.select(*race_id_cols, \"time_stamp\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Group and Sort `sectionals_df` by race and `gate_numeric`\n",
    "sectionals_sorted_df = sectionals_df.orderBy(\n",
    "    *[col(col_name).asc() for col_name in race_id_cols],\n",
    "    col(\"gate_numeric\").asc()\n",
    ")\n",
    "\n",
    "# Optional: Verify the sorting\n",
    "sectionals_sorted_df.select(*race_id_cols, \"gate_numeric\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbc771a",
   "metadata": {},
   "source": [
    "1.\tCreate the sec_time_stamp column in sectionals_df.\n",
    "\n",
    "2.\tFor each race, take the first time_stamp from gps_df, add the first sectional_time to it, and populate the sec_time_stamp column.\n",
    "\n",
    "3.\tContinue adding sectional_time to the previous sec_time_stamp for each subsequent row within the race.\n",
    "\n",
    "4.\tApply this process for every race in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62debd6e",
   "metadata": {},
   "source": [
    "### Change precision of time stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a UDF that takes a timestamp and a number of seconds\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import timedelta\n",
    "\n",
    "# Define the UDF to add seconds (including fractional seconds) to a timestamp\n",
    "def add_seconds(ts, seconds):\n",
    "    if ts is None or seconds is None:\n",
    "        return None\n",
    "    return ts + timedelta(seconds=seconds)\n",
    "\n",
    "# Register the UDF\n",
    "add_seconds_udf = udf(add_seconds, TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af61c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 and 6: Compute 'sec_time_stamp' using the UDF and preserve sub-second precision\n",
    "sectionals_final_df = sectionals_with_cum_sum_df.withColumn(\n",
    "    \"sec_time_stamp\",\n",
    "    add_seconds_udf(col(\"base_time\"), col(\"cumulative_sectional_time\"))\n",
    ").drop(\"base_time\", \"cumulative_sectional_time\", \"sec_time_stamp_unix\")\n",
    "\n",
    "# Step 7: Verify the result by displaying sample data\n",
    "sectionals_final_df.select(\n",
    "    *race_id_cols, \"gate_numeric\", \"sectional_time\", \"sec_time_stamp\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a878d402",
   "metadata": {},
   "source": [
    "### Merge on time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc44ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define race identifier columns\n",
    "race_id_cols = [\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"]\n",
    "\n",
    "# Step 1.1: Sort `gps_df` by race identifiers and `time_stamp` in ascending order\n",
    "gps_sorted_df = gps_df.orderBy(\n",
    "    *[col(column).asc() for column in race_id_cols],\n",
    "    col(\"time_stamp\").asc()\n",
    ")\n",
    "\n",
    "# Step 1.2: Sort `sectionals_df` by race identifiers and `sec_time_stamp` in ascending order\n",
    "sectionals_sorted_df = sectionals_df.orderBy(\n",
    "    *[col(column).asc() for column in race_id_cols],\n",
    "    col(\"sec_time_stamp\").asc()\n",
    ")\n",
    "\n",
    "# Optional: Verify the sorting by displaying the first few rows of each DataFrame\n",
    "print(\"Sorted GPS DataFrame:\")\n",
    "gps_sorted_df.select(*race_id_cols, \"time_stamp\").show(5, truncate=False)\n",
    "\n",
    "print(\"Sorted Sectionals DataFrame:\")\n",
    "sectionals_sorted_df.select(*race_id_cols, \"sec_time_stamp\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e883fbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import abs, col\n",
    "\n",
    "# Define race identifier columns\n",
    "race_id_cols = [\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"]\n",
    "\n",
    "# Step 2.1: Convert 'time_stamp' and 'sec_time_stamp' to milliseconds since epoch to preserve sub-second precision\n",
    "gps_with_ms = gps_sorted_df.withColumn(\n",
    "    \"time_stamp_ms\",\n",
    "    (col(\"time_stamp\").cast(\"double\") * 1000).cast(\"long\")\n",
    ")\n",
    "\n",
    "sectionals_with_ms = sectionals_sorted_df.withColumn(\n",
    "    \"sec_time_stamp_ms\",\n",
    "    (col(\"sec_time_stamp\").cast(\"double\") * 1000).cast(\"long\")\n",
    ")\n",
    "\n",
    "# Step 2.2: Define the join condition with time window (±1000 milliseconds)\n",
    "join_condition = (\n",
    "    (gps_with_ms.course_cd == sectionals_with_ms.course_cd) &\n",
    "    (gps_with_ms.race_date == sectionals_with_ms.race_date) &\n",
    "    (gps_with_ms.race_number == sectionals_with_ms.race_number) &\n",
    "    (gps_with_ms.saddle_cloth_number == sectionals_with_ms.saddle_cloth_number) &\n",
    "    (abs(gps_with_ms.time_stamp_ms - sectionals_with_ms.sec_time_stamp_ms) <= 1000)\n",
    ")\n",
    "\n",
    "# Step 2.3: Perform the left join based on the join condition\n",
    "matched_df = gps_with_ms.join(\n",
    "    sectionals_with_ms,\n",
    "    on=join_condition,\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    gps_with_ms[\"*\"],\n",
    "    sectionals_with_ms[\"sec_time_stamp\"],\n",
    "    sectionals_with_ms[\"gate_numeric\"],\n",
    "    sectionals_with_ms[\"sectional_time\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08207bf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Verify the matched records\n",
    "print(\"Sample of matched_df (All gps_df records with sectional data where available):\")\n",
    "matched_df.select(\n",
    "    *race_id_cols,\n",
    "    \"time_stamp\",\n",
    "    \"sec_time_stamp\",\n",
    "    \"gate_numeric\",\n",
    "    \"sectional_time\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6639b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Show the total number of rows in matched_df, gps_df, and sectionals_df\n",
    "\n",
    "# Count of matched_df\n",
    "matched_count = matched_df.count()\n",
    "print(f\"Total number of rows in matched_df: {matched_count}\")\n",
    "\n",
    "# Count of gps_df\n",
    "gps_count = gps_df.count()\n",
    "print(f\"Total number of rows in gps_df: {gps_count}\")\n",
    "\n",
    "# Count of sectionals_df\n",
    "sectionals_count = sectionals_df.count()\n",
    "print(f\"Total number of rows in sectionals_df: {sectionals_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f566ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Show the number of unmatched sectional records\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Step 2.1: Extract distinct sectional records from matched_df\n",
    "matched_sectionals = matched_df.select(\n",
    "    *race_id_cols,\n",
    "    \"sec_time_stamp\"\n",
    ").distinct()\n",
    "\n",
    "# Step 2.2: Perform a left anti join to find sectionals in sectionals_df not present in matched_sectionals\n",
    "unmatched_sectionals_df = sectionals_sorted_df.join(\n",
    "    matched_sectionals,\n",
    "    on=race_id_cols + [\"sec_time_stamp\"],\n",
    "    how=\"leftanti\"\n",
    ")\n",
    "\n",
    "# Step 2.3: Count the number of unmatched sectional records\n",
    "unmatched_sectionals_count = unmatched_sectionals_df.count()\n",
    "print(f\"Number of unmatched sectional records: {unmatched_sectionals_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b5833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127d6c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9709ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e5567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1ba9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e2257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e570337f",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "> •\tAliases: sec for sectionals_df and gps for gps_df to simplify column references.\n",
    "\n",
    "> •\tJoin Conditions:\n",
    "\n",
    "> •\tRace Identifiers: Ensure that sectionals and GPS points belong to the same race.\n",
    "\n",
    "> •\tTime Window: GPS time_stamp_local must be within ±1 second of sec_time_stamp.\n",
    "\n",
    "#### Calculate Absolute Time Difference\n",
    "\n",
    "Compute the absolute difference between sec_time_stamp and time_stamp_local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6acf796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import abs as spark_abs, unix_timestamp\n",
    "\n",
    "# Calculate absolute time difference in seconds\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"time_diff\",\n",
    "    spark_abs(unix_timestamp(col(\"sec.sec_time_stamp\")) - unix_timestamp(col(\"gps.time_stamp_local\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f14c9",
   "metadata": {},
   "source": [
    "#### Assign the Closest GPS Point to Each Sectional\n",
    "\n",
    "Use window functions to rank GPS points based on time_diff and select the top-ranked GPS point for each sectional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caec620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Define window specification to rank GPS points within each sectional\n",
    "window_spec = Window.partitionBy(\"sec.sectional_id\").orderBy(\"time_diff\")\n",
    "\n",
    "# Assign row numbers based on time difference\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"rank\",\n",
    "    row_number().over(window_spec)\n",
    ")\n",
    "\n",
    "# Filter to keep only the closest GPS point (rank = 1)\n",
    "matched_sectionals_df = joined_df.filter(col(\"rank\") == 1).select(\n",
    "    \"sec.*\",\n",
    "    \"gps.time_stamp_local\",\n",
    "    \"gps.longitude\",\n",
    "    \"gps.latitude\",\n",
    "    \"gps.speed\",\n",
    "    \"gps.progress\",\n",
    "    \"gps.stride_frequency\",\n",
    "    \"gps.post_time\",\n",
    "    \"gps.location\",\n",
    "    \"gps.time_stamp\",\n",
    "    \"time_diff\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6d458",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "> •\tWindow Specification: For each sectional_id, rank GPS points based on the smallest time_diff.\n",
    "\n",
    "> •\tRow Number: Assigns a unique row number starting from 1 within each window.\n",
    "\t\n",
    "> •\tFiltering: Retain only the GPS point with rank = 1, i.e., the closest GPS point.\n",
    "\n",
    "#### Handle Sectionals Without Matching GPS Points\n",
    "\n",
    "Identify sectionals that did not find any matching GPS point within the ±1-second window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all sectionals\n",
    "all_sectionals_df = sectionals_df.select(\"sectional_id\")\n",
    "\n",
    "# Find matched sectionals\n",
    "matched_sectionals_ids = matched_sectionals_df.select(\"sectional_id\")\n",
    "\n",
    "# Identify unmatched sectionals\n",
    "unmatched_sectionals_df = all_sectionals_df.join(\n",
    "    matched_sectionals_ids,\n",
    "    on=\"sectional_id\",\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "\n",
    "# Optionally, log the number of unmatched sectionals\n",
    "unmatched_count = unmatched_sectionals_df.count()\n",
    "logging.warning(f\"There are {unmatched_count} sectionals without a matching GPS point within ±1 second.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b714c069",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "> •\tLeft Anti Join: Retrieves sectionals that do not have a corresponding entry in matched_sectionals_df.\n",
    "\n",
    "> •\tLogging: Alerts you to the number of sectionals without a match, enabling further investigation if necessary.\n",
    "\n",
    "#### Finalize the Matched Sectionals DataFrame\n",
    "\n",
    "Clean up the matched_sectionals_df to include only necessary columns and make it ready for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6275c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and rename columns as needed\n",
    "final_matched_sectionals_df = matched_sectionals_df.select(\n",
    "    \"sectional_id\",\n",
    "    \"course_cd\",\n",
    "    \"race_date\",\n",
    "    \"race_number\",\n",
    "    \"saddle_cloth_number\",\n",
    "    \"gate_name\",\n",
    "    \"gate_numeric\",\n",
    "    \"length_to_finish\",\n",
    "    \"sectional_time\",\n",
    "    \"running_time\",\n",
    "    \"distance_back\",\n",
    "    \"distance_ran\",\n",
    "    \"number_of_strides\",\n",
    "    \"sec_time_stamp\",\n",
    "    \"time_stamp_local\",    # GPS local time\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"speed\",\n",
    "    \"progress\",\n",
    "    \"stride_frequency\",\n",
    "    \"post_time\",\n",
    "    \"location\",\n",
    "    \"time_stamp\",          # Original GPS timestamp (UTC)\n",
    "    \"time_diff\"\n",
    ")\n",
    "\n",
    "# Display sample matched sectionals\n",
    "logging.info(\"Displaying sample matched sectionals with GPS data:\")\n",
    "final_matched_sectionals_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f791c",
   "metadata": {},
   "source": [
    "Unmatched sectionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f1f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample unmatched sectionals\n",
    "if unmatched_count > 0:\n",
    "    logging.warning(\"Displaying sample unmatched sectionals:\")\n",
    "    unmatched_sectionals_df.select(\"sectional_id\", \"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\", \"gate_name\", \"gate_numeric\", \"sectional_time\", \"sec_time_stamp\").show(10, truncate=False)\n",
    "else:\n",
    "    logging.info(\"All sectionals have been successfully matched with GPS points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74720e0",
   "metadata": {},
   "source": [
    "### 3. Merge with gps_df Based on Time Alignment\n",
    "\n",
    "To integrate GPS data, you’ll need to align GPS points with the sectional times. This typically involves matching GPS timestamps with the corresponding sec_time_stamp to determine which sectional a GPS point belongs to.\n",
    "\n",
    "Approach: Range Join Using Spark SQL\n",
    "\n",
    "Spark doesn’t natively support range joins (joins based on a range of values), but you can achieve this using Spark SQL or broadcast joins with window functions. Here’s a method using window functions to assign each GPS point to the nearest sec_time_stamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import abs as spark_abs\n",
    "\n",
    "# Assuming gps_df has 'time_stamp_local' and sectionals_df has 'sec_time_stamp'\n",
    "\n",
    "# First, join GPS points with sectional times based on race identifiers\n",
    "logging.info(\"Joining 'merged_results_sectionals' with 'gps_df' based on race identifiers...\")\n",
    "\n",
    "joined_df = merged_results_sectionals.join(\n",
    "    gps_df,\n",
    "    on=[\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "logging.info(\"Joining complete. Now aligning GPS points with sectional times...\")\n",
    "\n",
    "# Define a window partitioned by race and ordered by time_stamp_local\n",
    "window_spec = Window.partitionBy(\n",
    "    \"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"\n",
    ").orderBy(\"sec_time_stamp\")\n",
    "\n",
    "# Add a column that holds the closest sec_time_stamp for each GPS point\n",
    "from pyspark.sql.functions import lag, lead\n",
    "\n",
    "# For each GPS point, find the previous and next sec_time_stamp\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"prev_sec_time_stamp\",\n",
    "    lag(\"sec_time_stamp\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"next_sec_time_stamp\",\n",
    "    lead(\"sec_time_stamp\").over(window_spec)\n",
    ")\n",
    "\n",
    "# Define logic to assign GPS points to the nearest sectional\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Assign GPS points to the nearest sectional based on time\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"assigned_sec_time_stamp\",\n",
    "    when(\n",
    "        (col(\"time_stamp_local\") >= col(\"prev_sec_time_stamp\")) & (col(\"time_stamp_local\") < col(\"sec_time_stamp\")),\n",
    "        col(\"sec_time_stamp\")\n",
    "    ).otherwise(col(\"next_sec_time_stamp\"))\n",
    ")\n",
    "\n",
    "# Clean up intermediate columns\n",
    "joined_df = joined_df.drop(\"prev_sec_time_stamp\", \"next_sec_time_stamp\")\n",
    "\n",
    "logging.info(\"GPS points aligned with sectional times successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b11f3",
   "metadata": {},
   "source": [
    "### 4. Validate the Merged Data\n",
    "\n",
    "It’s crucial to ensure that the merging process has correctly aligned the GPS points with the sectional times. Here are some steps to validate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db11b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "logging.info(\"Displaying sample of the merged data...\")\n",
    "joined_df.select(\n",
    "    \"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\",\n",
    "    \"official_fin\", \"finish_time\", \"speed_rating\", \"sec_time_stamp\",\n",
    "    \"time_stamp_local\", \"longitude\", \"latitude\", \"speed\",\n",
    "    \"assigned_sec_time_stamp\", \"gate_numeric\", \"sec_time_stamp\"\n",
    ").show(30, truncate=False)\n",
    "\n",
    "# Check for any nulls in 'assigned_sec_time_stamp'\n",
    "logging.info(\"Checking for nulls in 'assigned_sec_time_stamp'...\")\n",
    "null_count = joined_df.filter(col(\"assigned_sec_time_stamp\").isNull()).count()\n",
    "if null_count > 0:\n",
    "    logging.warning(f\"There are {null_count} GPS points without an assigned sectional time stamp.\")\n",
    "else:\n",
    "    logging.info(\"All GPS points have been successfully assigned to sectional time stamps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfc3b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to display the shape of a Spark DataFrame\n",
    "def display_shape(df, name):\n",
    "    count = df.count()\n",
    "    columns = len(df.columns)\n",
    "    print(f\"{name} DataFrame Shape: ({count}, {columns})\")\n",
    "\n",
    "# Display shapes of all DataFrames\n",
    "display_shape(results_df, \"results_df\")\n",
    "display_shape(sectionals_df, \"sectionals_df\")\n",
    "display_shape(gps_df, \"gps_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9870ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_shape(joined_df, \"joined_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76311a1",
   "metadata": {},
   "source": [
    "### 5. Save the Final Merged DataFrame\n",
    "\n",
    "Once validated, save the final merged DataFrame for downstream analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78dd4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged DataFrame\n",
    "final_output_path = os.path.join(parquet_dir, \"final_merged_data.parquet\")\n",
    "logging.info(f\"Saving final merged DataFrame to Parquet at {final_output_path}...\")\n",
    "joined_df.write.mode(\"overwrite\").parquet(final_output_path)\n",
    "logging.info(\"Final merged data saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f55978b",
   "metadata": {},
   "source": [
    "### 6. Optional: Optimize for Performance\n",
    "\n",
    "Depending on the size of your datasets, you might want to optimize the performance of your Spark jobs:\n",
    "\t•\tCaching: If you need to reuse certain DataFrames multiple times, consider caching them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results_sectionals.cache()\n",
    "gps_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbca914",
   "metadata": {},
   "source": [
    "Partitioning: Repartition your DataFrames based on high-cardinality columns to optimize join operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0917b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results_sectionals = merged_results_sectionals.repartition(\"race_number\")\n",
    "gps_df = gps_df.repartition(\"race_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a368a8",
   "metadata": {},
   "source": [
    "\t•\tBroadcast Joins: For smaller DataFrames, you can use broadcast joins to speed up the join process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac05657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "merged_df = merged_results_sectionals.join(\n",
    "    broadcast(gps_df),\n",
    "    on=[\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"],\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6fe883",
   "metadata": {},
   "source": [
    "### 7. Summary of the Merging Process\n",
    "\n",
    "\t1.\tInitial Join: Merge results_df with sectionals_df based on race identifiers.\n",
    "\t2.\tGPS Alignment: Assign each GPS point to the nearest sec_time_stamp using window functions.\n",
    "\t3.\tValidation: Ensure that all GPS points are correctly aligned without missing assignments.\n",
    "\t4.\tFinal Save: Persist the merged and enriched DataFrame for further use.\n",
    "\n",
    "### 8. Final Recommendations\n",
    "\n",
    "\t•\tMonitor Performance: Use Spark’s UI (http://<driver-node>:4040) to monitor job execution and identify any performance bottlenecks.\n",
    "\t•\tHandle Edge Cases: Ensure that GPS points falling outside the range of sectional times are handled appropriately, possibly by assigning them to the nearest sectional or flagging them for review.\n",
    "\t•\tDocumentation: Keep detailed logs and documentation of each step to facilitate debugging and future maintenance.\n",
    "\n",
    "Congratulations!\n",
    "\n",
    "You’ve successfully set up a scalable and efficient data pipeline using PySpark, PostgreSQL, SQL, and Parquet. This architecture is well-suited for handling large datasets and performing complex transformations necessary for your horse racing analytics and modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3703ec1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbc77d5c",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "## Sample data initially\n",
    "\n",
    "Taking a random sample will not work for time series as was attempted, but taking a smaller sample by filtering on date should work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a4907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for a specific date range or course\n",
    "#df_filtered = merged_df[merged_df['race_date'] >= '2024-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab39f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3497607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_filtered.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64664ca4",
   "metadata": {},
   "source": [
    "## Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b96e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Heatmap of missing values (on a small sample)\n",
    "#sns.heatmap(df.isnull(), cbar=False)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef399eb",
   "metadata": {},
   "source": [
    "## Imputation for Stride Frequency and number_of_strides\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5765470",
   "metadata": {},
   "source": [
    "#### Group-Based Imputation: Impute based on groups, such as per horse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d22ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stride_frequency'] = df.groupby('saddle_cloth_number')['stride_frequency'].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f917dd",
   "metadata": {},
   "source": [
    "#### gate_numeric remains the same within a group until changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea16726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['gate_numeric'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8464e",
   "metadata": {},
   "source": [
    "#### Interprolation distance_back changes over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bafc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['distance_back'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465eb678",
   "metadata": {},
   "source": [
    "#### Group Based Imputation for number of strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['number_of_strides'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc1d8b",
   "metadata": {},
   "source": [
    "## Choose Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ffa9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d11bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f5fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'speed',\n",
    "    'progress',\n",
    "    'stride_frequency',\n",
    "    'number_of_strides',\n",
    "    'post_pos',\n",
    "    'gate_numeric',\n",
    "    'length_to_finish',\n",
    "    'sectional_time',\n",
    "    'running_time',\n",
    "    'distance_back',\n",
    "    'distance_ran'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb71eed",
   "metadata": {},
   "source": [
    "## Feature Engineering -- calculate additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33596bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['course_cd', 'race_date', 'race_number', 'saddle_cloth_number', 'time_stamp'], inplace=True)\n",
    "df['acceleration'] = df.groupby(\n",
    "    ['course_cd', 'race_date', 'race_number', 'saddle_cloth_number']\n",
    ")['speed'].diff() / df.groupby(\n",
    "    ['course_cd', 'race_date', 'race_number', 'saddle_cloth_number']\n",
    ")['time_stamp'].diff().dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02a724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['acceleration'] = df['acceleration'].replace([np.inf, -np.inf], np.nan)\n",
    "df['acceleration'] = df['acceleration'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d74bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns.append('acceleration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b93a8a",
   "metadata": {},
   "source": [
    "## Scale Features\n",
    "\n",
    "Scaling helps in training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Scaling should be done after sequences are created to avoid data leakage.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752c49a",
   "metadata": {},
   "source": [
    "## Create Sequences for LSTM\n",
    "\n",
    "a. Group Data\n",
    "\n",
    "Group the data to create sequences for each horse in each race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0fd29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_columns = ['course_cd', 'race_date', 'race_number', 'saddle_cloth_number']\n",
    "groups = df_sampled.groupby(group_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381cd694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f8bfdbd",
   "metadata": {},
   "source": [
    "##  Create Sequences and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c3e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for name, group in groups:\n",
    "    # Ensure group is sorted by time\n",
    "    group = group.sort_values('time_stamp')\n",
    "\n",
    "    # Extract features\n",
    "    features = group[feature_columns].values\n",
    "\n",
    "    # Append the sequence\n",
    "    sequences.append(features)\n",
    "\n",
    "    # Extract label (official finishing position)\n",
    "    label = group['official_fin'].iloc[0]  # Assuming it's the same for all entries in the group\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e7000",
   "metadata": {},
   "source": [
    "## Determine max_seq_length and num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9175f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Alternatively, set a fixed max_seq_length to limit memory usage.\n",
    "max_seq_length = max(len(seq) for seq in sequences)\n",
    "num_features = len(feature_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_seq_length)\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa5185",
   "metadata": {},
   "source": [
    "## Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d49e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, maxlen=max_seq_length, padding='post', dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa80b133",
   "metadata": {},
   "source": [
    "## Convert Labels\n",
    "\n",
    "Adjust labels to start from 0 if they start from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2dd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels).astype(int) - 1\n",
    "num_classes = labels.max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8066c970",
   "metadata": {},
   "source": [
    "## Scale Features\n",
    "\n",
    "Now, scale the features. Be cautious to fit the scaler only on the training data to prevent data leakage.\n",
    "\n",
    "Flatten sequences for scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1131fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = padded_sequences.shape[0]\n",
    "X_flat = padded_sequences.reshape(-1, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13668f1",
   "metadata": {},
   "source": [
    "## Fit scaler on the flattened data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_flat = scaler.fit_transform(X_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d826ea",
   "metadata": {},
   "source": [
    "## Reshape back to original shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6450ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = X_scaled_flat.reshape(num_samples, max_seq_length, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9aa653",
   "metadata": {},
   "source": [
    "# Split Data into Training, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987873f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume sequences and labels have been created and padded_sequences is available\n",
    "\n",
    "# Convert labels\n",
    "labels = np.array(labels).astype(int) - 1\n",
    "num_classes = labels.max() + 1\n",
    "\n",
    "# Split data\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    padded_sequences, labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Check shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Flatten training data and fit scaler\n",
    "num_samples_train = X_train.shape[0]\n",
    "X_train_flat = X_train.reshape(-1, num_features)\n",
    "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
    "X_train_scaled = X_train_scaled_flat.reshape(num_samples_train, max_seq_length, num_features)\n",
    "\n",
    "# Scale validation data\n",
    "num_samples_val = X_val.shape[0]\n",
    "X_val_flat = X_val.reshape(-1, num_features)\n",
    "X_val_scaled_flat = scaler.transform(X_val_flat)\n",
    "X_val_scaled = X_val_scaled_flat.reshape(num_samples_val, max_seq_length, num_features)\n",
    "\n",
    "# Scale test data\n",
    "num_samples_test = X_test.shape[0]\n",
    "X_test_flat = X_test.reshape(-1, num_features)\n",
    "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
    "X_test_scaled = X_test_scaled_flat.reshape(num_samples_test, max_seq_length, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92557e9",
   "metadata": {},
   "source": [
    "Ensure that X_train, X_val, X_test, y_train, y_val, and y_test are correctly shaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a4499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520c7e1",
   "metadata": {},
   "source": [
    "# Prepare Data for Model Training\n",
    "\n",
    "## Training the LSTM Model\n",
    "\n",
    "### Build the Model\n",
    "\n",
    "This model combines dropout, regularization, and normalization for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ef1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model_lstm = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(max_seq_length, num_features)),\n",
    "    tf.keras.layers.Masking(mask_value=0.0),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "        256, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(1e-4))),\n",
    "    tf.keras.layers.LayerNormalization(),    \n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "        128, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(1e-4))),\n",
    "    tf.keras.layers.LayerNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "        64, kernel_regularizer=tf.keras.regularizers.l2(1e-4))),\n",
    "    tf.keras.layers.LayerNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "#model_lstm = tf.keras.Sequential([\n",
    "#    tf.keras.Input(shape=(max_seq_length, num_features)),\n",
    "#    tf.keras.layers.Masking(mask_value=0.0),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "#    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "#])\n",
    "\n",
    "#model_lstm = tf.keras.Sequential([\n",
    "#    tf.keras.Input(shape=(max_seq_length, num_features)),\n",
    "#    tf.keras.layers.Masking(mask_value=0.0),\n",
    "#    tf.keras.layers.LSTM(128),\n",
    "#    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "#])\n",
    "\n",
    "#model_lstm = tf.keras.Sequential([\n",
    "#    tf.keras.Input(shape=(max_seq_length, num_features)),\n",
    "#    tf.keras.layers.Masking(mask_value=0.0),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "#    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "#])\n",
    "\n",
    "#988/988 ━━━━━━━━━━━━━━━━━━━━ 7s 7ms/step - accuracy: 0.3606 - loss: 1.6184\n",
    "#Test Loss: 1.6182985305786133, Test Accuracy: 0.36063656210899353"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d55364",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0502b82b",
   "metadata": {},
   "source": [
    "RMSprop is often a good choice for RNNs.\n",
    "\n",
    ">\t•\tThe learning rate of 0.001 is a typical starting point.\n",
    "\n",
    ">   •\tRecommendation:\n",
    "\n",
    ">   •\tYou can experiment with different learning rates (e.g., 0.0005, 0.0001) if needed.\n",
    "\n",
    ">   •\tAlternatively, you can also try the Adam optimizer and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimenting with different learning rates (e.g., 0.0005, 0.0001) to see if it affects convergence.\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "model_lstm.compile(\n",
    "    optimizer=optimizer,   # 'adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'] #,tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133892ef",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b320083",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "> Learning Rate Scheduler and Early Stopping\n",
    "\n",
    "> * Learning Rate Scheduler\n",
    "\n",
    ">  * Earlystopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914da2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb51d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_lstm.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,  \n",
    "    batch_size=128,  # 64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[\n",
    "        lr_scheduler, \n",
    "        early_stopping,\n",
    "        model_checkpoint\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa1f52",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fce6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_lstm.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae00699",
   "metadata": {},
   "source": [
    "## Plot Training and Validation Loss and Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ea95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb32d9b",
   "metadata": {},
   "source": [
    "### Check for Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e62123",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf8e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(unique, counts)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dec097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd02bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48bdbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e32c3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3efda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd412208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08664aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33143647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ef749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c30501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df[['speed', 'progress', 'stride_frequency', 'longitude', 'latitude', 'post_pos', 'official_fin']].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a5d93-705d-42e3-956a-9cb9dbc4178a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define your variables\n",
    "max_seq_length = 120  # Replace with your maximum sequence length\n",
    "num_features = 5      # Replace with the actual number of features in your data\n",
    "num_classes = 12      # Replace with the actual number of classes\n",
    "\n",
    "# Build your model\n",
    "model_lstm = tf.keras.Sequential()\n",
    "model_lstm.add(tf.keras.layers.Masking(mask_value=0., input_shape=(max_seq_length, num_features)))\n",
    "model_lstm.add(tf.keras.layers.LSTM(128))\n",
    "model_lstm.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dddd20-cbb3-4444-98d7-10f3931ef518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into dataframe:\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02ef34-d54b-4619-bf4b-c2128e9bc171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "history_lstm = model_lstm.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d5e01-6b93-41af-af61-7ee2fa5827c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Combining the Models\n",
    "\n",
    "To create an ensemble, you can combine the predictions of these models in several ways:\n",
    "\t1.\tAveraging Probabilities:\n",
    "\t•\tObtain probability distributions over finishing positions from each model.\n",
    "\t•\tAverage the probabilities across models to get the final prediction.\n",
    "\t2.\tWeighted Averaging:\n",
    "\t•\tAssign weights to each model based on validation performance.\n",
    "\t•\tCompute a weighted average of the probabilities.\n",
    "\t3.\tStacking (Meta-Learner):\n",
    "\t•\tUse the predictions from the individual models as input features to a meta-model (e.g., a logistic regression or another neural network).\n",
    "\t•\tThe meta-model learns how to best combine the individual predictions.\n",
    "\t4.\tVoting (for Classification):\n",
    "\t•\tIf treating the problem as classification into discrete positions, use majority voting among the models.\n",
    "\t•\tNot as suitable if you need probability distributions.\n",
    "\n",
    "Implementation Steps\n",
    "\n",
    "1. Data Preparation\n",
    "\n",
    "\t•\tSequences:\n",
    "\t•\tUse the raw GPS data (gpspoint) to create sequences for each horse in each race.\n",
    "\t•\tEnsure that sequences are properly sorted by time_stamp.\n",
    "\t•\tFeatures:\n",
    "\t•\tInclude raw features such as speed, progress, stride_frequency.\n",
    "\t•\tAvoid hand-engineering features like acceleration to adhere to your objective.\n",
    "\t•\tLabels:\n",
    "\t•\tUse official_fin from results_entries as the target variable.\n",
    "\t•\tSince you want probabilities for each finishing position, consider encoding official_fin as categorical labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdede23-c3ae-4f35-9620-a281a6c599c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
