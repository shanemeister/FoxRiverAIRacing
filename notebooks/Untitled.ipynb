{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277e9121-0ed4-44f3-b848-ba98dc6ce795",
   "metadata": {},
   "source": [
    "# Proposed Ensemble Models\n",
    "\n",
    "Given the constraints and objectives, I recommend considering the following models for the ensemble:\n",
    "\t\n",
    "    1.\tModel 1: LSTM Network on Raw GPS Data\n",
    "    \n",
    ">•\tInput Data: Sequences of raw GPS data (speed, progress, stride_frequency, etc.).\n",
    "\n",
    ">•\tArchitecture: An LSTM network designed to capture temporal dependencies and patterns in the sequential data.\n",
    "\n",
    ">•\tAdvantage: LSTMs are well-suited for time-series data and can learn complex temporal dynamics without the need for hand-engineered features like acceleration.\n",
    "\n",
    "    2.\tModel 2: 1D Convolutional Neural Network (1D-CNN)\n",
    "\t\n",
    ">•\tInput Data: The same raw GPS sequences as in Model 1.\n",
    "\n",
    ">•\tArchitecture: A 1D-CNN that applies convolutional filters across the time dimension to detect local patterns.\n",
    "\n",
    ">•\tAdvantage: CNNs can capture spatial hierarchies and are effective in recognizing patterns in sequences, potentially identifying features like sudden changes in speed or stride frequency.\n",
    "\n",
    "    3.\tModel 3: Transformer-based Model\n",
    "\t\n",
    ">•\tInput Data: Raw GPS sequences and possibly sectionals data.\n",
    "\n",
    ">•\tArchitecture: A Transformer model that uses self-attention mechanisms to weigh the importance of different parts of the sequence.\n",
    "\n",
    ">•\tAdvantage: Transformers can model long-range dependencies and focus on the most relevant parts of the sequence for prediction.\n",
    "\n",
    "## Additional Models (Optional):\n",
    "\n",
    "    4.\tModel 4: Gated Recurrent Unit (GRU) Network\n",
    "\n",
    ">•\tSimilar to LSTMs but with a simpler architecture, GRUs can be more efficient and may perform better on certain datasets.\n",
    "\n",
    ">•\tModel 5: Temporal Convolutional Network (TCN)\n",
    "\n",
    ">•\tTCNs are designed for sequential data and can capture long-term dependencies using causal convolutions and residual connections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144788c4-1601-45af-97bd-2832f1d4d22d",
   "metadata": {},
   "source": [
    "# The LSTM Network on Raw GPS Data\n",
    "\n",
    "Initially I desired to merge the GPS data with Sectionals, but the timestamp and gate_name intervals of each respectively made it difficult to align the data in sequences -- something that is needed for Long-Short Term Memory models. Therefore, it was decided to go with an ensemble approach. There will be additional models that incorporate Equibase data as well, but for the time being, the focus will be on Total Performance GPS data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0647b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import configparser\n",
    "from src.data_ingestion.ingestion_utils import (\n",
    "    get_db_connection, update_tracking, load_processed_files\n",
    ")\n",
    "from src.data_ingestion.eqb_ppData import process_pluspro_data\n",
    "from src.data_ingestion.eqb_resultsCharts import process_resultscharts_data\n",
    "from src.data_ingestion.tpd_datasets import (\n",
    "    process_tpd_sectionals_data,\n",
    "    process_tpd_gpsdata_data\n",
    ")\n",
    "import traceback\n",
    "\n",
    "# Load the configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('/home/exx/myCode/horse-racing/FoxRiverAIRacing/config.ini')\n",
    "\n",
    "# Set up logging for consistent logging behavior in Notebook\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Retrieve database credentials from config file\n",
    "# Retrieve database credentials from config file\n",
    "db_host = config['database']['host']\n",
    "db_port = config['database']['port']\n",
    "db_name = config['database']['dbname']  # Corrected from 'name' to 'dbname'\n",
    "db_user = config['database']['user']\n",
    "\n",
    "# Establish connection using get_db_connection\n",
    "conn = get_db_connection(config)\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(f'postgresql+psycopg2://{db_user}@{db_host}:{db_port}/{db_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78392887",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results = \"\"\"\n",
    "   SELECT course_cd, race_date, race_number, program_num, official_fin, post_pos\n",
    "    FROM v_results_entries\n",
    "    WHERE breed = 'TB';\n",
    "\"\"\"\n",
    "\n",
    "query_sectionals = \"\"\"    \n",
    "    \n",
    "SELECT course_cd, race_date, race_number, saddle_cloth_number, gate_numeric, length_to_finish, \n",
    "    sectional_time, running_time, distance_back, distance_ran, number_of_strides \n",
    "FROM v_sectionals;\n",
    "\"\"\"\n",
    "query_gpspoint = \"\"\"\n",
    "select course_cd, race_date, race_number, saddle_cloth_number, time_stamp, longitude, latitude,\n",
    "    speed, progress, stride_frequency\n",
    "from v_gpspoint;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c8b5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query and load data into a DataFrame\n",
    "gps_df = pd.read_sql_query(query_gpspoint, engine, parse_dates=['time_stamp'])\n",
    "sectionals_df = pd.read_sql_query(query_sectionals, engine)\n",
    "results_df = pd.read_sql_query(query_results, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f2d314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_df['race_date'] = pd.to_datetime(gps_df['race_date'])\n",
    "sectionals_df['race_date'] = pd.to_datetime(sectionals_df['race_date'])\n",
    "results_df['race_date'] = pd.to_datetime(results_df['race_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c51918fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gps_df.to_parquet('/home/exx/myCode/horse-racing/FoxRiverAIRacing/notebooks/data/gps.parquet', index=False)\n",
    "sectionals_df.to_parquet('/home/exx/myCode/horse-racing/FoxRiverAIRacing/notebooks/data/sectionals.parquet', index=False)\n",
    "results_df.to_parquet('/home/exx/myCode/horse-racing/FoxRiverAIRacing/notebooks/data/results.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d7adb",
   "metadata": {},
   "source": [
    "# Start here if using same frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20e1766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Parquet file into a DataFrame\n",
    "gps_df = pd.read_parquet('/home/exx/myCode/horse-racing/FoxRiverAIRacing/notebooks/data/gps.parquet')\n",
    "sectionals_df = pd.read_parquet('/home/exx/myCode/horse-racing/FoxRiverAIRacing/notebooks/data/sectionals.parquet')\n",
    "results_df = pd.read_parquet('/home/exx/myCode/horse-racing/FoxRiverAIRacing/notebooks/data/results.parquet')\n",
    "\n",
    "# Display the DataFrames\n",
    "# print(gps_df.head())\n",
    "# print(sectionals_df.head())\n",
    "# print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a21a1e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48014372, 10)\n",
      "(388056, 6)\n",
      "(5603785, 11)\n"
     ]
    }
   ],
   "source": [
    "print(gps_df.shape)\n",
    "print(results_df.shape)\n",
    "print(sectionals_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7423ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join gps_df with results_df\n",
    "merged_df = gps_df.merge(results_df, \n",
    "                          left_on=['course_cd', 'race_date', 'race_number', 'saddle_cloth_number'], \n",
    "                          right_on=['course_cd', 'race_date', 'race_number', 'program_num'], \n",
    "                          how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9ede2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34194909, 13)\n",
      "(5603785, 11)\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.shape)\n",
    "print(sectionals_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60c1f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the result with sectionals_df\n",
    "merged_df = merged_df.merge(sectionals_df, \n",
    "                             left_on=['course_cd', 'race_date', 'race_number', 'saddle_cloth_number'], \n",
    "                             right_on=['course_cd', 'race_date', 'race_number', 'saddle_cloth_number'], \n",
    "                             how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "406a7957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(488547778, 20)\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b12fe7ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course_cd                      object\n",
      "race_date              datetime64[ns]\n",
      "race_number                     int64\n",
      "saddle_cloth_number            object\n",
      "time_stamp             datetime64[ns]\n",
      "longitude                     float64\n",
      "latitude                      float64\n",
      "speed                         float64\n",
      "progress                      float64\n",
      "stride_frequency              float64\n",
      "program_num                    object\n",
      "official_fin                    int64\n",
      "post_pos                        int64\n",
      "gate_numeric                  float64\n",
      "length_to_finish              float64\n",
      "sectional_time                float64\n",
      "running_time                  float64\n",
      "distance_back                 float64\n",
      "distance_ran                  float64\n",
      "number_of_strides             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3b96e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course_cd                     0\n",
      "race_date                     0\n",
      "race_number                   0\n",
      "saddle_cloth_number           0\n",
      "time_stamp                    0\n",
      "longitude                     0\n",
      "latitude                      0\n",
      "speed                         0\n",
      "progress                      0\n",
      "stride_frequency       41832491\n",
      "program_num                   0\n",
      "official_fin                  0\n",
      "post_pos                      0\n",
      "gate_numeric                  0\n",
      "length_to_finish              0\n",
      "sectional_time                0\n",
      "running_time                  0\n",
      "distance_back              5480\n",
      "distance_ran                  0\n",
      "number_of_strides        242196\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(merged_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a4907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 100000  # Adjust this size based on your memory capacity\n",
    "\n",
    "# Create a copy of the DataFrame to avoid modifying the original during iteration\n",
    "merged_df_copy = merged_df.copy()\n",
    "\n",
    "# Iterate through the DataFrame in chunks\n",
    "for start in range(0, len(merged_df_copy), chunk_size):\n",
    "    end = start + chunk_size\n",
    "    chunk = merged_df_copy.iloc[start:end]\n",
    "    \n",
    "    # Impute missing stride_frequency based on official_fin\n",
    "    for index, row in chunk.iterrows():\n",
    "        if pd.isnull(row['stride_frequency']):\n",
    "            avg_stride_freq = merged_df_copy[merged_df_copy['official_fin'] == row['official_fin']]['stride_frequency'].mean()\n",
    "            merged_df_copy.at[index, 'stride_frequency'] = avg_stride_freq\n",
    "\n",
    "# Replace the original DataFrame with the updated one\n",
    "merged_df['stride_frequency'] = merged_df_copy['stride_frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c30501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df[['speed', 'progress', 'stride_frequency', 'longitude', 'latitude', 'post_pos', 'official_fin']].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a5d93-705d-42e3-956a-9cb9dbc4178a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define your variables\n",
    "max_seq_length = 120  # Replace with your maximum sequence length\n",
    "num_features = 5      # Replace with the actual number of features in your data\n",
    "num_classes = 12      # Replace with the actual number of classes\n",
    "\n",
    "# Build your model\n",
    "model_lstm = tf.keras.Sequential()\n",
    "model_lstm.add(tf.keras.layers.Masking(mask_value=0., input_shape=(max_seq_length, num_features)))\n",
    "model_lstm.add(tf.keras.layers.LSTM(128))\n",
    "model_lstm.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dddd20-cbb3-4444-98d7-10f3931ef518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into dataframe:\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02ef34-d54b-4619-bf4b-c2128e9bc171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "history_lstm = model_lstm.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d5e01-6b93-41af-af61-7ee2fa5827c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Combining the Models\n",
    "\n",
    "To create an ensemble, you can combine the predictions of these models in several ways:\n",
    "\t1.\tAveraging Probabilities:\n",
    "\t•\tObtain probability distributions over finishing positions from each model.\n",
    "\t•\tAverage the probabilities across models to get the final prediction.\n",
    "\t2.\tWeighted Averaging:\n",
    "\t•\tAssign weights to each model based on validation performance.\n",
    "\t•\tCompute a weighted average of the probabilities.\n",
    "\t3.\tStacking (Meta-Learner):\n",
    "\t•\tUse the predictions from the individual models as input features to a meta-model (e.g., a logistic regression or another neural network).\n",
    "\t•\tThe meta-model learns how to best combine the individual predictions.\n",
    "\t4.\tVoting (for Classification):\n",
    "\t•\tIf treating the problem as classification into discrete positions, use majority voting among the models.\n",
    "\t•\tNot as suitable if you need probability distributions.\n",
    "\n",
    "Implementation Steps\n",
    "\n",
    "1. Data Preparation\n",
    "\n",
    "\t•\tSequences:\n",
    "\t•\tUse the raw GPS data (gpspoint) to create sequences for each horse in each race.\n",
    "\t•\tEnsure that sequences are properly sorted by time_stamp.\n",
    "\t•\tFeatures:\n",
    "\t•\tInclude raw features such as speed, progress, stride_frequency.\n",
    "\t•\tAvoid hand-engineering features like acceleration to adhere to your objective.\n",
    "\t•\tLabels:\n",
    "\t•\tUse official_fin from results_entries as the target variable.\n",
    "\t•\tSince you want probabilities for each finishing position, consider encoding official_fin as categorical labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdede23-c3ae-4f35-9620-a281a6c599c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
