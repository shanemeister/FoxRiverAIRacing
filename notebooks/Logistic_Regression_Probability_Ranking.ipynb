{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a43fc8b",
   "metadata": {},
   "source": [
    "# Logistic Regression Data Preparation\n",
    "\n",
    "A lot of work has gone into compiling the current dataset. I have merged the gps_df, sectionals_df and results_df. I have limited the amount of Equibase data I am using just to keep the focus on the TPD GPS data, and to do some feature engineering.  However, there are some good metrics from the Equibase data that are just basic measures that could be obtained from any racebook sheet. \n",
    "\n",
    "## Get Started\n",
    "\n",
    "1. Going to load the parquet DataFrame from disk and do some imputation, one-hot encoding, string indexing, and scaling. The run it through XBBoost to see how it's looking. At this point I will do the integration of route data, and add the GPS aggregations. I just want to see what I can minimally do and how its working before I go down the wrong path. If the XGBoost doesn't do any better than the LSTM, at least I won't have wasted any more time on it. \n",
    "\n",
    "### Load master_results_df.parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1463424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pyspark.sql.functions as F\n",
    "import xgboost as xgb\n",
    "from sklearn import set_config\n",
    "from pyspark.sql.functions import (col, count, row_number, abs, unix_timestamp, mean, \n",
    "                                   when, lit, min as F_min, max as F_max , \n",
    "                                   row_number, mean as F_mean, countDistinct, last, first, when)\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession\n",
    "from src.data_preprocessing.data_prep1.sql_queries import sql_queries\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame, Window\n",
    "from src.data_preprocessing.data_prep1.data_utils import (save_parquet, gather_statistics, \n",
    "                initialize_environment, load_config, initialize_logging, initialize_spark, \n",
    "                identify_and_impute_outliers, \n",
    "                identify_and_remove_outliers, identify_missing_and_outliers)\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Set global references to None\n",
    "spark = None\n",
    "master_results_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73671a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 11:41:38,296 - INFO - Environment setup initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark, jdbc_url, jdbc_properties, queries, parquet_dir, log_file = initialize_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c57a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(os.path.join(parquet_dir, \"results_only_clean.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ccd5955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "777100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce2ed82",
   "metadata": {},
   "source": [
    "# Switching to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e3b985b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert Spark DataFrame -> Pandas DataFrame\n",
    "\n",
    "rf_df = df.toPandas()\n",
    "df = None\n",
    "# Quick info about the DataFrame\n",
    "#print(df.info())\n",
    "#print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92eb1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose the finish position is in a column named 'official_fin'\n",
    "# Create a binary label: 1 = first place, 0 = others\n",
    "#df[\"label\"] = (df[\"official_fin\"] == 1).astype(int)\n",
    "\n",
    "# Check distribution of the label\n",
    "#print(df[\"label\"].value_counts())\n",
    "\n",
    "# Define a function to map official_fin to label\n",
    "def map_official_fin_to_label(official_fin):\n",
    "    if official_fin == 1:\n",
    "        return 0  # Win\n",
    "    elif official_fin == 2:\n",
    "        return 1  # Place\n",
    "    elif official_fin == 3:\n",
    "        return 2  # Show\n",
    "    elif official_fin == 4:\n",
    "        return 3  # Fourth\n",
    "    elif official_fin == 5:\n",
    "        return 4  # Fifth\n",
    "    elif official_fin == 6:\n",
    "        return 5  # Sixth\n",
    "    elif official_fin == 7:\n",
    "        return 6  # Seventh\n",
    "    else:\n",
    "        return 7  # Outside top-7\n",
    "\n",
    "# Apply the function to create the label column\n",
    "rf_df['label'] = rf_df['official_fin'].apply(map_official_fin_to_label)\n",
    "\n",
    "# Check the DataFrame\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eb90e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a) Identify columns with high missingness\n",
    "missing_summary = rf_df.isna().sum().sort_values(ascending=False)\n",
    "#print(\"Missing Value Summary:\\n\", missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f3dade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Descriptive Stats:\\n\", df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0d9caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick correlation matrix\n",
    "# corr_matrix = df.corr(numeric_only=True)\n",
    "# print(\"Correlation Matrix:\\n\", corr_matrix[\"label\"].sort_values(ascending=False))\n",
    "\n",
    "# # Possibly visualize\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(corr_matrix, cmap=\"coolwarm\")\n",
    "# plt.title(\"Correlation Heatmap\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84383e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Histograms for numeric columns\n",
    "# df.hist(bins=30, figsize=(15,10))\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f701e552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.2897043   0.53291582  0.02192181 ... -0.55929365 -0.81725626\n",
      "  -1.38242958]\n",
      " [ 0.93588108  0.67407432 -1.47097668 ... -1.08219468 -0.44065956\n",
      "  -1.01882963]\n",
      " [-0.73036289 -1.16098625 -0.0087242  ...  0.56695473  0.68913051\n",
      "   0.79917007]\n",
      " ...\n",
      " [ 0.62346033  0.39175731 -0.3195623  ... -0.51907049  1.0657272\n",
      "  -0.29162975]\n",
      " [-1.14692388 -0.17287671 -0.07877222 ... -1.68554203  1.44232389\n",
      "   0.07197019]\n",
      " [ 1.76900306  0.67407432  0.25395589 ...  1.04963261 -1.57044964\n",
      "  -1.01882963]]\n",
      "[[-0.52208239 -1.58446177  1.2214942  ...  0.28539264  1.0657272\n",
      "  -0.65522969]\n",
      " [-1.14692388  0.53291582 -0.09628423 ... -1.68554203 -0.06406287\n",
      "  -0.65522969]\n",
      " [-0.20966165  0.53291582  0.24519988 ...  0.28539264 -0.06406287\n",
      "   0.79917007]\n",
      " ...\n",
      " [-1.04278363 -0.45519372  0.84498608 ...  0.48650842  0.68913051\n",
      "   0.79917007]\n",
      " [ 0.41517984 -1.86677878  0.66548802 ...  0.80829367  1.0657272\n",
      "   1.88996989]\n",
      " [-1.04278363 -0.17287671 -1.52351269 ...  1.49208733  0.68913051\n",
      "   0.79917007]]\n"
     ]
    }
   ],
   "source": [
    "metadata = rf_df[[\"race_date\", \"race_number\", \"horse_id\"]]  # Replace with your metadata columns\n",
    "\n",
    "feature_cols = [\"morn_odds\", \"net_sentiment\", \"power\", \"avg_spd_sd\", \"hi_spd_sd\",\n",
    "                \"avgspd\", \"ave_cl_sd\", \"cond_win\", \"cond_place\", \"all_win\", \"all_place\",\n",
    "                \"cond_earnings\", \"all_earnings\", \"weight\", \"cond_show\", \"all_show\", \n",
    "                \"cond_starts\", \"all_starts\", \"class_rating\", \"age_at_race_day\", \"distance\",\n",
    "                \"horse_id\", \"claimprice\", \"wps_pool\", \"cond_fourth\", \"all_fourth\", \n",
    "                \"purse\", \"pstyerl\", \"race_number\", \"start_position\"]\n",
    "\n",
    "# 6b) Convert categorical columns to numeric dummies\n",
    "categorical_cols = [\"course_cd\", \n",
    "                    \"equip\", \n",
    "                    \"surface\", \n",
    "                    \"trk_cond\", \n",
    "                    \"weather\", \n",
    "                    \"med\", \n",
    "                    \"stk_clm_md\", \n",
    "                    \"turf_mud_mark\", \n",
    "                    \"race_type\"]\n",
    "\n",
    "# Perform one-hot encoding on the categorical columns\n",
    "df_encoded = pd.get_dummies(rf_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Separate the label from the features\n",
    "y = df_encoded[\"label\"].values\n",
    "\n",
    "X = rf_df[feature_cols].copy()\n",
    "\n",
    "# Combine the encoded features with the numeric features\n",
    "X = df_encoded[feature_cols].copy()\n",
    "\n",
    "# Separate the date columns\n",
    "# date_cols = ['race_date']\n",
    "# X = X.drop(columns=date_cols)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test, metadata_train, metadata_test = train_test_split(\n",
    "    X, y, metadata, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "# Optional: Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Check the scaled features\n",
    "print(X_train_scaled)\n",
    "print(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842de6d",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b51ef4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    105776\n",
      "1    105638\n",
      "2    105525\n",
      "3    105107\n",
      "4    102116\n",
      "7     95879\n",
      "5     89994\n",
      "6     67065\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the distribution of the labels\n",
    "label_distribution = rf_df['label'].value_counts()\n",
    "print(label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a326fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# # Assuming y_train contains the labels\n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "# class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Use 'balanced' which automatically computes the class weights based on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d4e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RF classifier\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=1000,       # number of trees\n",
    "    max_depth=None,         # can tune if you want\n",
    "    random_state=42,        # for reproducibility\n",
    "    class_weight=\"balanced\"  # class_weights_dict       # or \"balanced\" if you have class imbalance\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aeff08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 9924  3457  2273  1768  1313   591    77  1752]\n",
      " [ 7427  3426  2669  2351  1906   883   141  2325]\n",
      " [ 5670  3180  2522  2873  2499  1229   196  2936]\n",
      " [ 4269  2661  2646  2880  3139  1665   250  3511]\n",
      " [ 3209  2236  2243  2888  3164  2045   320  4318]\n",
      " [ 2205  1621  1690  2307  3040  1865   395  4876]\n",
      " [ 1311   957   963  1329  1946  1497   265  5145]\n",
      " [ 1258   888   941  1204  1619  1390   330 11546]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.47      0.35     21155\n",
      "           1       0.19      0.16      0.17     21128\n",
      "           2       0.16      0.12      0.14     21105\n",
      "           3       0.16      0.14      0.15     21021\n",
      "           4       0.17      0.15      0.16     20423\n",
      "           5       0.17      0.10      0.13     17999\n",
      "           6       0.13      0.02      0.03     13413\n",
      "           7       0.32      0.60      0.42     19176\n",
      "\n",
      "    accuracy                           0.23    155420\n",
      "   macro avg       0.20      0.22      0.19    155420\n",
      "weighted avg       0.20      0.23      0.20    155420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b66f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_model.feature_importances_\n",
    "for col, imp in zip(feature_cols, importances):\n",
    "    print(f\"{col}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939f420",
   "metadata": {},
   "source": [
    "# Predicting Probabilities for Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Predict Probabilities\n",
    "predicted_probs = rf_model.predict_proba(X_test_scaled)[:, 1]  # Probability for winning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8963293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Combine Metadata with Predictions\n",
    "ranked_df = metadata_test.copy()\n",
    "ranked_df[\"predicted_probability\"] = predicted_probs\n",
    "\n",
    "# Step 3: Rank Horses\n",
    "ranked_df[\"rank\"] = (\n",
    "    ranked_df.groupby([\"race_date\", \"race_number\"])[\"predicted_probability\"]\n",
    "    .rank(method=\"first\", ascending=False)\n",
    ")\n",
    "\n",
    "# Step 4: Sort Ranked DataFrame\n",
    "ranked_df = ranked_df.sort_values(by=[\"race_date\", \"race_number\", \"rank\"])\n",
    "\n",
    "# Step 5: Save or Display Results\n",
    "print(ranked_df.head(20))  # Display top 20 ranked horses\n",
    "ranked_df.to_csv(\"ranked_horses.csv\", index=False)  # Save to CSV if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b85ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X_test corresponds to the feature matrix for your test set\n",
    "# And you have additional columns `race_date`, `race_number`, `horse_id` in the original data.\n",
    "\n",
    "# Step 1: Predict Probabilities\n",
    "proba = rf_model.predict_proba(X_test_scaled)  # Shape: (n_samples, 2)\n",
    "predicted_probs = proba[:, 1]  # Probability for class 1 (winning)\n",
    "\n",
    "# Step 2: Create a DataFrame for `X_test` with metadata\n",
    "# Replace with actual metadata (e.g., from original test data before splitting)\n",
    "metadata = pd.DataFrame({\n",
    "    \"race_date\": race_date,  # Replace with actual race dates\n",
    "    \"race_number\": race_number,  # Replace with actual race numbers\n",
    "    \"horse_id\": horse_id,  # Replace with actual horse IDs\n",
    "})\n",
    "\n",
    "# Combine predicted probabilities with metadata\n",
    "ranked_df = metadata.copy()\n",
    "ranked_df[\"predicted_probability\"] = predicted_probs\n",
    "\n",
    "# Step 3: Rank Horses\n",
    "# Group by race_date and race_number, then rank by predicted_probability\n",
    "ranked_df[\"rank\"] = (\n",
    "    ranked_df.groupby([\"race_date\", \"race_number\"])[\"predicted_probability\"]\n",
    "    .rank(method=\"first\", ascending=False)\n",
    ")\n",
    "\n",
    "# Step 4: Sort Ranked DataFrame\n",
    "ranked_df = ranked_df.sort_values(by=[\"race_date\", \"race_number\", \"rank\"])\n",
    "\n",
    "# Step 5: Save or Display Results\n",
    "print(ranked_df.head(20))  # Display top 20 ranked horses\n",
    "ranked_df.to_csv(\"ranked_horses.csv\", index=False)  # Save to CSV if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca5f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_df = metadata_test.copy()  # Metadata includes race_date, race_number, horse_id\n",
    "ranked_df[\"predicted_probability\"] = predicted_probs\n",
    "ranked_df[\"actual_label\"] = y_test  # Optional: for evaluation purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b920b06",
   "metadata": {},
   "source": [
    "## Predict via Ranking using Test Race from Dataset\n",
    "\n",
    "You can test the model on a race in your dataset by excluding the target variable (official_fin) and making predictions as if it were a new race. Here’s how you can proceed:\n",
    "\n",
    "Step 1: Select a Race for Testing\n",
    "\n",
    "Extract a specific race from your dataset based on race_date and race_number.\n",
    "\n",
    "race_to_test = df[\n",
    "    (df[\"race_date\"] == \"2023-05-15\") & (df[\"race_number\"] == 5)\n",
    "].drop(columns=[\"official_fin\"])  # Drop the target variable\n",
    "\n",
    "Step 2: Prepare the Features\n",
    "\n",
    "Ensure that the extracted race has the same feature processing (scaling, encoding) as was done during training.\n",
    "\n",
    "# Extract features\n",
    "X_race = race_to_test[feature_cols]  # Ensure `feature_cols` matches the training feature set\n",
    "\n",
    "# Scale features\n",
    "X_race_scaled = scaler.transform(X_race)  # Use the scaler fitted during training\n",
    "\n",
    "Step 3: Predict Probabilities\n",
    "\n",
    "Use the model to predict probabilities for this specific race.\n",
    "\n",
    "# Predict probabilities\n",
    "race_probs = rf_model.predict_proba(X_race_scaled)\n",
    "\n",
    "# Attach probabilities back to the metadata\n",
    "race_to_test[\"predicted_probability\"] = race_probs[:, 1]  # Assuming class 1 is 'winning'\n",
    "\n",
    "# Rank horses by predicted probability\n",
    "race_to_test[\"rank\"] = race_to_test[\"predicted_probability\"].rank(ascending=False)\n",
    "race_to_test = race_to_test.sort_values(by=\"rank\")\n",
    "\n",
    "Step 4: Compare to Actual Results\n",
    "\n",
    "If you still have the actual official_fin values in a backup, compare the model’s ranking against the real results.\n",
    "\n",
    "# Add actual finish positions for comparison (if available)\n",
    "actual_results = df[\n",
    "    (df[\"race_date\"] == \"2023-05-15\") & (df[\"race_number\"] == 5)\n",
    "][[\"horse_id\", \"official_fin\"]]\n",
    "\n",
    "race_to_test = race_to_test.merge(actual_results, on=\"horse_id\", how=\"left\")\n",
    "\n",
    "# Display results\n",
    "print(race_to_test[[\"horse_id\", \"predicted_probability\", \"rank\", \"official_fin\"]])\n",
    "\n",
    "Testing Without horse_id, race_date, or race_number\n",
    "\n",
    "If you remove horse_id, race_date, or race_number, the model should still be able to make predictions if those columns are not part of the feature set. However, you won’t be able to group or rank the predictions by race because race_date and race_number are critical for distinguishing horses in the same race.\n",
    "\n",
    "To simulate a future race:\n",
    "\t1.\tSelect a race that the model hasn’t seen during training (e.g., from a holdout set).\n",
    "\t2.\tRemove any identifying metadata (e.g., horse_id, race_date, race_number).\n",
    "\t3.\tProcess the features the same way as during training.\n",
    "\t4.\tUse the model to predict probabilities for the horses in that race.\n",
    "\t5.\tRank the predictions by probability.\n",
    "\n",
    "Testing with a Future Race\n",
    "\n",
    "For a future race:\n",
    "\t1.\tCollect horse features for that race (e.g., from Equibase).\n",
    "\t2.\tProcess the data the same way as the training data.\n",
    "\t3.\tUse the model to make predictions.\n",
    "\t4.\tRank horses by predicted probabilities.\n",
    "\t5.\tWait for the race results and compare the model’s rankings to the actual outcomes.\n",
    "\n",
    "This approach ensures the model is evaluated on unseen data, simulating a real-world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea8560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9d5cf9c",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69c255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f05de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", 0.5],\n",
    "    \"class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",    # or \"f1\", \"balanced_accuracy\", etc.\n",
    "    cv=3,                  # 3-fold cross-validation\n",
    "    n_jobs=-1             # use all CPU cores\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Params:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Then evaluate on the test set:\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_best = best_rf.predict(X_test)\n",
    "print(\"Final Test Accuracy:\", (y_pred_best == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a0649f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486d1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb16452a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
