{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db68106",
   "metadata": {},
   "source": [
    "# LSTM: Sectionals_v3 Predictions\n",
    "\n",
    "Train an LSTM to analyze a horse’s past performance sequence and predict the horse's race pattern that reflects its projected strength today.\n",
    "\n",
    "### Target is distance_back\n",
    "\n",
    "✅ This model, unless it proves predictive on a standalone apactity, will be fed into the CatBoost/YetiRank model as a feature.\n",
    "\n",
    "⸻\n",
    "\n",
    "🔍 How Accurate is Ranking by Sectional Data Alone?\n",
    "\n",
    "🔥 The Good:\n",
    "\n",
    "If your LSTM is trained on well-normalized, consistent historical speed signals: • Relative ordering can be quite meaningful • Especially in smaller fields or when you’re identifying top 3–4 finishers • Even if absolute values are off, ranking is often more robust\n",
    "\n",
    "❄️ The Limitations: • LSTM alone won’t account for today’s track conditions, surface bias, jockey/trainer changes, distance, class jump/drop • It may favor horses that ran fast recently but are now outclassed or mispositioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d976bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "os.environ.pop(\"CUDA_VISIBLE_DEVICES\", None)\n",
    "\n",
    "# Setup Environment\n",
    "import time\n",
    "from optuna.importance import MeanDecreaseImpurityImportanceEvaluator\n",
    "import os\n",
    "import logging\n",
    "import copy\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "from torch.optim.lr_scheduler import OneCycleLR, StepLR\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import optuna.visualization as viz\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import (to_date, date_format, lpad, concat_ws, collect_list, struct, \n",
    "                                   col, count, row_number, abs, unix_timestamp, mean, \n",
    "                                   when, lit, min as F_min, max as F_max , upper, trim,\n",
    "                                   mean as F_mean, countDistinct, last, first, when)\n",
    "from src.data_preprocessing.data_prep1.data_utils import initialize_environment\n",
    "from src.data_preprocessing.data_prep1.data_loader import load_data_from_postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff7422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "spark, jdbc_url, jdbc_properties, parquet_dir, log_file = initialize_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c779b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gps_sql_queries():\n",
    "    queries = {\n",
    "#         \"gps_horse\": \"\"\"\n",
    "#             SELECT g.course_cd, g.race_date,g.race_number,\n",
    "#             REGEXP_REPLACE(TRIM(UPPER(saddle_cloth_number)), '\\s+$', '') AS saddle_cloth_number, time_stamp, \n",
    "#             longitude, latitude, speed, progress, stride_frequency, g.post_time, location,\n",
    "#             re.axciskey, h.horse_id, re.official_fin, h.horse_name\n",
    "#             FROM gpspoint g\n",
    "#             JOIN results_entries re on g.course_cd = re.course_cd\n",
    "#                 AND g.race_date = re.race_date\n",
    "#                 AND g.race_number = re.race_number\n",
    "#                 AND g.saddle_cloth_number = re.program_num\n",
    "#             JOIN horse h on re.axciskey = h.axciskey\n",
    "#             WHERE speed is not null\n",
    "#             AND progress is not null\n",
    "#             AND stride_frequency is not null\n",
    "#             \"\"\",\n",
    "        \"sectionals\": \"\"\"\n",
    "            --  A) Identify all horse_id's entered in upcoming races\n",
    "            WITH upcoming_horses AS (\n",
    "                SELECT DISTINCT h.horse_id\n",
    "                FROM runners r\n",
    "                JOIN horse h ON r.axciskey = h.axciskey\n",
    "                WHERE r.race_date >= CURRENT_DATE\n",
    "                  AND r.course_cd IN (\n",
    "                      'CNL','SAR','PIM','TSA','BEL','MVR','TWO','KEE','TAM','TTP','TKD',\n",
    "                      'ELP','PEN','HOU','DMR','TLS','AQU','MTH','TGP','TGG','CBY','LRL',\n",
    "                      'TED','IND','TCD','TOP'\n",
    "                  )\n",
    "            )\n",
    "            --  B) Retrieve all *historical* (past) sectionals for those horses\n",
    "            SELECT\n",
    "                s.course_cd,\n",
    "                s.race_date,\n",
    "                s.race_number,\n",
    "                h.horse_id,\n",
    "                REGEXP_REPLACE(TRIM(UPPER(s.saddle_cloth_number)), '\\s+$', '') AS saddle_cloth_number,\n",
    "                s.gate_name,\n",
    "                s.gate_numeric,\n",
    "                s.length_to_finish,\n",
    "                s.sectional_time,\n",
    "                s.running_time,\n",
    "                s.distance_back,\n",
    "                s.distance_ran,\n",
    "                s.number_of_strides,\n",
    "                s.post_time\n",
    "            FROM sectionals s\n",
    "            JOIN runners r\n",
    "                ON s.course_cd = r.course_cd\n",
    "               AND s.race_date = r.race_date\n",
    "               AND s.race_number = r.race_number\n",
    "               AND s.saddle_cloth_number = r.saddle_cloth_number\n",
    "            JOIN horse h\n",
    "                ON r.axciskey = h.axciskey\n",
    "            -- Only pull past races:\n",
    "            WHERE r.race_date < CURRENT_DATE\n",
    "              -- Basic data quality checks\n",
    "              AND s.length_to_finish IS NOT NULL\n",
    "              AND s.sectional_time   IS NOT NULL\n",
    "              AND s.running_time     IS NOT NULL\n",
    "              AND s.distance_back    IS NOT NULL\n",
    "              AND s.distance_ran     IS NOT NULL\n",
    "              AND s.number_of_strides IS NOT NULL\n",
    "              -- Restrict to the *same* set of courses you care about:\n",
    "              AND r.course_cd IN (\n",
    "                  'CNL','SAR','PIM','TSA','BEL','MVR','TWO','KEE','TAM','TTP','TKD',\n",
    "                  'ELP','PEN','HOU','DMR','TLS','AQU','MTH','TGP','TGG','CBY','LRL',\n",
    "                  'TED','IND','TCD','TOP'\n",
    "              )\n",
    "              -- Finally, only get rows for horses that have upcoming races\n",
    "              AND h.horse_id IN (\n",
    "                  SELECT horse_id FROM upcoming_horses\n",
    "              )\n",
    "            \"\"\"\n",
    "    }\n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d1447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- course_cd: string (nullable = true)\n",
      " |-- race_date: date (nullable = true)\n",
      " |-- race_number: integer (nullable = true)\n",
      " |-- horse_id: integer (nullable = true)\n",
      " |-- saddle_cloth_number: string (nullable = true)\n",
      " |-- gate_name: string (nullable = true)\n",
      " |-- gate_numeric: double (nullable = true)\n",
      " |-- length_to_finish: double (nullable = true)\n",
      " |-- sectional_time: double (nullable = true)\n",
      " |-- running_time: double (nullable = true)\n",
      " |-- distance_back: double (nullable = true)\n",
      " |-- distance_ran: double (nullable = true)\n",
      " |-- number_of_strides: double (nullable = true)\n",
      " |-- post_time: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "queries = gps_sql_queries()\n",
    "dfs = load_data_from_postgresql(spark, jdbc_url, jdbc_properties, queries, parquet_dir)\n",
    "        # Suppose we have a dictionary of queries\n",
    "for name, df in dfs.items():\n",
    "    logging.info(f\"DataFrame '{name}' loaded. Schema:\")\n",
    "    df.printSchema()\n",
    "    if name == \"gps_horse\":\n",
    "        gps_horse_df = df\n",
    "    elif name == \"sectionals\":\n",
    "        sec_pred_df = df    \n",
    "    else:\n",
    "        logging.error(f\"Unknown DataFrame name: {name}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d54ab74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "# # gps_horse_df.write.mode(\"overwrite\").parquet(f\"{parquet_dir}/gps_horse_df\")\n",
    "sec_pred_df.write.mode(\"overwrite\").parquet(f\"{parquet_dir}/sec_pred_df\")\n",
    "logging.info(f\"Data written to Parquet in {time.time() - start_time:.2f} seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26818130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gps_horse_df = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/gps_horse_df\")\n",
    "sec_pred_df = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/sec_pred_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5199c47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194332"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec_pred_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63c350",
   "metadata": {},
   "source": [
    "## Create race_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert race_date to a proper date format\n",
    "sec_pred__df = sec_pred__df.withColumn(\"race_date\", to_date(col(\"race_date\")))\n",
    "\n",
    "# Construct a unique race_id using course_cd, race_date, and race_number\n",
    "sec_pred__df = sec_pred__df.withColumn(\n",
    "    \"race_id\",\n",
    "    concat_ws(\n",
    "        \"_\",\n",
    "        col(\"course_cd\"),\n",
    "        date_format(col(\"race_date\"), \"yyyyMMdd\"),\n",
    "        lpad(col(\"race_number\").cast(\"string\"), 2, \"0\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24da2f0",
   "metadata": {},
   "source": [
    "## Create gate_seq_num Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f8d32e",
   "metadata": {},
   "source": [
    "### 1) Cast gate_numeric to a Numeric Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff8a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, cast gate_numeric to double:\n",
    "sectionals_df = sectionals_df.withColumn(\"gate_numeric\", col(\"gate_numeric\").cast(\"double\"))\n",
    "\n",
    "# Then (re)do your steps with the numeric column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a6389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Filter out finish rows to compute max gate_numeric per race\n",
    "non_finish_df = sectionals_df.filter(F.col(\"gate_name\") != \"finish\")\n",
    "\n",
    "# Define a window partitioning by race identifiers\n",
    "race_window = Window.partitionBy(\"course_cd\", \"race_date\", \"race_number\")\n",
    "\n",
    "# Compute maximum gate_numeric for each race (ignoring finish rows)\n",
    "max_gate_df = non_finish_df.groupBy(\"course_cd\", \"race_date\", \"race_number\") \\\n",
    "    .agg(F.max(\"gate_numeric\").alias(\"max_gate_numeric\"))\n",
    "\n",
    "# Step 2: Join the maximum gate value back to the original DataFrame\n",
    "joined_df = sectionals_df.join(max_gate_df, on=[\"course_cd\", \"race_date\", \"race_number\"], how=\"left\")\n",
    "\n",
    "# Step 3: Create the new column 'gate_seq_num'\n",
    "# If the gate is 'finish', assign max_gate_numeric + 0.5, else keep gate_numeric.\n",
    "sectionals_df = joined_df.withColumn(\n",
    "    \"gate_seq_num\",\n",
    "    F.when(F.col(\"gate_name\") == \"finish\", F.col(\"max_gate_numeric\") + F.lit(0.5))\n",
    "     .otherwise(F.col(\"gate_numeric\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2e36d",
   "metadata": {},
   "source": [
    "## Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e82da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sectionals_df.select(\"saddle_cloth_number\", \"gate_name\", \"gate_seq_num\", \"running_time\") \\\n",
    "  .filter(\"race_id = 'TGP_20240818_09' AND saddle_cloth_number = 3\") \\\n",
    "  .orderBy(F.asc(\"gate_seq_num\")) \\\n",
    "  .show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed6e1c4",
   "metadata": {},
   "source": [
    "## Extract the Final Distance for Each Horse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a48ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A) Filter just the finish rows\n",
    "finish_df = sectionals_df.filter(F.col(\"gate_name\") == \"finish\")\n",
    "\n",
    "# B) Select the final distance for that horse\n",
    "#    We'll rename it to \"final_dist\" to avoid conflict\n",
    "finish_dist_df = finish_df.select(\n",
    "    \"race_id\",\n",
    "    \"horse_id\",\n",
    "    F.col(\"distance_back\").alias(\"final_dist\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c1fd0",
   "metadata": {},
   "source": [
    "## Join This Final Distance Back to All Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da0b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join on (race_id, horse_id) so every row for that horse\n",
    "# gets the final distance, if it exists.\n",
    "with_final_dist = sectionals_df.join(\n",
    "    finish_dist_df,\n",
    "    on=[\"race_id\", \"horse_id\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542e2f3b",
   "metadata": {},
   "source": [
    "## Filter Out the Finish Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_only_df = with_final_dist.filter(F.col(\"gate_name\") != \"finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e6880",
   "metadata": {},
   "source": [
    "## Rename final_dist to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_only_df = partial_only_df.withColumnRenamed(\"final_dist\", \"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b50729",
   "metadata": {},
   "outputs": [],
   "source": [
    "sectionals_df = partial_only_df\n",
    "\n",
    "sectionals_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342919e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark query to filter and select specific columns\n",
    "filtered_sectionals_df = sectionals_df.filter(col(\"race_id\") == \"TGP_20240818_09\") \\\n",
    "    .select(\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\").distinct()\n",
    "\n",
    "# Show the result\n",
    "filtered_sectionals_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feea4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# # 1) First, get a DataFrame mapping each race_id to the unique horse count.\n",
    "# horses_per_race_df = (\n",
    "#     sectionals_df\n",
    "#     .groupBy(\"race_id\")\n",
    "#     .agg(F.countDistinct(\"horse_id\").alias(\"num_horses\"))\n",
    "# )\n",
    "\n",
    "# # 2) Now group *that* result by num_horses to see how many races have that count.\n",
    "# dist_df = (\n",
    "#     horses_per_race_df\n",
    "#     .groupBy(\"num_horses\")\n",
    "#     .count()\n",
    "#     .orderBy(\"num_horses\")\n",
    "# )\n",
    "# dist_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189a3d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PySpark query to filter and select specific columns\n",
    "# filtered_sectionals_df = sectionals_df.filter(col(\"race_id\") == \"TGP_20240818_09\") \\\n",
    "#     .select(\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\").distinct()\n",
    "\n",
    "# # Show the result\n",
    "# filtered_sectionals_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe16e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: create a DataFrame of only the races that have >=4 horses\n",
    "# valid_races_df = (\n",
    "#     horses_per_race_df\n",
    "#     .filter(F.col(\"num_horses\") >= 4)\n",
    "#     .select(\"race_id\")\n",
    "#     .distinct()\n",
    "# )\n",
    "\n",
    "# # Step 2: Join the original data with valid_races_df to keep only those race_ids\n",
    "# sectionals_df = sectionals_df.join(valid_races_df, on=\"race_id\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2398418d",
   "metadata": {},
   "source": [
    "# Compute Sequence Lenghts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb0804c",
   "metadata": {},
   "source": [
    "### Filter Rows with >=1 and <=38 Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e3142",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Compute sequence lengths per horse per race\n",
    "df_seq_len = sectionals_df.groupBy(\"race_id\", \"horse_id\").agg(F.count(\"*\").alias(\"seq_length\"))\n",
    "\n",
    "# Step 2: Filter sequences with length between 1 and 38\n",
    "filtered_df = df_seq_len.filter((F.col(\"seq_length\") >= 1) & (F.col(\"seq_length\") <= 38))\n",
    "\n",
    "# Step 3: Join the filtered sequence lengths back with the original data\n",
    "sectionals_df = sectionals_df.join(filtered_df, on=[\"race_id\", \"horse_id\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9761b0",
   "metadata": {},
   "source": [
    "### Plot Sequence Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1367b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming df_seq_len is a DataFrame with a 'seq_length' column\n",
    "# If df_seq_len is a PySpark DataFrame, convert it to Pandas\n",
    "df_seq_len_pd = df_seq_len.toPandas()\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_seq_len_pd['seq_length'], bins=50, kde=True)\n",
    "plt.title('Distribution of Sequence Lengths')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea62df",
   "metadata": {},
   "source": [
    "## Padding Sequences to a Fixed Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31f4d5",
   "metadata": {},
   "source": [
    "###  Step 1: Convert to Sequence Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0afb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sectionals_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0742015d",
   "metadata": {},
   "source": [
    "## Create segment_ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462dc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "w = Window.partitionBy(\"race_id\", \"horse_id\").orderBy(F.asc(\"gate_seq_num\"))\n",
    "\n",
    "sectionals_df = sectionals_df.withColumn(\n",
    "    \"segment_ordinal\",\n",
    "    F.row_number().over(w)   # 1-based index for each call in ascending order\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c677ca",
   "metadata": {},
   "source": [
    "## Make Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"length_to_finish\",\n",
    "    \"sectional_time\",\n",
    "    \"running_time\",\n",
    "    \"distance_back\",\n",
    "    \"distance_ran\",\n",
    "    \"number_of_strides\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bcf51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct, collect_list\n",
    "\n",
    "df_features = sectionals_df.select(\n",
    "    \"race_id\",\n",
    "    \"horse_id\",\n",
    "    \"segment_ordinal\",\n",
    "    F.struct(*[F.col(f) for f in features]).alias(\"feature_struct\")\n",
    ")\n",
    "\n",
    "# Window to collect all rows (time steps) for each (race_id, horse_id).\n",
    "collect_w = Window.partitionBy(\"race_id\", \"horse_id\").orderBy(\"segment_ordinal\") \\\n",
    "                  .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"sequence\",\n",
    "    collect_list(\"feature_struct\").over(collect_w)\n",
    ")\n",
    "\n",
    "# Now each row for that horse has a \"sequence\", but repeated for each time step.\n",
    "# We only want the final row for each horse, i.e. the row with max(segment_ordinal).\n",
    "df_features = df_features.withColumn(\n",
    "    \"rn\",\n",
    "    F.row_number().over(\n",
    "        Window.partitionBy(\"race_id\", \"horse_id\").orderBy(F.desc(\"segment_ordinal\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Keep only rn == 1 (the row with the largest segment_ordinal => entire sequence):\n",
    "df_features = df_features.filter(F.col(\"rn\") == 1)\n",
    "\n",
    "# Now df_features has columns: [race_id, horse_id, sequence], one row per horse.\n",
    "df_features = df_features.select(\"race_id\", \"horse_id\", \"sequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ff4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_features.join(\n",
    "    sectionals_df\n",
    "        .select(\"race_id\", \"horse_id\", \"target\")\n",
    "        .dropDuplicates([\"race_id\", \"horse_id\"]), \n",
    "    on=[\"race_id\", \"horse_id\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca1dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StructType, StructField, FloatType\n",
    "\n",
    "# We must define the schema for the array of structs that we store in \"sequence\".\n",
    "# If you have 6 float features, define them accordingly. For example:\n",
    "feature_struct_schema = StructType([\n",
    "    StructField(\"length_to_finish\", FloatType(), True),\n",
    "    StructField(\"sectional_time\", FloatType(), True),\n",
    "    StructField(\"running_time\", FloatType(), True),\n",
    "    StructField(\"distance_back\", FloatType(), True),\n",
    "    StructField(\"distance_ran\", FloatType(), True),\n",
    "    StructField(\"number_of_strides\", FloatType(), True)\n",
    "])\n",
    "\n",
    "target_len = 38\n",
    "\n",
    "# Python function to pad/truncate a list of structs:\n",
    "def pad_sequence(seq, max_len=target_len):\n",
    "    \"\"\"Truncates if len(seq)>max_len, else pads with zero-like structs.\"\"\"\n",
    "    if seq is None:\n",
    "        return []\n",
    "    \n",
    "    seq = list(seq)  # ensure it's mutable\n",
    "\n",
    "    # 1) Truncate if too long\n",
    "    if len(seq) > max_len:\n",
    "        seq = seq[:max_len]\n",
    "\n",
    "    # 2) Pad if too short\n",
    "    while len(seq) < max_len:\n",
    "        seq.append({\n",
    "            \"length_to_finish\": 0.0,\n",
    "            \"sectional_time\": 0.0,\n",
    "            \"running_time\": 0.0,\n",
    "            \"distance_back\": 0.0,\n",
    "            \"distance_ran\": 0.0,\n",
    "            \"number_of_strides\": 0.0\n",
    "        })\n",
    "    return seq\n",
    "\n",
    "pad_sequence_udf = F.udf(lambda seq: pad_sequence(seq, target_len), \n",
    "                         ArrayType(feature_struct_schema))\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"padded_sequence\", \n",
    "    pad_sequence_udf(\"sequence\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c341f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e5420e",
   "metadata": {},
   "source": [
    "### Convert to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b0b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pd = df_features.toPandas()\n",
    "\n",
    "# Sort by race_date to maintain temporal order\n",
    "all_pd = all_pd.sort_values(\"race_id\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0fb288",
   "metadata": {},
   "source": [
    "### Drop NaN target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c1db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows before\n",
    "old_count = len(all_pd)\n",
    "\n",
    "# Drop rows where \"target\" is NaN\n",
    "all_pd = all_pd.dropna(subset=[\"target\"])\n",
    "\n",
    "# Number of rows after\n",
    "new_count = len(all_pd)\n",
    "\n",
    "print(f\"Rows before drop: {old_count}, after drop: {new_count}\")\n",
    "print(f\"Rows dropped: {old_count - new_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a55d6",
   "metadata": {},
   "source": [
    "# Split & Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce58ff23",
   "metadata": {},
   "source": [
    "## Split by Entire Race "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1eca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_data_3way_split_by_race(\n",
    "    df,\n",
    "    race_date_col=None,\n",
    "    train_frac=0.70,\n",
    "    val_frac=0.15,\n",
    "    test_frac=0.15,\n",
    "    target_len=38\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits the entire dataset into (train, val, test) by race_id so that\n",
    "    all horses in a race go to the same split. Then, for each subset,\n",
    "    we build X arrays of shape (num_samples, target_len, 6) [or #features],\n",
    "    y arrays of shape (num_samples,), and we keep an array of race_ids as well.\n",
    "\n",
    "    Returns:\n",
    "      (X_train, y_train, race_ids_train,\n",
    "       X_val,   y_val,   race_ids_val,\n",
    "       X_test,  y_test,  race_ids_test)\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # (A) Sort the race_ids\n",
    "    # -----------------------------\n",
    "    #  If you have a race_date column, use it to sort the races in chronological order\n",
    "    if race_date_col and race_date_col in df.columns:\n",
    "        # Make sure race_date_col is a date or something sortable\n",
    "        # Then collect unique (race_id, race_date), sort by race_date\n",
    "        race_date_map = (\n",
    "            df[[race_date_col, \"race_id\"]]\n",
    "            .drop_duplicates(subset=[\"race_id\"])\n",
    "            .sort_values(by=race_date_col)\n",
    "        )\n",
    "        race_ids_sorted = race_date_map[\"race_id\"].tolist()\n",
    "    else:\n",
    "        # If no date, lexicographically sort the unique race_ids\n",
    "        race_ids_sorted = sorted(df[\"race_id\"].unique())\n",
    "\n",
    "    n_races = len(race_ids_sorted)\n",
    "    n_train = int(train_frac * n_races)\n",
    "    n_val   = int(val_frac   * n_races)\n",
    "    # remainder goes to test\n",
    "    n_test  = n_races - (n_train + n_val)\n",
    "\n",
    "    # slice them\n",
    "    train_races = race_ids_sorted[:n_train]\n",
    "    val_races   = race_ids_sorted[n_train : n_train + n_val]\n",
    "    test_races  = race_ids_sorted[n_train + n_val : ]\n",
    "\n",
    "    # -----------------------------\n",
    "    # (B) Filter the main df\n",
    "    # -----------------------------\n",
    "    df_train = df[df[\"race_id\"].isin(train_races)].copy()\n",
    "    df_val   = df[df[\"race_id\"].isin(val_races)].copy()\n",
    "    df_test  = df[df[\"race_id\"].isin(test_races)].copy()\n",
    "\n",
    "    # Optionally ensure no duplicates\n",
    "    df_train = df_train.drop_duplicates(subset=[\"race_id\", \"horse_id\"])\n",
    "    df_val   = df_val.drop_duplicates(subset=[\"race_id\", \"horse_id\"])\n",
    "    df_test  = df_test.drop_duplicates(subset=[\"race_id\", \"horse_id\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # (C) Build arrays (X, y, race_ids) for each subset\n",
    "    # -----------------------------\n",
    "\n",
    "    def build_lstm_arrays(sub_df, target_len):\n",
    "        X_list = []\n",
    "        y_list = []\n",
    "        rid_list = []\n",
    "        hid_list = []\n",
    "        \n",
    "        for _, row in sub_df.iterrows():\n",
    "            # This is already a list of exactly 'target_len' dicts\n",
    "            seq_data = row[\"padded_sequence\"]  \n",
    "\n",
    "            # Convert each dict in seq_data into a numeric [6]-list\n",
    "            numeric_seq = []\n",
    "            for d in seq_data:\n",
    "                numeric_seq.append([\n",
    "                    d[\"length_to_finish\"],\n",
    "                    d[\"sectional_time\"],\n",
    "                    d[\"running_time\"],\n",
    "                    d[\"distance_back\"],\n",
    "                    d[\"distance_ran\"],\n",
    "                    d[\"number_of_strides\"]\n",
    "                ])\n",
    "\n",
    "            X_list.append(numeric_seq)\n",
    "            y_list.append(row[\"target\"])   # or row[\"target\"], if that’s your label\n",
    "            rid_list.append(row[\"race_id\"])\n",
    "            hid_list.append(row[\"horse_id\"])\n",
    "            \n",
    "        X_array = np.array(X_list, dtype=np.float32)\n",
    "        y_array = np.array(y_list, dtype=np.float32)\n",
    "        r_array = np.array(rid_list)\n",
    "        h_array = np.array(hid_list, dtype=np.int64)\n",
    "        \n",
    "        return X_array, y_array, r_array, h_array\n",
    "\n",
    "    # do it for each\n",
    "    X_train, y_train, raceids_train, horseids_train = build_lstm_arrays(df_train, target_len)\n",
    "    X_val,   y_val,   raceids_val,   horseids_val   = build_lstm_arrays(df_val,   target_len)\n",
    "    X_test,  y_test,  raceids_test,  horseids_test  = build_lstm_arrays(df_test,  target_len)\n",
    "\n",
    "    return (\n",
    "        X_train, y_train, raceids_train, horseids_train,\n",
    "        X_val,   y_val,   raceids_val,   horseids_val,\n",
    "        X_test,  y_test,  raceids_test,  horseids_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe93dd2",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2fb340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHorseDataset(Dataset):\n",
    "    def __init__(self, X_array, y_array):\n",
    "        \"\"\"\n",
    "        X_array: shape (N, seq_len, num_features)\n",
    "        y_array: shape (N,)\n",
    "        \"\"\"\n",
    "        self.X_array = X_array\n",
    "        self.y_array = y_array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X_array[idx]  # (seq_len, num_features)\n",
    "        y = self.y_array[idx]  # scalar\n",
    "        # Convert to tensors\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "        return X_tensor, y_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24ebab",
   "metadata": {},
   "source": [
    "# SAVE YOUR SCALERS NEXT TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826134f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_SingleHorse_data(\n",
    "    X_train, y_train, race_ids_train,\n",
    "    X_val,   y_val,   race_ids_val,\n",
    "    X_test,  y_test,  race_ids_test,\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Caps outliers (1st/99th percentile).\n",
    "    2) MinMax-scales features.\n",
    "    3) Builds three SingleHorseDatasets & DataLoaders (train, val, test).\n",
    "    \"\"\"\n",
    "\n",
    "    def cap_outliers(data, lower=1, upper=99):\n",
    "        lo = np.percentile(data, lower, axis=0)\n",
    "        hi = np.percentile(data, upper, axis=0)\n",
    "        return np.clip(data, lo, hi)\n",
    "    \n",
    "    # --- Training set ---\n",
    "    num_train, seq_len, num_features = X_train.shape\n",
    "    X_train_flat = X_train.reshape(-1, num_features)\n",
    "    X_train_cap = cap_outliers(X_train_flat)\n",
    "    scaler_X = MinMaxScaler()\n",
    "    X_train_scaled_flat = scaler_X.fit_transform(X_train_cap)\n",
    "    X_train_scaled = X_train_scaled_flat.reshape(num_train, seq_len, num_features)\n",
    "    \n",
    "    # --- Validation set ---\n",
    "    num_val, seq_len_val, num_features_val = X_val.shape\n",
    "    X_val_flat = X_val.reshape(-1, num_features_val)\n",
    "    X_val_cap = cap_outliers(X_val_flat)\n",
    "    X_val_scaled_flat = scaler_X.transform(X_val_cap)\n",
    "    X_val_scaled = X_val_scaled_flat.reshape(num_val, seq_len_val, num_features_val)\n",
    "    \n",
    "    # --- Test set ---\n",
    "    num_test, seq_len_test, num_features_test = X_test.shape\n",
    "    X_test_flat = X_test.reshape(-1, num_features_test)\n",
    "    X_test_cap = cap_outliers(X_test_flat)\n",
    "    X_test_scaled_flat = scaler_X.transform(X_test_cap)\n",
    "    X_test_scaled = X_test_scaled_flat.reshape(num_test, seq_len_test, num_features_test)\n",
    "    \n",
    "    ####################################\n",
    "    # 1) CAP + SCALE FOR Y_TRAIN\n",
    "    ####################################\n",
    "    y_train_unscaled = y_train\n",
    "    # Reshape to 2D so scaler can work on columns\n",
    "    y_train_2d = y_train.reshape(-1, 1)\n",
    "\n",
    "    # Cap outliers\n",
    "    y_train_cap = cap_outliers(y_train_2d)\n",
    "\n",
    "    # Fit a separate scaler for the labels\n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_train_scaled_2d = scaler_y.fit_transform(y_train_cap)\n",
    "\n",
    "    # Flatten back to 1D\n",
    "    y_train_scaled = y_train_scaled_2d.flatten()\n",
    "\n",
    "    ####################################\n",
    "    # 2) CAP + SCALE FOR Y_VAL\n",
    "    ####################################\n",
    "    y_val_2d = y_val.reshape(-1, 1)\n",
    "    y_val_cap = cap_outliers(y_val_2d)\n",
    "    y_val_scaled_2d = scaler_y.transform(y_val_cap)\n",
    "    y_val_scaled = y_val_scaled_2d.flatten()\n",
    "\n",
    "    ####################################\n",
    "    # 3) CAP + SCALE FOR Y_TEST\n",
    "    ####################################\n",
    "    y_test_2d = y_test.reshape(-1, 1)\n",
    "    y_test_cap = cap_outliers(y_test_2d)\n",
    "    y_test_scaled_2d = scaler_y.transform(y_test_cap)\n",
    "    y_test_scaled = y_test_scaled_2d.flatten()\n",
    "    \n",
    "    ####################################\n",
    "    # 4) Save Scalers\n",
    "    ####################################\n",
    "    save_dir=\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/models/LSTM/training_scalers_sectionals\"\n",
    "    try:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        joblib.dump(scaler_X, os.path.join(save_dir, \"scaler_X.pkl\"))\n",
    "        joblib.dump(scaler_y, os.path.join(save_dir, \"scaler_y.pkl\"))\n",
    "        print(f\"Scalers saved to {save_dir!r}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save scalers: {e!r}\")\n",
    "\n",
    "    # Build the SingleHorseDatasets\n",
    "    train_dataset = SingleHorseDataset(X_train_scaled, y_train_scaled)\n",
    "    val_dataset   = SingleHorseDataset(X_val_scaled,   y_val_scaled)\n",
    "    test_dataset  = SingleHorseDataset(X_test_scaled,  y_test_scaled)\n",
    "\n",
    "    # Build DataLoaders (you can pick any batch_size > 1)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=16, shuffle=False)\n",
    "\n",
    "    return (\n",
    "        X_train_scaled, y_train_scaled, y_train_unscaled, scaler_X, scaler_y,\n",
    "        X_val_scaled,   y_val_scaled,\n",
    "        X_test_scaled,  y_test_scaled,\n",
    "        scaler_X,       scaler_y,  \n",
    "        train_dataset,  val_dataset,     test_dataset,\n",
    "        train_loader,   val_loader,     test_loader\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e625168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train, raceids_train, horseids_train,\n",
    " X_val,   y_val,   raceids_val,   horseids_val,\n",
    " X_test,  y_test,  raceids_test,  horseids_test\n",
    ")= prepare_data_3way_split_by_race(\n",
    "    df=all_pd, \n",
    "    race_date_col=\"race_date\",  # or None if you want lexicographic\n",
    "    train_frac=0.70,\n",
    "    val_frac=0.15,\n",
    "    test_frac=0.15,\n",
    "    target_len=38\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04458d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_train_scaled, y_train_scaled, y_train_unscaled, scaler_x, scaler_y,\n",
    "    X_val_scaled,   y_val_scaled,\n",
    "    X_test_scaled,  y_test_scaled,\n",
    "    scaler_X, scaler_y,\n",
    "    train_dataset, val_dataset, test_dataset,\n",
    "    train_loader, val_loader, test_loader\n",
    ") = prepare_SingleHorse_data(\n",
    "    X_train, y_train, raceids_train,\n",
    "    X_val,   y_val,   raceids_val,\n",
    "    X_test,  y_test,  raceids_test\n",
    ")\n",
    "\n",
    "print(\"Train loader has\", len(train_loader), \"batches.\")\n",
    "print(\"Val loader has\", len(val_loader), \"batches.\")\n",
    "print(\"Test loader has\", len(test_loader), \"batches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2992324",
   "metadata": {},
   "source": [
    "# 🧠 Basic LSTM Model for Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b760d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"Device name 0:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c2819",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd162c",
   "metadata": {},
   "source": [
    "# Train a Simple LSTM Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a46e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HorseTimeSeriesLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 6,\n",
    "        hidden_size: int = 64,\n",
    "        embedding_size: int = 16,\n",
    "        dropout: float = 0.0,\n",
    "        num_layers: int = 1       # ← new parameter with default\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,  # ← pass it here\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout   = nn.Dropout(dropout)\n",
    "        self.head      = nn.Linear(hidden_size, 1)\n",
    "        self.projection = nn.Linear(hidden_size, embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, input_size)\n",
    "        Returns:\n",
    "          pred => (batch_size,) numeric regression output\n",
    "          embed => (batch_size, embedding_size)\n",
    "        \"\"\"\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        last_hidden    = out[:, -1, :]\n",
    "        last_hidden    = self.dropout(last_hidden)\n",
    "        pred           = self.head(last_hidden).squeeze(1)\n",
    "        embed          = self.projection(last_hidden)\n",
    "        return pred, embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f974e2e",
   "metadata": {},
   "source": [
    "### 🧪 Mini Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d23231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "###############################################################################\n",
    "# Training Loop for One Epoch\n",
    "###############################################################################\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for X_batch, y_batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred, embed = model(X_batch)\n",
    "        loss = F.mse_loss(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    return total_loss / num_batches if num_batches > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba7d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Evaluation Loop\n",
    "###############################################################################\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            pred, embed = model(X_batch)\n",
    "            loss = F.mse_loss(pred, y_batch)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    return total_loss / num_batches if num_batches > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577af66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"Device 0:\", torch.cuda.get_device_name(0))\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Device 1:\", torch.cuda.get_device_name(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c2e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import optuna\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    import torch.optim as optim\n",
    "\n",
    "    # If you want to pick a specific GPU, or see how many you have\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(\"GPU Count:\", gpu_count)\n",
    "    device = torch.device(\"cuda\" if gpu_count > 0 else \"cpu\")\n",
    "\n",
    "    # 1) Hyperparams from Optuna\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 5e-3, log=True)  # e.g. between 5e-4 and 5e-3\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [32, 64, 128, 256])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.47, 0.9)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
    "\n",
    "    # 2) Build the single-horse LSTM\n",
    "    model = HorseTimeSeriesLSTM(\n",
    "        input_size=6,\n",
    "        hidden_size=hidden_size,\n",
    "        embedding_size=16,  # fixed\n",
    "        dropout=dropout,\n",
    "        num_layers=num_layers\n",
    "    )\n",
    "\n",
    "    # Optionally wrap in DataParallel if >1 GPU\n",
    "    # Note: This requires batch_size > 1 if we want to see a speedup across GPUs.\n",
    "    if gpu_count > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # 3) Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # 4) DataLoaders (single-horse datasets)\n",
    "    #    Make sure these have batch_size > 1 if you want multi-GPU usage to help.\n",
    "    #    For example, batch_size=16 or 32.\n",
    "    batch_size = 128\n",
    "    train_loader = DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True,\n",
    "                    num_workers=32,     # or 8, or more\n",
    "                    pin_memory=True    # often helps on GPU\n",
    "                )\n",
    "    train_loader = DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True,\n",
    "                    num_workers=32,     # or 8, or more\n",
    "                    pin_memory=True    # often helps on GPU\n",
    "                )\n",
    "    # 5) Early Stopping Setup\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = copy.deepcopy(model.state_dict())\n",
    "    patience = 5\n",
    "    epochs_without_improvement = 0\n",
    "    num_epochs = 50\n",
    "\n",
    "    # 6) Main Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss   = evaluate(model, val_loader, device)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss - 1e-5:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        # Optuna pruning\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # 7) Load best model state and return best_val_loss\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc96935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the Optuna study with SQLite database persistence\n",
    "def run_optuna(n_trials=100):\n",
    "    # Create a study with SQLite storage; it will load the study if it already exists\n",
    "    study = optuna.create_study(\n",
    "        study_name=\"horse_race_study_v1\",\n",
    "        storage=\"sqlite:///optuna_sec_lstm.db\",\n",
    "        direction=\"minimize\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    \n",
    "    # Optimize the objective function\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    # Print the best trial's details\n",
    "    print(\"Best trial:\")\n",
    "    best_trial = study.best_trial\n",
    "    print(\"  Validation Loss:\", best_trial.value)\n",
    "    print(\"  Best hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de91e1af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run Optuna tuning\n",
    "study = run_optuna(n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63acbba0",
   "metadata": {},
   "source": [
    "# Then Use the LSTM’s “Embedding” for a Ranking Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b3637",
   "metadata": {},
   "source": [
    "## Build the final model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd0265",
   "metadata": {},
   "source": [
    ">Best trial:\n",
    ">  Validation Loss: 0.002173538051654727\n",
    "\n",
    ">  Best hyperparameters:\n",
    "\n",
    ">    dropout: 0.7997257360718246\n",
    "\n",
    ">    hidden_size: 128\n",
    "\n",
    ">    lr: 0.003481825508715826\n",
    "\n",
    ">    num_layers: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best parameters from the study\n",
    "best_params = study.best_trial.params\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea37235",
   "metadata": {},
   "source": [
    "{'dropout': 0.7997257360718246, 'hidden_size': 128, 'lr': 0.003481825508715826, 'num_layers': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d52c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined_loader(train_dataset, val_dataset, batch_size=128, shuffle=True, \n",
    "                          pin_memory=True, num_workers=8):\n",
    "    combined_dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "    combined_loader = DataLoader(\n",
    "        combined_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return combined_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658587f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "class HorseTimeSeriesLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 6,\n",
    "        hidden_size: int = 128,\n",
    "        embedding_size: int = 16,\n",
    "        dropout: float = 0.7997257360718246,\n",
    "        num_layers: int = 3         # ← new argument\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,    # ← pass it here\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head = nn.Linear(hidden_size, 1)\n",
    "        self.projection = nn.Linear(hidden_size, embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, input_size)\n",
    "        Returns:\n",
    "          pred => (batch_size,) numeric regression output\n",
    "          embed => (batch_size, embedding_size) (optional use)\n",
    "        \"\"\"\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        last_hidden = out[:, -1, :]\n",
    "        last_hidden = self.dropout(last_hidden)\n",
    "        pred = self.head(last_hidden).squeeze(1)\n",
    "        embed = self.projection(last_hidden)\n",
    "        return pred, embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7395c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred, _ = model(X_batch)\n",
    "        loss = nn.functional.mse_loss(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    return total_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            pred, _ = model(X_batch)\n",
    "            loss = nn.functional.mse_loss(pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "    return total_loss / num_batches if num_batches > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a0732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your saved scalers (so you can re-scale the test set later)\n",
    "scaler_X = joblib.load(\n",
    "    \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/models/\"\n",
    "    \"LSTM/training_scalers_sectionals/scaler_X.pkl\"\n",
    ")\n",
    "scaler_y = joblib.load(\n",
    "    \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/models/\"\n",
    "    \"LSTM/training_scalers_sectionals/scaler_y.pkl\"\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "final_model = HorseTimeSeriesLSTM(\n",
    "    input_size=6,\n",
    "    hidden_size=best_params[\"hidden_size\"],\n",
    "    embedding_size=16,\n",
    "    dropout=0.2,                        # lower for full‐data training\n",
    "    num_layers=best_params[\"num_layers\"]\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(final_model.parameters(),\n",
    "                       lr=best_params[\"lr\"] * 0.1)\n",
    "\n",
    "\n",
    "# Combine train+val\n",
    "combined_loader = build_combined_loader(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    batch_size=128,  # you can pick whatever batch size you prefer\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_final = 50\n",
    "for epoch in range(num_epochs_final):\n",
    "    train_loss = train_one_epoch(final_model, combined_loader, optimizer, device)\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs_final}] combined_train_loss={train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f3996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Layers:\", final_model.lstm.num_layers)\n",
    "print(\"Hidden:\", final_model.lstm.hidden_size)\n",
    "print(\"Dropout:\", final_model.dropout.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd70c283",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49325d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(final_model, test_loader, device)\n",
    "print(f\"Final test loss = {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39aaa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_unscaled(model, test_loader, device, scaler_y):\n",
    "    \"\"\"\n",
    "    1) Make predictions in the scaled domain.\n",
    "    2) Inverse transform them to the real domain.\n",
    "    3) Compute MSE/RMSE/MAE in real units.\n",
    "    \"\"\"\n",
    "    model.eval().to(device)\n",
    "    all_preds_scaled, all_labels_scaled = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            Xb = X_batch.to(device)\n",
    "            pred_scaled = model(Xb)[0].cpu().numpy()\n",
    "            all_preds_scaled.append(pred_scaled)\n",
    "            all_labels_scaled.append(y_batch.numpy())\n",
    "\n",
    "    all_preds_scaled  = np.concatenate(all_preds_scaled, axis=0)\n",
    "    all_labels_scaled = np.concatenate(all_labels_scaled, axis=0)\n",
    "\n",
    "    pred_unscaled   = scaler_y.inverse_transform(all_preds_scaled.reshape(-1,1)).ravel()\n",
    "    truth_unscaled = scaler_y.inverse_transform(all_labels_scaled.reshape(-1,1)).ravel()\n",
    "\n",
    "    mse  = np.mean((pred_unscaled - truth_unscaled)**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae  = np.mean(np.abs(pred_unscaled - truth_unscaled))\n",
    "\n",
    "    print(f\"Test → MSE={mse:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f}\")\n",
    "    return rmse, mae, pred_unscaled, truth_unscaled\n",
    "\n",
    "# now just call it:\n",
    "rmse, mae, preds_sec, actual_sec = evaluate_unscaled(\n",
    "    final_model, test_loader, device, scaler_y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1343408b",
   "metadata": {},
   "source": [
    "# Update Horse Sectionals Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, X, horse_ids):\n",
    "        \"\"\"\n",
    "        X: shape (N, seq_len, input_size) -> features for each horse\n",
    "        horse_ids: shape (N,) -> integer IDs or some identifier\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.horse_ids = horse_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return (features, horse_id)\n",
    "        x_i = self.X[idx]\n",
    "        h_id = self.horse_ids[idx]\n",
    "        return x_i, h_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da267800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Combine them:\n",
    "X_all = np.concatenate([X_train_scaled, X_val_scaled, X_test_scaled], axis=0)\n",
    "X_all = X_all.astype(np.float32)  \n",
    "all_horse_ids = np.concatenate([horseids_train, horseids_val, horseids_test], axis=0)\n",
    "\n",
    "inference_dataset = InferenceDataset(X_all, all_horse_ids)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce57b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# We'll assume final_model(...) => (raw_score, embed_vec)\n",
    "# raw_score: shape (batch,), embed_vec: shape (batch, 8)\n",
    "final_model.eval()\n",
    "\n",
    "projected_scores = {}   # {horse_id: raw_score}\n",
    "projected_embeds = {}   # {horse_id: embedding array}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, ids_batch in inference_loader:\n",
    "        X_batch = X_batch.to(device)            # shape (batch_size, seq_len, input_size)\n",
    "        raw_scores, embed_vecs = final_model(X_batch)  \n",
    "        # raw_scores => (batch_size,)\n",
    "        # embed_vecs => (batch_size, embed_size)\n",
    "\n",
    "        # Move them back to CPU + NumPy\n",
    "        raw_scores_np = raw_scores.cpu().numpy()     # shape (batch_size,)\n",
    "        embed_vecs_np = embed_vecs.cpu().numpy()     # shape (batch_size, embed_size)\n",
    "\n",
    "        # For each item in the batch, store in the dictionaries\n",
    "        for i in range(len(ids_batch)):\n",
    "            horse_id = int(ids_batch[i])  # cast to int if needed\n",
    "            projected_scores[horse_id] = float(raw_scores_np[i])\n",
    "            projected_embeds[horse_id]   = embed_vecs_np[i]  # e.g. shape (embed_size,)\n",
    "            \n",
    "# 2) Min–max scale the raw scores to [0..1000]\n",
    "scores_array = np.array(list(projected_scores.values()))\n",
    "min_s = scores_array.min()\n",
    "max_s = scores_array.max()\n",
    "range_s = max_s - min_s if max_s > min_s else 1e-9\n",
    "\n",
    "scaled_scores = {}\n",
    "for horse_id, raw_score in projected_scores.items():\n",
    "    normalized = (raw_score - min_s) / range_s   # [0..1]\n",
    "    scaled_val = normalized * 1000.0            # [0..1000]\n",
    "    scaled_scores[horse_id] = scaled_val\n",
    "\n",
    "# 3) Build a Spark DataFrame with columns:\n",
    "#  [horse_id, score, dim1, dim2, ..., dim8]\n",
    "\n",
    "rows = []\n",
    "for horse_id, sc in scaled_scores.items():\n",
    "    emb = projected_embeds[horse_id]\n",
    "    row_dict = {\n",
    "        \"horse_id\": horse_id,\n",
    "        \"score\": float(sc)\n",
    "    }\n",
    "    # Suppose embed size is 8 (like your example)\n",
    "    for i, val in enumerate(emb):\n",
    "        row_dict[f\"dim{i+1}\"] = float(val)\n",
    "\n",
    "    rows.append(Row(**row_dict))\n",
    "\n",
    "df_scores = spark.createDataFrame(rows)\n",
    "\n",
    "# 4) Write to DB via Spark JDBC\n",
    "staging_table = \"horse_sectionals_lstm\"\n",
    "\n",
    "(\n",
    "    df_scores.write\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", jdbc_url)\n",
    "    .option(\"dbtable\", staging_table)\n",
    "    .option(\"user\", jdbc_properties[\"user\"])\n",
    "    .option(\"driver\", jdbc_properties[\"driver\"])\n",
    "    .mode(\"overwrite\")  # or \"append\"\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef288dec",
   "metadata": {},
   "source": [
    "# Sectionals Disribution of Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4149ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming df_scores is your Spark DataFrame\n",
    "df_scores_pandas = df_scores.select(\"score\").toPandas()\n",
    "\n",
    "# Plot the distribution of scores with more bins\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_scores_pandas[\"score\"], bins=100, kde=True)  # Increase the number of bins to 100\n",
    "plt.title(\"Distribution of Scores\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979fc56c",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_model.state_dict(), \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/models/LSTM/sectionals_lstm_20250426.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c149e412",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d24b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best parameters from the study\n",
    "best_params = study.best_trial.params\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HorseTimeSeriesLSTM(input_size=6, hidden_size=128, num_layers=3, dropout=0.2)\n",
    "model.load_state_dict(torch.load(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/models/LSTM/sectionals_lstm_20250426.pt\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60a450",
   "metadata": {},
   "source": [
    "alter table horse_scores_lstm_gps\n",
    "add CONSTRAINT horse_gps_lstm_id_key UNIQUE (horse_id);\n",
    "\n",
    "CREATE INDEX idx_horse_gps_lstm ON public.horse_scores_lstm_gps USING btree (horse_id);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2943fd00",
   "metadata": {},
   "source": [
    "### MSE or MAE on Unscaled Values, e.g.m “actual seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e57e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have your trained model and your scaler_y from training:\n",
    "\n",
    "# 1) Get predicted values on the test set (scaled):\n",
    "scaled_preds = []\n",
    "scaled_truth = []\n",
    "\n",
    "for X_batch, y_batch in test_loader:\n",
    "    X_batch = X_batch.to(device)\n",
    "    # forward pass\n",
    "    pred_batch, _ = model(X_batch)\n",
    "    # Move back to numpy\n",
    "    pred_batch_np = pred_batch.cpu().detach().numpy()\n",
    "    scaled_preds.append(pred_batch_np)\n",
    "    scaled_truth.append(y_batch.numpy())\n",
    "\n",
    "scaled_preds = np.concatenate(scaled_preds, axis=0)\n",
    "scaled_truth = np.concatenate(scaled_truth, axis=0)\n",
    "\n",
    "# 2) Invert the transform (assuming a MinMaxScaler or StandardScaler)\n",
    "unscaled_preds = scaler_y.inverse_transform(scaled_preds.reshape(-1, 1)).flatten()\n",
    "unscaled_truth = scaler_y.inverse_transform(scaled_truth.reshape(-1, 1)).flatten()\n",
    "\n",
    "# 3) Compute MSE/MAE in real units:\n",
    "mse_unscaled = np.mean((unscaled_preds - unscaled_truth)**2)\n",
    "mae_unscaled = np.mean(np.abs(unscaled_preds - unscaled_truth))\n",
    "\n",
    "print(\"MSE in real (seconds) domain:\", mse_unscaled)\n",
    "print(\"MAE in real (seconds) domain:\", mae_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0acc1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_unscaled(model, test_loader, device, scaler_y):\n",
    "    \"\"\"\n",
    "    1) Make predictions in the scaled domain over *all* batches.\n",
    "    2) Inverse-transform the *entire* arrays back to real units.\n",
    "    3) Compute & print MSE, RMSE, MAE in the real domain.\n",
    "    \"\"\"\n",
    "    model.eval().to(device)\n",
    "    all_preds_scaled  = []\n",
    "    all_labels_scaled = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            Xb = X_batch.to(device)\n",
    "            pred_scaled_t, _ = model(Xb)                # (batch_size,)\n",
    "            all_preds_scaled.append(pred_scaled_t.cpu().numpy())\n",
    "            all_labels_scaled.append(y_batch.numpy())\n",
    "\n",
    "    # Concatenate across all batches\n",
    "    all_preds_scaled  = np.concatenate(all_preds_scaled, axis=0)   # (N_test,)\n",
    "    all_labels_scaled = np.concatenate(all_labels_scaled, axis=0)  # (N_test,)\n",
    "\n",
    "    # Inverse-transform the *full* arrays\n",
    "    pred_unscaled  = scaler_y.inverse_transform(\n",
    "                        all_preds_scaled.reshape(-1,1)\n",
    "                     ).ravel()\n",
    "    label_unscaled = scaler_y.inverse_transform(\n",
    "                        all_labels_scaled.reshape(-1,1)\n",
    "                     ).ravel()\n",
    "\n",
    "    # Compute real-domain metrics\n",
    "    mse  = np.mean((pred_unscaled - label_unscaled)**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae  = np.mean(np.abs(pred_unscaled - label_unscaled))\n",
    "\n",
    "    print(f\"Test set → MSE={mse:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f}\")\n",
    "    return rmse, mae, pred_unscaled, label_unscaled, all_preds_scaled\n",
    "\n",
    "# Call it like this:\n",
    "rmse, mae, preds_sec, actual_sec, scaled_preds = evaluate_unscaled(\n",
    "    final_model, test_loader, device, scaler_y\n",
    ")\n",
    "\n",
    "print(f\"Final RMSE in real domain: {rmse:.3f}\")\n",
    "print(f\"Final MAE  in real domain: {mae:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb9c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse, mae, preds_sec, actual_sec, pred_scaled = evaluate_unscaled(final_model, test_loader, device, scaler_y)\n",
    "\n",
    "print(f\"RMSE in real domain: {rmse:.3f}\")\n",
    "print(f\"MAE  in real domain: {mae:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d6776",
   "metadata": {},
   "source": [
    "## Compare Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e46d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_unscaled = scaler_y.inverse_transform(y_train_scaled.reshape(-1, 1)).flatten()\n",
    "y_val_unscaled   = scaler_y.inverse_transform(y_val_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_unscaled  = scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "y_unscaled = all_pd[\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17739a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_unscaled = scaler_y.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_mean_baseline_metrics(y_train_unscaled, y_test_unscaled):\n",
    "    \"\"\"\n",
    "    1) Compute the mean finishing time on the training set.\n",
    "    2) Create baseline predictions => the same mean time for every test row.\n",
    "    3) Compute MSE, RMSE, and MAE in real (seconds) domain.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Mean finishing time from training set\n",
    "    train_mean_time = np.mean(y_train_unscaled)\n",
    "\n",
    "    # 2) Baseline predictions: fill every test sample with 'train_mean_time'\n",
    "    baseline_pred = np.full_like(y_test_unscaled, train_mean_time)\n",
    "\n",
    "    # 3) Metrics\n",
    "    mse  = np.mean((baseline_pred - y_test_unscaled)**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae  = np.mean(np.abs(baseline_pred - y_test_unscaled))\n",
    "\n",
    "    print(f\"[Mean Baseline]\")\n",
    "    print(f\"  Train mean time = {train_mean_time:.3f} seconds\")\n",
    "    print(f\"  Test MSE  = {mse:.3f}\")\n",
    "    print(f\"  Test RMSE = {rmse:.3f}\")\n",
    "    print(f\"  Test MAE  = {mae:.3f}\")\n",
    "\n",
    "    return mse, rmse, mae\n",
    "\n",
    "# EXAMPLE USAGE (assuming you already have unscaled arrays):\n",
    "# y_train_unscaled = ...\n",
    "# y_test_unscaled  = ...\n",
    "# compute_mean_baseline_metrics(y_train_unscaled, y_test_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939dc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST: invert them to get unscaled times:\n",
    "y_train_unscaled = scaler_y.inverse_transform(y_train_scaled.reshape(-1,1)).ravel()\n",
    "y_test_unscaled  = scaler_y.inverse_transform(y_test_scaled.reshape(-1,1)).ravel()\n",
    "\n",
    "# THEN: call the baseline function:\n",
    "compute_mean_baseline_metrics(y_train_unscaled, y_test_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd99c659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfabb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
