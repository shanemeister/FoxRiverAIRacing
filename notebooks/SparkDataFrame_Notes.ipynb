{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0e2e79",
   "metadata": {},
   "source": [
    "Below are some commonly used PySpark DataFrame actions and transformations, along with brief explanations. You can chain many of these methods together, and also combine them with .select(...).show(10, truncate=False) to inspect subsets of your data:\n",
    "\t1.\tdf.dtypes\n",
    "\t•\tPurpose: Returns a list of (columnName, columnType) pairs for the DataFrame.\n",
    "\t•\tUsage:\n",
    "\n",
    "print(df.dtypes)\n",
    "\n",
    "\n",
    "\t2.\tdf.columns\n",
    "\t•\tPurpose: Returns a list of the column names.\n",
    "\t•\tUsage:\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "\t3.\tdf.printSchema()\n",
    "\t•\tPurpose: Prints the schema of the DataFrame in a tree format.\n",
    "\t•\tUsage:\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "\t4.\tdf.count()\n",
    "\t•\tPurpose: Returns the number of rows in the DataFrame.\n",
    "\t•\tUsage:\n",
    "\n",
    "row_count = df.count()\n",
    "\n",
    "\n",
    "\t5.\tdf.head(n) or df.take(n)\n",
    "\t•\tPurpose: Returns the first n rows as a list of Row objects.\n",
    "\t•\tUsage:\n",
    "\n",
    "first_ten_rows = df.head(10)\n",
    "\n",
    "\n",
    "\t6.\tdf.show(n, truncate=False)\n",
    "\t•\tPurpose: Prints the first n rows in a tabular form.\n",
    "\t•\tUsage:\n",
    "\n",
    "df.show(10, truncate=False)\n",
    "\n",
    "\n",
    "\t7.\t*df.select(cols)\n",
    "\t•\tPurpose: Selects a subset of columns.\n",
    "\t•\tUsage:\n",
    "\n",
    "df.select(\"col1\", \"col2\").show(10, truncate=False)\n",
    "\n",
    "\n",
    "You can chain .select(...) before .show(...) to print only certain columns:\n",
    "\n",
    "df.select(\"course_cd\", \"race_date\", \"horse_id\").show(10, truncate=False)\n",
    "\n",
    "\n",
    "\t8.\tdf.filter(condition) or df.where(condition)\n",
    "\t•\tPurpose: Filters rows based on a condition.\n",
    "\t•\tUsage:\n",
    "\n",
    "df.filter(col(\"speed\") > 10).select(\"course_cd\", \"horse_id\").show(10, truncate=False)\n",
    "\n",
    "\n",
    "\t9.\t*df.groupBy(cols)\n",
    "\t•\tPurpose: Groups DataFrame by the specified columns and returns a GroupedData object for aggregation.\n",
    "\t•\tUsage:\n",
    "\n",
    "df.groupBy(\"course_cd\").count().show(10, truncate=False)\n",
    "\n",
    "\n",
    "\t10.\tdf.agg(…)\n",
    "\t•\tPurpose: Use aggregation functions like count, mean, sum on the entire DataFrame or grouped data.\n",
    "\t•\tUsage:\n",
    "\n",
    "from pyspark.sql.functions import mean, count\n",
    "df.agg(mean(\"speed\").alias(\"avg_speed\"), count(\"*\").alias(\"row_count\")).show()\n",
    "\n",
    "\n",
    "\t11.\t*df.orderBy(cols) or *df.sort(cols)\n",
    "\t•\tPurpose: Sorts the DataFrame by specified columns.\n",
    "\t•\tUsage:\n",
    "\n",
    "df.orderBy(\"speed\").select(\"course_cd\", \"speed\").show(10, truncate=False)\n",
    "\n",
    "\n",
    "\t12.\tdf.distinct()\n",
    "\t•\tPurpose: Returns a new DataFrame containing distinct rows.\n",
    "\t•\tUsage:\n",
    "\n",
    "distinct_horses = df.select(\"horse_id\").distinct()\n",
    "distinct_horses.show(10, truncate=False)\n",
    "\n",
    "\n",
    "\t13.\t*df.drop(cols)\n",
    "\t•\tPurpose: Drops specified columns.\n",
    "\t•\tUsage:\n",
    "\n",
    "df.drop(\"time_stamp\", \"location\").show(10, truncate=False)\n",
    "\n",
    "\n",
    "\t14.\tdf.withColumn(newColName, expression)\n",
    "\t•\tPurpose: Adds or replaces a column based on a column expression.\n",
    "\t•\tUsage:\n",
    "\n",
    "from pyspark.sql.functions import col, lit\n",
    "df.withColumn(\"adjusted_speed\", col(\"speed\") * lit(1.1)).show(10, truncate=False)\n",
    "\n",
    "\n",
    "\t15.\t*df.describe(cols)\n",
    "\t•\tPurpose: Computes basic statistics for numeric columns.\n",
    "\t•\tUsage:\n",
    "\n",
    "df.describe(\"speed\", \"progress\").show()\n",
    "\n",
    "\n",
    "    16. Check to see if a column contains nulls/missing values:\n",
    "    \n",
    "    results.select(\"equip\").filter(\"equip IS NULL\").show()\n",
    "    OR\n",
    "    results.filter(col(\"equip\").isNull()).count()\n",
    "    \n",
    "Combining Methods\n",
    "\n",
    "You can chain these methods. For example, to show 10 records of a filtered dataset with only certain columns:\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col(\"speed\") > 10).select(\"course_cd\", \"horse_id\", \"speed\").show(10, truncate=False)\n",
    "\n",
    "This command:\n",
    "\t•\tFilters rows where speed > 10\n",
    "\t•\tSelects only course_cd, horse_id, and speed columns\n",
    "\t•\tDisplays the first 10 rows without truncating string columns.\n",
    "\n",
    "By mixing and matching these operations, you can inspect, transform, and analyze your data effectively before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72c1ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
