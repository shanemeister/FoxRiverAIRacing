{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37cdfb53",
   "metadata": {},
   "source": [
    "# Modeling Strategy\n",
    "\n",
    "> 1.\tIndividual Models for Different Labels:\n",
    "\n",
    ">   •\tBinary Top-4 Model (Model A):\n",
    "\n",
    ">.   Predicts whether a horse finishes in the top 4 (1) or not (0).\n",
    "\n",
    ">\t•\tBinary Win Model (Model B):\n",
    ">.   Predicts whether the horse wins (1) or not (0).\n",
    "\t\n",
    ">    •\tOrdinal Model (Model C):\n",
    "\n",
    "> Attempts to predict the exact finishing position (e.g., a regression model outputting a continuous value you can round to an integer, or an ordinal classification model that outputs probabilities for each rank).\n",
    "\n",
    ">    •\tMulti-Class Model (Model D):\n",
    "\n",
    ">.   Classifies a horse into one of several buckets:\n",
    "\n",
    ">.   •\tClass 0: Winner (finish = 1)\n",
    "\n",
    ">.   •\tClass 1: Top-3 but not win (2 ≤ finish ≤ 3)\n",
    "\n",
    ">.   •\tClass 2: Top-4 but not top-3 (finish = 4)\n",
    "\n",
    ">.   •\tClass 3: Outside top-4 (finish > 4)\n",
    "\n",
    "> 2.\tEnsembling the Results:\n",
    "After training these models, you’ll have a set of predictions per horse. For example, from a single race:\n",
    "\n",
    ">.   •\tModel A gives a probability of top-4 (P(top4))\n",
    "\n",
    ">.   •\tModel B gives a probability of winning (P(win))\n",
    "\n",
    ">.   •\tModel C gives a predicted finishing position or a probability distribution over positions (depending on the approach)\n",
    "\n",
    ">.   •\tModel D gives probabilities for each of the four categories (P(win), P(top3_not_win), P(top4_not_top3), P(outside_top4))\n",
    "\t\n",
    "> 3.\tCombining Predictions:\n",
    "\n",
    ">.   You can combine these outputs using:\n",
    "\n",
    ">.   •\tA Weighted Average or Voting Scheme:\n",
    "\n",
    "> For instance, to estimate the probability of finishing 1st, you might weigh Model B’s probability of winning heavily, and also consider Model D’s probability for the “win” class. If both strongly indicate a win, you trust that horse as a likely winner.\n",
    "\n",
    "> •\tDecision Rules:\n",
    "For example:\n",
    "\n",
    "•\tIf Model B’s P(win) is high and Model D also puts most probability mass on “win” class, that horse is likely first.\n",
    "\n",
    "•\tIf Model A’s P(top4) is high but Model B’s P(win) is low, and Model D suggests a class in top 4 but not top 3, you can infer something about its likely finishing bracket.\n",
    "\n",
    "•\tA Second-Level Model (Stacking):\n",
    "You could train another model (e.g., a logistic regression or another XGBoost) that takes as input the predictions from all the above models and outputs a refined probability distribution over exact finishes or top-4 ordering. This “meta-model” learns how best to combine the signals from each specialized model.\n",
    "\n",
    "> 4.\tInterpreting the Final Outcome for First Four Positions:\n",
    "\n",
    "With the combined predictions, you could:\n",
    "\n",
    "•\tRank horses by their predicted probability of being the winner.\n",
    "\n",
    "•\tAmong the remaining horses, rank them by their predicted probability of making top-3 or top-4, etc.\n",
    "\n",
    "Essentially, by merging the signals from these various target definitions, you might get a more robust and nuanced understanding of each horse’s chances. For instance:\n",
    "\n",
    "•\tModel B and D give good signals for the winner.\n",
    "\n",
    "•\tModel A and D help solidify who belongs in the top-4 bracket.\n",
    "\n",
    "•\tModel C could refine ordering among the top candidates if it provides a direct finishing position estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61679ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import xgboost\n",
    "from pyspark.sql.functions import (col, count, row_number, abs, unix_timestamp, mean, \n",
    "                                   when, lit, min as spark_min, max as spark_max , \n",
    "                                   row_number, mean, countDistinct, last, first)\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession\n",
    "from src.data_preprocessing.data_prep1.sql_queries import sql_queries\n",
    "from src.data_preprocessing.data_prep1.data_loader import load_data_from_postgresql, reload_parquet_files\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame, Window\n",
    "from src.data_preprocessing.data_prep1.data_utils import (save_parquet, gather_statistics, \n",
    "                initialize_environment, load_config, initialize_logging, initialize_spark, \n",
    "                drop_duplicates_with_tolerance, identify_and_impute_outliers, \n",
    "                identify_and_remove_outliers, identify_missing_and_outliers)\n",
    "# Set global references to None\n",
    "spark = None\n",
    "master_results_df = None\n",
    "sectional_results = None\n",
    "results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8d1fc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 20:17:57,641 - INFO - Environment setup initialized.\n",
      "2024-12-17 20:17:57,644 - INFO - Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "spark, jdbc_url, jdbc_properties, queries, parquet_dir, log_file = initialize_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ac8872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.read.parquet(os.path.join(parquet_dir, \"results.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc78050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00038843",
   "metadata": {},
   "source": [
    "# Everything below is scratch pad and not part of this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5301be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpspoint = spark.read.parquet(os.path.join(parquet_dir, \"gpspoint.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eeedbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = spark.read.parquet(os.path.join(parquet_dir, \"merged_df.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5b53dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- course_cd: string (nullable = true)\n",
      " |-- race_date: date (nullable = true)\n",
      " |-- race_number: integer (nullable = true)\n",
      " |-- saddle_cloth_number: string (nullable = true)\n",
      " |-- sectionals_gate_name: string (nullable = true)\n",
      " |-- official_fin: integer (nullable = true)\n",
      " |-- purse: integer (nullable = true)\n",
      " |-- wps_pool: decimal(10,2) (nullable = true)\n",
      " |-- weight: decimal(10,2) (nullable = true)\n",
      " |-- date_of_birth: date (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- start_position: long (nullable = true)\n",
      " |-- equip: string (nullable = true)\n",
      " |-- claimprice: double (nullable = true)\n",
      " |-- surface: string (nullable = true)\n",
      " |-- surface_type_description: string (nullable = true)\n",
      " |-- trk_cond: string (nullable = true)\n",
      " |-- trk_cond_desc: string (nullable = true)\n",
      " |-- weather: string (nullable = true)\n",
      " |-- distance: decimal(10,2) (nullable = true)\n",
      " |-- dist_unit: string (nullable = true)\n",
      " |-- power: decimal(10,2) (nullable = true)\n",
      " |-- med: string (nullable = true)\n",
      " |-- morn_odds: decimal(10,2) (nullable = true)\n",
      " |-- avgspd: double (nullable = true)\n",
      " |-- avgcls: double (nullable = true)\n",
      " |-- jock_key: string (nullable = true)\n",
      " |-- train_key: string (nullable = true)\n",
      " |-- race_type: string (nullable = true)\n",
      " |-- class_rating: integer (nullable = true)\n",
      " |-- net_sentiment: integer (nullable = true)\n",
      " |-- stk_clm_md: string (nullable = true)\n",
      " |-- turf_mud_mark: string (nullable = true)\n",
      " |-- avg_spd_sd: double (nullable = true)\n",
      " |-- ave_cl_sd: double (nullable = true)\n",
      " |-- hi_spd_sd: double (nullable = true)\n",
      " |-- pstyerl: double (nullable = true)\n",
      " |-- all_starts: integer (nullable = true)\n",
      " |-- all_win: integer (nullable = true)\n",
      " |-- all_place: integer (nullable = true)\n",
      " |-- all_show: integer (nullable = true)\n",
      " |-- all_fourth: integer (nullable = true)\n",
      " |-- all_earnings: decimal(12,2) (nullable = true)\n",
      " |-- cond_starts: integer (nullable = true)\n",
      " |-- cond_win: integer (nullable = true)\n",
      " |-- cond_place: integer (nullable = true)\n",
      " |-- cond_show: integer (nullable = true)\n",
      " |-- cond_fourth: integer (nullable = true)\n",
      " |-- cond_earnings: decimal(12,2) (nullable = true)\n",
      " |-- sectionals_gate_numeric: double (nullable = true)\n",
      " |-- sectionals_length_to_finish: double (nullable = true)\n",
      " |-- sectionals_sectional_time: double (nullable = true)\n",
      " |-- sectionals_running_time: double (nullable = true)\n",
      " |-- sectionals_distance_back: double (nullable = true)\n",
      " |-- sectionals_distance_ran: double (nullable = true)\n",
      " |-- sectionals_number_of_strides: double (nullable = true)\n",
      " |-- sectionals_post_time: timestamp (nullable = true)\n",
      " |-- earliest_time_stamp: timestamp (nullable = true)\n",
      " |-- sec_time_stamp: timestamp (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- end_time: timestamp (nullable = true)\n",
      " |-- gps_section_avg_speed: double (nullable = true)\n",
      " |-- gps_section_avg_stride_freq: double (nullable = true)\n",
      " |-- gps_max_speed: double (nullable = true)\n",
      " |-- gps_min_speed: double (nullable = true)\n",
      " |-- gps_first_location: string (nullable = true)\n",
      " |-- gps_last_location: string (nullable = true)\n",
      " |-- gps_avg_acceleration: double (nullable = true)\n",
      " |-- gps_max_acceleration: double (nullable = true)\n",
      " |-- gps_min_acceleration: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a42d06",
   "metadata": {},
   "source": [
    "1.\tConvert a Small Subset to Pandas:\n",
    "Spark DataFrames are generally too large to visualize directly. Instead, select a manageable sample or a single race and convert it to a Pandas DataFrame. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eba2ea8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Passing in 'datetime64' dtype with no precision is not allowed. Please pass in 'datetime64[ns]' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Let's say final_df is your resulting Spark DataFrame\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Choose one race or a small subset\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sample_df \u001b[38;5;241m=\u001b[39m merged_df\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[1;32m      4\u001b[0m     (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcourse_cd\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAQU\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m&\u001b[39m \n\u001b[1;32m      5\u001b[0m     (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrace_date\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-11-16\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m&\u001b[39m \n\u001b[1;32m      6\u001b[0m     (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrace_number\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      7\u001b[0m )\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m1000\u001b[39m) \u001b[38;5;66;03m# Just a small limit for sampling\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m pandas_df \u001b[38;5;241m=\u001b[39m \u001b[43msample_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:251\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m should_check_timedelta \u001b[38;5;241m=\u001b[39m is_timedelta64_dtype(t) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pdf) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_timedelta64_dtype(t)) \u001b[38;5;129;01mor\u001b[39;00m should_check_timedelta:\n\u001b[0;32m--> 251\u001b[0m     series \u001b[38;5;241m=\u001b[39m \u001b[43mseries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PerformanceWarning\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pandas/core/generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   6639\u001b[0m     ]\n\u001b[1;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pandas/core/internals/managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pandas/core/internals/blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pandas/core/dtypes/astype.py:110\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_np_dtype(dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatetimeArray\n\u001b[0;32m--> 110\u001b[0m     dta \u001b[38;5;241m=\u001b[39m \u001b[43mDatetimeArray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dta\u001b[38;5;241m.\u001b[39m_ndarray\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_np_dtype(dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py:327\u001b[0m, in \u001b[0;36mDatetimeArray._from_sequence\u001b[0;34m(cls, scalars, dtype, copy)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_sequence\u001b[39m(\u001b[38;5;28mcls\u001b[39m, scalars, \u001b[38;5;241m*\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_sequence_not_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscalars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py:354\u001b[0m, in \u001b[0;36mDatetimeArray._from_sequence_not_strict\u001b[0;34m(cls, data, dtype, copy, tz, freq, dayfirst, yearfirst, ambiguous)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     tz \u001b[38;5;241m=\u001b[39m timezones\u001b[38;5;241m.\u001b[39mmaybe_get_tz(tz)\n\u001b[0;32m--> 354\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_dt64_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# if dtype has an embedded tz, capture it\u001b[39;00m\n\u001b[1;32m    356\u001b[0m tz \u001b[38;5;241m=\u001b[39m _validate_tz_from_dtype(dtype, tz, explicit_tz_none)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py:2544\u001b[0m, in \u001b[0;36m_validate_dt64_dtype\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m   2538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM8\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2539\u001b[0m     \u001b[38;5;66;03m# no precision, disallowed GH#24806\u001b[39;00m\n\u001b[1;32m   2540\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime64\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dtype with no precision is not allowed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2542\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime64[ns]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2543\u001b[0m     )\n\u001b[0;32m-> 2544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_supported_dtype(dtype))\n\u001b[1;32m   2549\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, (np\u001b[38;5;241m.\u001b[39mdtype, DatetimeTZDtype)):\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2551\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected value for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2552\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime64[s]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime64[ms]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime64[us]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2553\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime64[ns]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or DatetimeTZDtype\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2554\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Passing in 'datetime64' dtype with no precision is not allowed. Please pass in 'datetime64[ns]' instead."
     ]
    }
   ],
   "source": [
    "# Let's say final_df is your resulting Spark DataFrame\n",
    "# Choose one race or a small subset\n",
    "sample_df = merged_df.filter(\n",
    "    (col(\"course_cd\") == \"AQU\") & \n",
    "    (col(\"race_date\") == \"2024-11-16\") & \n",
    "    (col(\"race_number\") == 5)\n",
    ").limit(1000) # Just a small limit for sampling\n",
    "\n",
    "pandas_df = sample_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a2fec",
   "metadata": {},
   "source": [
    "Use Python Plotting Libraries (e.g., Matplotlib, Seaborn, Plotly):\n",
    "With pandas_df, you can use standard Python data visualization libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa4534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example: Plot avg speed by gate\n",
    "sns.barplot(data=pandas_df, x=\"sectionals_gate_name\", y=\"gps_section_avg_speed\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Average Speed by Gate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c82ad70",
   "metadata": {},
   "source": [
    "3.\tVisualize Race Progression Over Time:\n",
    "If you have time-based columns like sec_time_stamp, you can plot how speed or stride_frequency changes over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by time\n",
    "pandas_df = pandas_df.sort_values(by=\"sec_time_stamp\")\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(pandas_df[\"sec_time_stamp\"], pandas_df[\"gps_section_avg_speed\"], marker='o')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Average Speed Over Sections\")\n",
    "plt.xlabel(\"Section Timestamp\")\n",
    "plt.ylabel(\"Avg Speed (m/s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f29c8",
   "metadata": {},
   "source": [
    "\t4.\tCheck Acceleration Patterns:\n",
    "If you’ve computed average acceleration per section, you can plot a line chart of acceleration over the race sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b8f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(pandas_df[\"sec_time_stamp\"], pandas_df[\"avg_acceleration\"], marker='o', color='red')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Average Acceleration Over Sections\")\n",
    "plt.xlabel(\"Section Timestamp\")\n",
    "plt.ylabel(\"Avg Acceleration (m/s^2)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1ba326",
   "metadata": {},
   "source": [
    "\t5.\tMapping Horse Locations on a Track (Optional):\n",
    "If you have longitude/latitude data and a small subset, you can visualize the horse’s path. Convert the gps_first_location or gps_last_location into separate longitude/latitude columns (if they are WKT POINT strings, parse them). For example, if gps_first_location is a WKT “POINT (lon lat)”, you might extract coordinates using Python string operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a362de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming gps_first_location is something like \"POINT (lon lat)\"\n",
    "pandas_df[\"gps_first_lon\"] = pandas_df[\"gps_first_location\"].apply(lambda s: float(s.split(\"(\")[1].split(\" \")[0]))\n",
    "pandas_df[\"gps_first_lat\"] = pandas_df[\"gps_first_location\"].apply(lambda s: float(s.split(\"(\")[1].split(\" \")[1].replace(\")\",\"\")))\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(pandas_df[\"gps_first_lon\"], pandas_df[\"gps_first_lat\"], c='blue', marker='o')\n",
    "plt.title(\"First GPS Location in Each Section\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7be2dd",
   "metadata": {},
   "source": [
    "\t6.\tInteractive Tools (Jupyter Notebook):\n",
    "If you’re in a Jupyter environment, you can use Plotly or other interactive visualization tools to gain more insight.\n",
    "\n",
    "In Summary:\n",
    "\t•\tConvert a manageable subset of your Spark DataFrame to Pandas.\n",
    "\t•\tUse standard Python libraries to plot speed, acceleration, stride frequency, etc., against time or section indices.\n",
    "\t•\tIf geographic visualization is desired, parse coordinates and plot them on a scatter plot or use a map visualization library.\n",
    "\n",
    "This approach will help you see if the results line up with expectations (e.g., speeds increasing or decreasing logically, acceleration patterns making sense, locations along the track consistent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b8385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f97ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4489a5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc2bd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11600f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef75b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71acd5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189bfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac05ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf558e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf9116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b366b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f063a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e2ffd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed222702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e73e2ff",
   "metadata": {},
   "source": [
    "# Everything above is scratch pad and not part of this analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb399090",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "Before beginning here, go to /src/data_preprocessing/data_prep2/main_data_prep2.py and run data_check and cardinality reports. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908c1db",
   "metadata": {},
   "source": [
    "### Convert decimal columns to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e0dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distinct_distances = results.select(\"distance\", \"dist_unit\").distinct()\n",
    "#distinct_distances.show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.withColumn(\"distance\", col(\"distance\").cast(\"double\"))\n",
    "results = results.withColumn(\"wps_pool\", col(\"wps_pool\").cast(\"double\"))\n",
    "results = results.withColumn(\"weight\", col(\"weight\").cast(\"double\"))\n",
    "results = results.withColumn(\"power\", col(\"power\").cast(\"double\"))\n",
    "results = results.withColumn(\"morn_odds\", col(\"morn_odds\").cast(\"double\"))\n",
    "results = results.withColumn(\"all_earnings\", col(\"all_earnings\").cast(\"double\"))\n",
    "results = results.withColumn(\"cond_earnings\", col(\"cond_earnings\").cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae1e09",
   "metadata": {},
   "source": [
    "### date_of_birth: Impute with Global median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b92bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, count, expr\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Convert date_of_birth to a numeric timestamp for median calculation\n",
    "results = results.withColumn(\"date_of_birth_ts\", col(\"date_of_birth\").cast(\"timestamp\").cast(\"long\"))\n",
    "\n",
    "# Calculate the median of date_of_birth\n",
    "median_window = Window.orderBy(\"date_of_birth_ts\")\n",
    "row_count = results.filter(col(\"date_of_birth_ts\").isNotNull()).count()\n",
    "\n",
    "if row_count % 2 == 0:  # Even number of rows\n",
    "    median_row_1 = row_count // 2\n",
    "    median_row_2 = median_row_1 + 1\n",
    "    median_ts = results.filter(col(\"date_of_birth_ts\").isNotNull()) \\\n",
    "        .select(\"date_of_birth_ts\") \\\n",
    "        .withColumn(\"row_num\", expr(\"row_number() over (ORDER BY date_of_birth_ts)\")) \\\n",
    "        .filter((col(\"row_num\") == median_row_1) | (col(\"row_num\") == median_row_2)) \\\n",
    "        .groupBy().agg(expr(\"avg(date_of_birth_ts)\").alias(\"median_ts\")) \\\n",
    "        .collect()[0][\"median_ts\"]\n",
    "else:  # Odd number of rows\n",
    "    median_row = (row_count + 1) // 2\n",
    "    median_ts = results.filter(col(\"date_of_birth_ts\").isNotNull()) \\\n",
    "        .select(\"date_of_birth_ts\") \\\n",
    "        .withColumn(\"row_num\", expr(\"row_number() over (ORDER BY date_of_birth_ts)\")) \\\n",
    "        .filter(col(\"row_num\") == median_row) \\\n",
    "        .collect()[0][\"date_of_birth_ts\"]\n",
    "\n",
    "# Convert median timestamp back to date\n",
    "median_date = lit(expr(f\"CAST(FROM_UNIXTIME({median_ts}) AS DATE)\"))\n",
    "\n",
    "# Fill missing values with the global median date\n",
    "results = results.withColumn(\n",
    "    \"date_of_birth\",\n",
    "    when(col(\"date_of_birth\").isNull(), median_date).otherwise(col(\"date_of_birth\"))\n",
    ").drop(\"date_of_birth_ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc816871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the updated DataFrame\n",
    "results.filter(col(\"date_of_birth\").isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd06f6d1",
   "metadata": {},
   "source": [
    "## Convert DOB to AGE_AT_RACE_DAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1483325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, datediff, expr\n",
    "\n",
    "# Ensure both date_of_birth and race_date are in date format\n",
    "results = results.withColumn(\"date_of_birth\", col(\"date_of_birth\").cast(\"date\"))\n",
    "results = results.withColumn(\"race_date\", col(\"race_date\").cast(\"date\"))\n",
    "\n",
    "# Calculate age in days, then convert to years\n",
    "results = results.withColumn(\n",
    "    \"age_at_race_day\",\n",
    "    datediff(col(\"race_date\"), col(\"date_of_birth\")) / 365.25  # Convert days to years\n",
    ")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "results.select(\"date_of_birth\", \"race_date\", \"age_at_race_day\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff894cf",
   "metadata": {},
   "source": [
    "### Encoding Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45df1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to pay attention to case\n",
    "results = results.fillna({\"weather\": \"Clear\"})\n",
    "results.filter(col(\"weather\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfd1acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.select(\"weather\").distinct().count()\n",
    "# Count the occurrences of each distinct value in the \"weather\" column\n",
    "distinct_value_counts = results.groupBy(\"weather\").count()\n",
    "\n",
    "# Show the result\n",
    "distinct_value_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2b6ea6",
   "metadata": {},
   "source": [
    "### wps_pool: Imputing with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f85026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, mean, when\n",
    "\n",
    "# Calculate the mean of the 'wps_pool' column, excluding nulls\n",
    "mean_value = results.select(mean(col(\"wps_pool\")).alias(\"mean_wps_pool\")).collect()[0][\"mean_wps_pool\"]\n",
    "\n",
    "# Replace null values in 'wps_pool' with the calculated mean\n",
    "results = results.withColumn(\n",
    "    \"wps_pool\",\n",
    "    when(col(\"wps_pool\").isNull(), mean_value).otherwise(col(\"wps_pool\"))\n",
    ")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "results.filter(col(\"wps_pool\").isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62ba6ef",
   "metadata": {},
   "source": [
    "### equip: Conversion and Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0906d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.fillna({\"equip\": \"No_Equip\"})\n",
    "results.filter(col(\"equip\").isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9245f5fa",
   "metadata": {},
   "source": [
    "### trk_cond: Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6c6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"trk_cond\", \"trk_cond_desc\"] \n",
    "results.select(cols).distinct().count()\n",
    "\n",
    "# Count the occurrences of each distinct value in the \"weather\" column\n",
    "distinct_value_counts = results.groupBy(cols).count()\n",
    "\n",
    "# Show the result\n",
    "distinct_value_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b163ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"trk_cond\", \"trk_cond_desc\"]\n",
    "\n",
    "# Fill missing values with \"MISSING\" for the specified columns\n",
    "results = results.fillna({col: \"MISSING\" for col in cols})\n",
    "\n",
    "# Verify no null values remain\n",
    "missing_count = results.filter(\n",
    "    (col(\"trk_cond\").isNull()) | (col(\"trk_cond_desc\").isNull())\n",
    ").count()\n",
    "\n",
    "print(f\"Number of missing values: {missing_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240de23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.drop(\"trk_cond_desc\", \"saddle_cloth_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f433187",
   "metadata": {},
   "source": [
    "### Encoding Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4fe7cb",
   "metadata": {},
   "source": [
    "Horse Sex \tCode\tDescription\n",
    "\tC\tColt\n",
    "\tF\tFilly\n",
    "\tG\tGelding\n",
    "\tH\tHorse\n",
    "\tM\tMare\n",
    "\tR\tRidgling\n",
    "\tB\tSpayed Mare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3296a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"equip\", \"surface\", \"surface_type_description\", \"trk_cond\", \"trk_cond_desc\", \"weather\", \"dist_unit\", \"race_type\"]\n",
    "\n",
    "cols = [\"sex\"] \n",
    "\n",
    "distinct_values = results.select(*cols).distinct()\n",
    "distinct_values.show(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f3769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim, col\n",
    "results = results.withColumn(\"dist_unit\", trim(col(\"dist_unit\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.replace(\"\", \"MISSING\", subset=[\"med\", \"turf_mud_mark\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0138f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.filter(col(\"sex\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c54cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.drop(\"surface_type_description\", \"avgcls\")\n",
    "results = results.drop(\"trk_cond_desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.drop(\"horse_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda7b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = results.drop(\"saddle_cloth_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9bf93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"race_type\"] \n",
    "\n",
    "distinct_values = results.select(*cols).distinct()\n",
    "distinct_values.show(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b6ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e997d",
   "metadata": {},
   "source": [
    "# OHE and Prep for XGBoost Trainining in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85719ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34eb64",
   "metadata": {},
   "source": [
    "### Create Label from Official_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f0f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert official_fin to binary label for top-4 finish\n",
    "results = results.withColumn(\"label\", when(col(\"official_fin\") <= 4, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18653c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Took out course_cd to see if it would help identify other predictive features.\n",
    "# Categorical columns equip, surface, trk_cond, weather, dist_unit, race_type \n",
    "categorical_cols = [\"equip\", \"surface\", \"trk_cond\", \"weather\", \"dist_unit\", \"race_type\", \"sex\" , \"med\", \"stk_clm_md\", \"turf_mud_mark\"]\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_index\", handleInvalid=\"keep\") for c in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCols=[c+\"_index\"], outputCols=[c+\"_ohe\"]) for c in categorical_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a31d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for c in categorical_cols:\n",
    "#    distinct_values = results.select(c).distinct().collect()\n",
    "#    print(c, [row[c] for row in distinct_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea184d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = results.drop(\"date_of_birth\", \"official_fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda66c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f0de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "jock_indexer = StringIndexer(inputCol=\"jock_key\", outputCol=\"jock_key_index\", handleInvalid=\"keep\")\n",
    "train_indexer = StringIndexer(inputCol=\"train_key\", outputCol=\"train_key_index\", handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf32b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns\n",
    "# Removing \"race_number\" \n",
    "numeric_cols = [\"morn_odds\", \"age_at_race_day\",  \"purse\", \"weight\", \"start_position\", \n",
    "                \"claimprice\", \"power\", \"avgspd\", \"class_rating\", \"net_sentiment\",\"weight\", \n",
    "                \"distance\", \"power\", \"all_earnings\", \"cond_earnings\", \"avg_spd_sd\", \n",
    "                \"ave_cl_sd\", \"hi_spd_sd\", \"pstyerl\", \"all_starts\", \n",
    "               \"all_win\", \"all_place\", \"all_show\", \"all_fourth\", \"cond_starts\", \n",
    "                \"cond_win\", \"cond_place\", \"cond_show\", \"cond_fourth\"]\n",
    "# Add later to numeric cols after normalization: \"jock_key_index\", \"train_key_index\", "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e9d668",
   "metadata": {},
   "source": [
    "### Spark Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2bffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline to transform data\n",
    "preprocessing_stages = [jock_indexer, train_indexer] + indexers + encoders\n",
    "pipeline = Pipeline(stages=preprocessing_stages)\n",
    "model = pipeline.fit(results)\n",
    "df_transformed = model.transform(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff93c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_cols = [c+\"_ohe\" for c in categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=numeric_cols + ohe_cols, outputCol=\"raw_features\")\n",
    "df_assembled = assembler.transform(df_transformed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e19c47",
   "metadata": {},
   "source": [
    "### Normalize Numeric Values\n",
    "\n",
    "1.\tAssemble Numeric Features Into a Vector:\n",
    "First, use a VectorAssembler to combine all numeric columns into a single feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d05da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# numeric_cols defined as above\n",
    "numeric_assembler = VectorAssembler(\n",
    "    inputCols=numeric_cols,\n",
    "    outputCol=\"numeric_vector\"\n",
    ")\n",
    "\n",
    "df_with_numeric_vector = numeric_assembler.transform(df_assembled)  # df_assembled is your DataFrame with numeric_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146f3a0",
   "metadata": {},
   "source": [
    "\t2.\tApply StandardScaler:\n",
    "Using StandardScaler with withMean=True and withStd=True ensures zero mean and unit variance scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1074b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(\n",
    "    inputCol=\"numeric_vector\",\n",
    "    outputCol=\"numeric_scaled\",\n",
    "    withMean=True,  # center the data with mean\n",
    "    withStd=True    # scale to unit variance\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(df_with_numeric_vector)\n",
    "df_scaled = scaler_model.transform(df_with_numeric_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c4b7ae",
   "metadata": {},
   "source": [
    "\t3.\tReplace Original Numeric Features with Scaled Vector:\n",
    "Now df_scaled has a new column numeric_scaled that contains the scaled versions of your numeric features. You can drop the original numeric columns if you no longer need them, or keep them for reference.\n",
    "When building your final features vector for the model, include numeric_scaled vector instead of individual numeric columns. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7924754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have categorical OHE columns in ohe_cols\n",
    "# Combine numeric_scaled with ohe_cols\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=[\"numeric_scaled\"] + ohe_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "df_final = final_assembler.transform(df_scaled)\n",
    "\n",
    "# Now df_final contains 'features' that has normalized numeric features plus OHE columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b830bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_final = scaler_model.transform(df_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48de885",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f60caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    \"wps_pool\", \"distance\", \"course_cd\", \"equip\", \"surface\", \"trk_cond\", \"weather\", \"dist_unit\", \"race_type\", \"sex\", \"med\",\n",
    "    \"stk_clm_md\", \"turf_mud_mark\",\n",
    "    \"course_cd_index\", \"equip_index\", \"surface_index\", \"trk_cond_index\", \"weather_index\",\n",
    "    \"dist_unit_index\", \"race_type_index\", \"sex_index\", \"med_index\", \"stk_clm_md_index\", \"turf_mud_mark_index\",\n",
    "    \"jock_key\", \"train_key\",\n",
    "    \"date_of_birth\", \"raw_features\",\n",
    "    # Numeric columns now included in features:\n",
    "    \"age_at_race_day\", \"race_number\", \"purse\", \"weight\", \"start_position\", \"claimprice\", \"power\",\n",
    "    \"morn_odds\", \"avgspd\", \"jock_key_index\", \"train_key_index\", \"class_rating\", \"net_sentiment\",\n",
    "    \"avg_spd_sd\", \"ave_cl_sd\", \"hi_spd_sd\", \"pstyerl\", \"all_starts\",\n",
    "    \"all_win\", \"all_place\", \"all_show\", \"all_fourth\", \"all_earnings\", \n",
    "    \"cond_starts\", \"cond_win\", \"cond_place\", \n",
    "    \"cond_show\", \"cond_fourth\", \"cond_earnings\"]\n",
    "df_final = df_final.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481e10d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Save as Parquet\n",
    "processed_data = save_parquet(spark, df_final, \"processed_data\", parquet_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6403b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = spark.read.parquet(os.path.join(parquet_dir, \"processed_data.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4437f0a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "processed_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c29f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80e244",
   "metadata": {},
   "source": [
    "#  Modify Data After original run processed as above\n",
    "\n",
    "It can definitely be instructive to simplify your feature set and see what happens. If the model’s top features heavily revolve around track identity (course_cd_* OHE features), that suggests the model is relying significantly on track-specific patterns. Removing those features might help you understand how robust the model is when it can’t rely on track-based signals.\n",
    "\n",
    "Why Try Removing course_cd?\n",
    "\t1.\tReduce Overfitting to Specific Tracks:\n",
    "If course_cd is a high-impact feature, the model might be “memorizing” track-specific patterns that don’t generalize well. Removing it forces the model to rely more on intrinsic horse-level and race-level features (morning odds, net_sentiment, equip, surface, etc.), potentially giving you a model that’s more stable across different tracks.\n",
    "\t2.\tDiscovering New Important Features:\n",
    "With course_cd features removed, the model can no longer lean on those easy signals. You’ll see which other features emerge as top contributors. For example, maybe morn_odds, net_sentiment, or equip features increase in relative importance.\n",
    "\t3.\tImprove Interpretability:\n",
    "Without track identity dominating the importance chart, it might be clearer how much impact your newly added features (like ALL_RACES stats or other cumulative metrics) have on predictions.\n",
    "\n",
    "Approach to Test This:\n",
    "\t•\tRemove course_cd and all derived OHE columns from the feature set. This means dropping course_cd, course_cd_index, and all course_cd_ohe_* columns from your vector assembler.\n",
    "\t•\tRerun the model training and compare:\n",
    "\t•\tAUC and Accuracy before and after removing course_cd.\n",
    "\t•\tFeature importance rankings in the new run.\n",
    "\n",
    "If performance drastically drops, it means track-based signals were genuinely valuable. If performance remains stable or only slightly worse—but the model’s top features become more horse-performance oriented—then you’ve gained a more track-agnostic model, which might be beneficial in certain scenarios.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Yes, a better approach (or at least a valuable experiment) would be to re-run the model without the course_cd features and see what happens. This helps you understand the model’s true dependencies and might lead to a more generalizable and insightful set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, test_df = processed_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# cutoff_date = \"2024-01-01\"\n",
    "# train_df = processed_data.filter(col(\"race_date\") < cutoff_date)\n",
    "# test_df = processed_data.filter(col(\"race_date\") >= cutoff_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8518687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.spark import SparkXGBClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import to_date, lit, add_months\n",
    "\n",
    "# Time-based cutoff dates\n",
    "dates = [\"2023-06-30\", \"2023-09-30\", \"2023-12-31\", \"2024-03-31\"]  # cutoff dates\n",
    "\n",
    "fold_metrics = []\n",
    "\n",
    "# Positive class is label=1\n",
    "total = processed_data.count()\n",
    "positives = processed_data.filter(col(\"label\") == 1).count()\n",
    "negatives = total - positives\n",
    "\n",
    "# Compute the ratio\n",
    "ratio = negatives / positives if positives > 0 else 1.0\n",
    "\n",
    "print(\"Total examples:\", total)\n",
    "print(\"Positives (label=1):\", positives)\n",
    "print(\"Negatives (label=0):\", negatives)\n",
    "print(\"scale_pos_weight ratio:\", ratio)\n",
    "\n",
    "# Fixed parameters (from your best params scenario)\n",
    "xgb_params = {\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.01,\n",
    "    \"gamma\": 0,\n",
    "    \"subsample\": 0.6,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"min_child_weight\": 5,\n",
    "    \"reg_lambda\": 1,\n",
    "    \"reg_alpha\": 1,\n",
    "    \"num_boost_round\": 500, # Adjust if needed\n",
    "    \"verbosity\": 2,\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"scale_pos_weight\": ratio\n",
    "}\n",
    "\n",
    "for cutoff_str in dates:\n",
    "    cutoff_date_expr = to_date(lit(cutoff_str), \"yyyy-MM-dd\")\n",
    "\n",
    "    # Training data: race_date <= cutoff\n",
    "    train_df = processed_data.filter(processed_data.race_date <= cutoff_str)\n",
    "\n",
    "    # Validation: 3 months after cutoff\n",
    "    validation_window_end_expr = add_months(cutoff_date_expr, 3)\n",
    "    validation_df = processed_data.filter(\n",
    "        (processed_data.race_date > cutoff_str) & \n",
    "        (processed_data.race_date <= validation_window_end_expr)\n",
    "    )\n",
    "\n",
    "    # If no validation data, assign a neutral metric and continue\n",
    "    if validation_df.count() == 0:\n",
    "        fold_metrics.append(0.5)\n",
    "        continue\n",
    "\n",
    "    # Create the classifier with the chosen parameters\n",
    "    xgb_model = SparkXGBClassifier(\n",
    "        features_col=\"features\",\n",
    "        label_col=\"label\",\n",
    "        num_workers=16,\n",
    "        prediction_col=\"prediction\",\n",
    "        probability_col=\"probability\",\n",
    "        raw_prediction_col=\"rawPrediction\",\n",
    "        max_depth=xgb_params[\"max_depth\"],\n",
    "        eta=xgb_params[\"eta\"],\n",
    "        gamma=xgb_params[\"gamma\"],\n",
    "        subsample=xgb_params[\"subsample\"],\n",
    "        colsample_bytree=xgb_params[\"colsample_bytree\"],\n",
    "        min_child_weight=xgb_params[\"min_child_weight\"],\n",
    "        reg_lambda=xgb_params[\"reg_lambda\"],\n",
    "        reg_alpha=xgb_params[\"reg_alpha\"],\n",
    "        num_boost_round=xgb_params[\"num_boost_round\"],\n",
    "        verbosity=xgb_params[\"verbosity\"],\n",
    "        eval_metric=xgb_params[\"eval_metric\"],\n",
    "        scale_pos_weight=xgb_params[\"scale_pos_weight\"]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model = xgb.fit(train_df)\n",
    "\n",
    "    # Predict on validation set\n",
    "    predictions = model.transform(validation_df)\n",
    "\n",
    "    # Evaluate AUC\n",
    "    evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol=\"label\",\n",
    "        rawPredictionCol=\"rawPrediction\",\n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    fold_metrics.append(auc)\n",
    "\n",
    "# Compute average AUC across all folds\n",
    "avg_auc = sum(fold_metrics) / len(fold_metrics)\n",
    "print(\"Average AUC across all folds:\", avg_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d091b75b",
   "metadata": {},
   "source": [
    "Best Params: {'max_depth': 6, 'eta': 0.01, 'gamma': 0, 'subsample': 0.6, 'colsample_bytree': 0.7, 'min_child_weight': 1, 'reg_lambda': 0, 'reg_alpha': 0}\n",
    "Best AUC: 0.7327572203895388\n",
    "\n",
    "\n",
    "Best Params: {'max_depth': 6, 'eta': 0.01, 'gamma': 0, 'subsample': 0.6, 'colsample_bytree': 0.7, 'min_child_weight': 1, 'reg_lambda': 0, 'reg_alpha': 0}\n",
    "Best AUC: 0.7327572203895388\n",
    "\n",
    "Best Params: {'max_depth': 6, 'eta': 0.01, 'gamma': 0, 'subsample': 0.6, 'colsample_bytree': 0.7, 'min_child_weight': 1, 'reg_lambda': 0, 'reg_alpha': 0}\n",
    "Best AUC: 0.7327584604076798\n",
    "\n",
    "Average AUC across all folds: 0.7327587069455473\n",
    "\n",
    "Average AUC across all folds: 0.7339438971137404\n",
    "\n",
    "Average AUC across all folds: 0.7339511832794069"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abebb2dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# xgb_model = xgb.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.printSchema()\n",
    "# predictions.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de556f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2d6b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca2b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, mean\n",
    "\n",
    "# Calculate the proportion of horses finishing in top-4\n",
    "positive_proportion = validation_df.select(mean(col(\"label\")).alias(\"pos_rate\")).collect()[0][\"pos_rate\"]\n",
    "print(\"Proportion of top-4 finishers in test set:\", positive_proportion)\n",
    "\n",
    "# Naive baseline metrics:\n",
    "# If we always predict 'not top-4' (label=0), the accuracy = (1 - positive_proportion).\n",
    "# If we always predict 'top-4' (label=1), the accuracy = positive_proportion.\n",
    "# Choose the majority class baseline:\n",
    "majority_class_accuracy = max(positive_proportion, 1 - positive_proportion)\n",
    "print(\"Naive majority class baseline accuracy:\", majority_class_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a790dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# predictions = xgb_model.transform(validation_df)\n",
    "\n",
    "# AUC (Binary)\n",
    "binary_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "auc = binary_evaluator.evaluate(predictions)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Precision\n",
    "precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"precisionByLabel\")\n",
    "precision = precision_evaluator.evaluate(predictions)\n",
    "print(\"Precision (Top-4 class):\", precision)\n",
    "\n",
    "# Recall\n",
    "recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"recallByLabel\")\n",
    "recall = recall_evaluator.evaluate(predictions)\n",
    "print(\"Recall (Top-4 class):\", recall)\n",
    "\n",
    "# F1 Score\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = f1_evaluator.evaluate(predictions)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd41f45a",
   "metadata": {},
   "source": [
    "# Save model to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b14b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/serialized_trained_models/eqb_only/xgb_model_depth6_eta0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21dfc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = SparkXGBClassifierModel.load(\"models/xgb_model_depth6_eta0.01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373237cf",
   "metadata": {},
   "source": [
    "## Feature Importance with get_booster().get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dcb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_metadata = df_assembled.schema[\"raw_features\"].metadata\n",
    "attrs = feature_metadata[\"ml_attr\"][\"attrs\"]\n",
    "\n",
    "def get_feature_names(attrs):\n",
    "    names = []\n",
    "    for attr_type in [\"numeric\", \"binary\", \"nominal\"]:\n",
    "        if attr_type in attrs:\n",
    "            for a in attrs[attr_type]:\n",
    "                names.append((a[\"idx\"], a[\"name\"]))\n",
    "    return [name for idx, name in sorted(names, key=lambda x: x[0])]\n",
    "\n",
    "feature_names = get_feature_names(attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b025a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = xgb_model.fit(train_df)\n",
    "\n",
    "# Access the underlying sklearn model and then the booster\n",
    "sklearn_model = model._xgb_sklearn_model\n",
    "booster = sklearn_model.get_booster()\n",
    "\n",
    "# Now get feature importance\n",
    "feature_importances = booster.get_score(importance_type='gain')\n",
    "\n",
    "mapped_importances = []\n",
    "for f, importance in feature_importances.items():\n",
    "    index = int(f[1:])\n",
    "    if index < len(feature_names):\n",
    "        feature_name = feature_names[index]\n",
    "    else:\n",
    "        feature_name = f\"Unknown_{index}\"\n",
    "    mapped_importances.append((feature_name, importance))\n",
    "\n",
    "mapped_importances.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Mapped Feature Importances:\")\n",
    "for name, imp in mapped_importances:\n",
    "    print(name, imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666bde16",
   "metadata": {},
   "source": [
    "## Error Analysis: Incorrect Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8604d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Predicted top-4 (prediction=1), actually not top-4 (label=0)\n",
    "false_positives = predictions.filter((col(\"prediction\") == 1) & (col(\"label\") == 0))\n",
    "false_positives.show(10)\n",
    "\n",
    "# Predicted not top-4 (prediction=0), actually top-4 (label=1)\n",
    "false_negatives = predictions.filter((col(\"prediction\") == 0) & (col(\"label\") == 1))\n",
    "false_negatives.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4fdf7d",
   "metadata": {},
   "source": [
    "## Inspect these subsets to see if there’s a pattern. For example, check if they occur at certain tracks, or with certain distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives.groupBy(\"course_cd_ohe\").count().orderBy(col(\"count\").desc()).show(10,False)\n",
    "false_negatives.groupBy(\"course_cd_ohe\").count().orderBy(col(\"count\").desc()).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c4b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
