{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d31374ea-f808-4394-a60c-c3f0a08e2627",
   "metadata": {},
   "source": [
    "# Spark Anaysis of Total Performance Data: GPS and Sectional Data\n",
    "\n",
    "Analyzing the TPD GPS data alongside Equibase (EQB) data can provide significant predictive insights into horse performance. By focusing on the granular details of a horse’s movement during a race, you can derive valuable metrics that complement traditional EQB ratings and help identify under- or over-rated horses. \n",
    "\n",
    "## Roadmap for maximizing the predictive capabilities of the TPD GPS data:\n",
    "\n",
    "### Key Ideas and Strategies\n",
    "\n",
    "1. Derive Advanced Pace Metrics\n",
    "\n",
    "Understanding how a horse’s speed changes over the course of a race can reveal its racing style and potential strengths or weaknesses:\n",
    "\n",
    "\t•\tEarly Pace: Average speed and acceleration during the first segment (e.g., first 20% of the race).\n",
    "\t•\tMid-Race Pace: Average speed and deceleration during the middle segments.\n",
    "\t•\tLate Pace: Average speed and deceleration in the final segment.\n",
    "\t•\tSustained Speed: Identify segments where the horse maintains a steady speed.\n",
    "\t•\tPeak Speed Timing: The point in the race where the horse reaches its peak speed.\n",
    "\n",
    "2. Fatigue Factor\n",
    "\n",
    "Calculate how much the horse slows down as the race progresses:\n",
    "\n",
    "\t•\tUse metrics like:\n",
    "\t•\tPercentage drop from peak speed to finish speed.\n",
    "\t•\tMaximum acceleration vs. deceleration ratios.\n",
    "\t•\tChange in stride frequency as the race progresses.\n",
    "\n",
    "3. Sectional Efficiency\n",
    "\n",
    "Quantify how efficiently the horse runs its sections:\n",
    "\n",
    "    •\tCompare actual times vs. expected times for each section based on the route characteristics.\n",
    "\t•\tEfficiency Ratio:\n",
    "    •\tA high ratio might indicate a horse ran extra distance due to poor cornering or positioning.\n",
    "\n",
    "4. Overlay TPD with EQB Ratings\n",
    "\n",
    "Use EQB’s traditional metrics (e.g., speed ratings, form) to cross-reference with TPD data:\n",
    "\n",
    "\t•\tIdentify horses that consistently outperform EQB predictions.\n",
    "\t•\tInvestigate horses with high EQB ratings but poor TPD-based performance (e.g., poor fatigue factors or inefficient sectional running).\n",
    "\n",
    "5. Route Characteristics\n",
    "\n",
    "If the routes table contains track-specific details (e.g., turn sharpness, surface type, gradient):\n",
    "\n",
    "\t•\tIncorporate these into the analysis.\n",
    "\t•\tEvaluate how specific horses handle different track conditions (e.g., wide turns, long stretches).\n",
    "\t•\tIdentify patterns like “performs better on flatter tracks” or “struggles on uphill finishes.”\n",
    "\n",
    "6. Horse vs. Peer Comparisons\n",
    "\n",
    "Evaluate how each horse performs relative to its competition in the same race:\n",
    "\n",
    "\t•\tCompare sectional times and speeds with other horses in the race.\n",
    "\t•\tRank horses based on performance within each race segment.\n",
    "\n",
    "7. Acceleration Profiles\n",
    "\n",
    "\t•\tPlot acceleration over time to identify patterns (e.g., burst speed vs. steady acceleration).\n",
    "    •\tHighlight horses with exceptional closing speed (valuable in longer races) or fast starts (important in short sprints).\n",
    "\n",
    "9. Cluster Analysis of Racing Styles\n",
    "\n",
    "Use clustering techniques to group horses by similar racing profiles:\n",
    "\n",
    "\t•\tInputs: Early pace, mid-pace, late pace, fatigue factor, sectional efficiency.\n",
    "\t•\tOutput: Clusters representing different racing styles (e.g., “early speed burners,” “closers,” “steady sustainers”).\n",
    "\n",
    "9. Historical Analysis\n",
    "\n",
    "Identify trends over a horse’s career:\n",
    "\n",
    "\t•\tDoes the horse improve or decline over time?\n",
    "\t•\tAre there patterns in performance tied to specific jockeys, trainers, or race conditions?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b2d7b-00ae-4a2c-9081-e27a7e1e1573",
   "metadata": {},
   "source": [
    "# Using Spark for Efficient Processing\n",
    "\n",
    "Steps to Implement\n",
    "\n",
    "\t1.\tLoad the Data\n",
    "\t•\tLoad gpspoint, gps_aggregated_results, and routes into Spark DataFrames.\n",
    "\t2.\tSegment the Race\n",
    "\t•\tDivide each race into sections (e.g., by distance markers or time intervals).\n",
    "\t•\tUse PARTITION BY in Spark to process horses within each race separately.\n",
    "\t3.\tDerive Metrics\n",
    "\t•\tSpeed Metrics: Use window functions to calculate average, min, max speeds.\n",
    "\t•\tAcceleration/Deceleration: Compute using differences in speed and timestamps.\n",
    "\t•\tEfficiency: Calculate distance ran vs. track distance.\n",
    "\t4.\tIntegrate with EQB Data\n",
    "\t•\tJoin with EQB tables on course_cd, race_date, and race_number for comparison.\n",
    "\t5.\tSave Aggregated Data\n",
    "\t•\tSave results into gps_aggregated_results and tpd_features.\n",
    "\n",
    "Example Spark Code\n",
    "\n",
    "Here’s a high-level implementation for deriving pace metrics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "139a5bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 21:24:51,295 - INFO - Environment setup initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark session...\n",
      "Spark session created successfully with Sedona and GeoTools integrated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pprint\n",
    "from pyspark.sql.functions import (\n",
    "    col, unix_timestamp, when, first, last, lag, udf, sum as spark_sum,\n",
    "    mean as spark_mean, min as spark_min, max as spark_max, round as spark_round\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import timedelta\n",
    "\n",
    "# Importing utility functions from your project structure\n",
    "from src.data_preprocessing.data_prep2.data_healthcheck import (\n",
    "    time_series_data_healthcheck, dataframe_summary\n",
    ")\n",
    "from src.data_preprocessing.data_prep1.data_loader import (\n",
    "    load_data_from_postgresql, reload_parquet_files, \n",
    "    load_named_parquet_files, merge_results_sectionals\n",
    ")\n",
    "from src.data_preprocessing.data_prep1.sql_queries import sql_queries\n",
    "from src.data_preprocessing.gps_aggregated.data_features_enhancements import (\n",
    "    data_enhancements\n",
    ")\n",
    "from src.data_preprocessing.gps_aggregated.merge_gps_sectionals_agg import (\n",
    "    merge_gps_sectionals\n",
    ")\n",
    "from src.data_preprocessing.data_prep1.data_utils import (\n",
    "    save_parquet, gather_statistics, initialize_environment,\n",
    "    load_config, initialize_logging, initialize_spark, drop_duplicates_with_tolerance,\n",
    "    identify_and_impute_outliers, identify_and_remove_outliers, process_merged_results_sectionals,\n",
    "    identify_missing_and_outliers\n",
    ")\n",
    "    \n",
    "spark = None\n",
    "try:\n",
    "    spark, jdbc_url, jdbc_properties, queries, parquet_dir, log_file = initialize_environment()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during initialization: {e}\")\n",
    "    logging.error(f\"An error occurred during initialization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aedbce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpspoint = spark.read.parquet(os.path.join(parquet_dir, \"gpspoint.parquet\"))\n",
    "sectionals = spark.read.parquet(os.path.join(parquet_dir, \"merge_results_sectionals.parquet\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2f88cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1108928"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sectionals.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5330e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20358278"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpspoint.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "83cad127",
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_data = spark.read.parquet(os.path.join(parquet_dir, \"enriched_data.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "95c74569",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "        \"course_cd\", \"equip\", \"surface\", \"trk_cond\", \"weather\", \n",
    "        \"race_type\", \"sex\", \"med\", \"stk_clm_md\", \"turf_mud_mark\"\n",
    "    ]\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ca72ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c+\"_index\") for c in categorical_cols\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2a94c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    \"course_cd\", \"equip\", \"surface\", \"trk_cond\", \"weather\", \n",
    "    \"race_type\", \"sex\", \"med\", \"stk_clm_md\", \"turf_mud_mark\"\n",
    "]\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c+\"_index\", handleInvalid=\"keep\") \n",
    "    for c in categorical_cols\n",
    "]\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCols=[c+\"_index\"], outputCols=[c+\"_ohe\"]) \n",
    "    for c in categorical_cols\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4bd3256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_cols = [c+\"_ohe\" for c in categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d5bf629d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['course_cd_ohe', 'equip_ohe', 'surface_ohe', 'trk_cond_ohe', 'weather_ohe', 'race_type_ohe', 'sex_ohe', 'med_ohe', 'stk_clm_md_ohe', 'turf_mud_mark_ohe']\n"
     ]
    }
   ],
   "source": [
    "print(ohe_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "74ec2a2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|distance_meters|\n",
      "+---------------+\n",
      "|1207.008       |\n",
      "|1207.008       |\n",
      "|1207.008       |\n",
      "|1207.008       |\n",
      "|1207.008       |\n",
      "|1207.008       |\n",
      "|1207.008       |\n",
      "|1207.008       |\n",
      "|1207.008       |\n",
      "|1207.008       |\n",
      "+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enriched_data.select(\"distance_meters\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "537a5142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique horses: 32104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 104:=============>                                       (26 + 74) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Just do a distinct count on the 'horse_id' column\n",
    "unique_horse_count = enriched_data.select(\"horse_id\").distinct().count()\n",
    "\n",
    "print(f\"Number of unique horses: {unique_horse_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a2b2d5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|mean_entries_per_horse|\n",
      "+----------------------+\n",
      "|     32.35369424370795|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, col\n",
    "\n",
    "# 1) Count how many rows each horse has:\n",
    "entries_per_horse_df = enriched_data.groupBy(\"horse_id\").count()\n",
    "\n",
    "# 2) Calculate the average of those counts:\n",
    "mean_entries_df = entries_per_horse_df.agg(avg(col(\"count\")).alias(\"mean_entries_per_horse\"))\n",
    "\n",
    "mean_entries_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e28fe8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|median_entries_per_horse|\n",
      "+------------------------+\n",
      "|                    24.0|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import median, col, count\n",
    "\n",
    "# 1) Count how many rows each horse has:\n",
    "entries_per_horse_df = enriched_data.groupBy(\"horse_id\").count()\n",
    "\n",
    "# 2) Calculate the average of those counts:\n",
    "median_entries_df = entries_per_horse_df.agg(median(col(\"count\")).alias(\"median_entries_per_horse\"))\n",
    "\n",
    "median_entries_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eaa9c2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "primary_keys = [\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\", \"sectionals_gate_name\"]\n",
    "\n",
    "duplicates = enriched_data.groupBy(*primary_keys) \\\n",
    "                   .agg(count(\"*\").alias(\"cnt\")) \\\n",
    "                   .filter(col(\"cnt\") > 1)\n",
    "dup_count = duplicates.count()\n",
    "\n",
    "print(dup_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "16d8d3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+-----------------+----------------------+----------------------+---------------------+-------------------+-------------------+--------------+\n",
      "|course_cd|race_date |race_number|accel_missing_sum|stridefreq_missing_sum|prev_speed_missing_sum|dist_back_missing_sum|strides_missing_sum|weather_missing_sum|race_row_count|\n",
      "+---------+----------+-----------+-----------------+----------------------+----------------------+---------------------+-------------------+-------------------+--------------+\n",
      "|TWO      |2022-11-06|7          |14               |14                    |14                    |0                    |0                  |0                  |252           |\n",
      "|TWO      |2024-10-05|6          |13               |13                    |13                    |0                    |0                  |0                  |286           |\n",
      "|TWO      |2024-10-05|4          |13               |13                    |13                    |0                    |0                  |0                  |208           |\n",
      "|TOP      |2023-04-07|10         |12               |12                    |12                    |0                    |0                  |0                  |216           |\n",
      "|AQU      |2023-04-14|8          |12               |12                    |12                    |0                    |0                  |0                  |204           |\n",
      "|LRL      |2022-04-16|8          |12               |12                    |12                    |0                    |0                  |0                  |192           |\n",
      "|TTP      |2023-03-17|6          |12               |12                    |12                    |0                    |0                  |0                  |192           |\n",
      "|AQU      |2023-04-22|6          |12               |12                    |12                    |0                    |0                  |0                  |156           |\n",
      "|TCD      |2024-06-15|10         |12               |12                    |12                    |0                    |0                  |0                  |204           |\n",
      "|TTP      |2023-02-02|5          |12               |12                    |12                    |0                    |0                  |0                  |192           |\n",
      "|TWO      |2023-10-14|11         |12               |12                    |12                    |0                    |0                  |0                  |156           |\n",
      "|TWO      |2023-08-13|4          |12               |12                    |12                    |0                    |0                  |0                  |204           |\n",
      "|TOP      |2024-02-03|4          |12               |12                    |12                    |0                    |0                  |0                  |144           |\n",
      "|TWO      |2022-10-09|7          |12               |12                    |12                    |0                    |0                  |0                  |204           |\n",
      "|TGP      |2024-01-11|1          |12               |12                    |12                    |0                    |0                  |0                  |132           |\n",
      "|TGP      |2023-01-05|9          |12               |12                    |12                    |0                    |0                  |0                  |180           |\n",
      "|ELP      |2023-08-18|9          |12               |12                    |12                    |0                    |0                  |0                  |204           |\n",
      "|IND      |2023-11-10|3          |12               |12                    |12                    |0                    |0                  |0                  |192           |\n",
      "|LRL      |2023-10-15|6          |12               |12                    |12                    |0                    |0                  |0                  |144           |\n",
      "|TTP      |2024-02-22|5          |12               |12                    |12                    |0                    |0                  |0                  |192           |\n",
      "|TGP      |2022-12-09|8          |12               |12                    |12                    |0                    |0                  |0                  |156           |\n",
      "|TGP      |2023-12-17|10         |12               |12                    |12                    |0                    |0                  |0                  |204           |\n",
      "|DMR      |2023-08-27|10         |12               |14                    |12                    |0                    |0                  |0                  |191           |\n",
      "|TOP      |2023-05-05|11         |12               |12                    |12                    |0                    |0                  |0                  |216           |\n",
      "|TOP      |2024-02-03|6          |12               |12                    |12                    |0                    |0                  |0                  |204           |\n",
      "|TCD      |2024-06-15|3          |12               |12                    |12                    |0                    |2                  |0                  |132           |\n",
      "|TWO      |2024-10-05|8          |12               |12                    |12                    |0                    |0                  |0                  |204           |\n",
      "|TGP      |2024-01-11|5          |12               |12                    |12                    |0                    |0                  |0                  |120           |\n",
      "|LRL      |2023-01-06|6          |12               |12                    |12                    |0                    |0                  |0                  |192           |\n",
      "|TGP      |2023-12-07|1          |12               |12                    |12                    |0                    |0                  |0                  |204           |\n",
      "|TWO      |2023-11-26|10         |12               |12                    |12                    |0                    |0                  |0                  |144           |\n",
      "|TAM      |2024-02-11|9          |12               |12                    |12                    |0                    |0                  |0                  |204           |\n",
      "|TOP      |2023-02-20|9          |12               |12                    |12                    |0                    |0                  |0                  |132           |\n",
      "|TOP      |2022-01-23|5          |12               |12                    |12                    |0                    |0                  |0                  |192           |\n",
      "|TTP      |2023-03-17|5          |12               |12                    |12                    |0                    |0                  |0                  |192           |\n",
      "|IND      |2022-09-14|2          |12               |12                    |12                    |0                    |0                  |0                  |144           |\n",
      "|TAM      |2022-12-23|9          |12               |12                    |12                    |0                    |0                  |0                  |168           |\n",
      "|TWO      |2024-09-29|6          |12               |1                     |12                    |0                    |0                  |0                  |132           |\n",
      "|AQU      |2023-11-03|10         |12               |5                     |12                    |0                    |0                  |0                  |204           |\n",
      "|TGP      |2023-03-02|9          |12               |12                    |12                    |0                    |0                  |0                  |192           |\n",
      "|LRL      |2023-11-04|1          |12               |12                    |12                    |0                    |0                  |0                  |204           |\n",
      "|TWO      |2024-11-30|5          |12               |12                    |12                    |0                    |0                  |0                  |168           |\n",
      "|TCD      |2023-09-15|4          |12               |13                    |12                    |0                    |0                  |0                  |132           |\n",
      "|TGP      |2022-07-24|6          |12               |12                    |12                    |0                    |0                  |0                  |156           |\n",
      "|TWO      |2022-07-21|8          |12               |12                    |12                    |0                    |0                  |0                  |120           |\n",
      "|TGG      |2022-12-11|12         |12               |12                    |12                    |0                    |0                  |0                  |144           |\n",
      "|TGP      |2023-03-11|9          |12               |12                    |12                    |0                    |0                  |0                  |213           |\n",
      "|TTP      |2023-02-02|8          |12               |12                    |12                    |0                    |0                  |0                  |156           |\n",
      "|DMR      |2022-07-29|3          |12               |12                    |12                    |0                    |0                  |0                  |132           |\n",
      "|TTP      |2023-03-17|8          |12               |12                    |12                    |0                    |0                  |0                  |156           |\n",
      "+---------+----------+-----------+-----------------+----------------------+----------------------+---------------------+-------------------+-------------------+--------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+---------------+\n",
      "|horse_id|accel_missing_sum|horse_row_count|\n",
      "+--------+-----------------+---------------+\n",
      "|30899   |19               |243            |\n",
      "|44568   |18               |234            |\n",
      "|11793   |17               |256            |\n",
      "|45136   |17               |226            |\n",
      "|15091   |16               |221            |\n",
      "|1258    |16               |217            |\n",
      "|2329    |16               |203            |\n",
      "|27865   |16               |218            |\n",
      "|15485   |16               |224            |\n",
      "|85831   |15               |207            |\n",
      "|12045   |15               |201            |\n",
      "|54019   |15               |193            |\n",
      "|24952   |15               |206            |\n",
      "|39507   |15               |231            |\n",
      "|24980   |15               |192            |\n",
      "|43730   |15               |208            |\n",
      "|20094   |15               |192            |\n",
      "|16016   |14               |219            |\n",
      "|43295   |14               |203            |\n",
      "|19642   |14               |211            |\n",
      "|12973   |14               |199            |\n",
      "|7083    |14               |177            |\n",
      "|37043   |14               |198            |\n",
      "|17574   |14               |207            |\n",
      "|69291   |14               |214            |\n",
      "|409931  |14               |192            |\n",
      "|31686   |14               |191            |\n",
      "|9885    |14               |187            |\n",
      "|68126   |14               |194            |\n",
      "|125971  |14               |200            |\n",
      "|42798   |14               |181            |\n",
      "|45373   |14               |211            |\n",
      "|1532    |14               |177            |\n",
      "|15151   |14               |213            |\n",
      "|24969   |14               |185            |\n",
      "|17488   |14               |189            |\n",
      "|24936   |13               |171            |\n",
      "|126041  |13               |184            |\n",
      "|1562    |13               |176            |\n",
      "|29067   |13               |181            |\n",
      "|1278    |13               |210            |\n",
      "|1310    |13               |217            |\n",
      "|15481   |13               |179            |\n",
      "|6650    |13               |177            |\n",
      "|109266  |13               |181            |\n",
      "|45084   |13               |176            |\n",
      "|62407   |13               |200            |\n",
      "|70357   |13               |184            |\n",
      "|15278   |13               |178            |\n",
      "|15116   |13               |178            |\n",
      "+--------+-----------------+---------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 149:=========================================>           (78 + 22) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col, sum as F_sum, count\n",
    "\n",
    "# 1. Create indicator columns for missingness\n",
    "df_missing_indicators = enriched_data.select(\n",
    "    \"*\",  # keep all original cols\n",
    "    (col(\"acceleration_m_s2\").isNull().cast(\"int\")).alias(\"accel_missing\"),\n",
    "    (col(\"gps_section_avg_stride_freq\").isNull().cast(\"int\")).alias(\"stridefreq_missing\"),\n",
    "    (col(\"prev_speed\").isNull().cast(\"int\")).alias(\"prev_speed_missing\"),\n",
    "    (col(\"sectionals_distance_back\").isNull().cast(\"int\")).alias(\"dist_back_missing\"),\n",
    "    (col(\"sectionals_number_of_strides\").isNull().cast(\"int\")).alias(\"strides_missing\"),\n",
    "    (col(\"weather\").isNull().cast(\"int\")).alias(\"weather_missing\")\n",
    ")\n",
    "\n",
    "# 2. Group by race or by horse (depending on your analysis needs)\n",
    "#    Example: Group by a \"race-level\" ID (course_cd, race_date, race_number).\n",
    "#    Then sum each missing indicator and also count total rows:\n",
    "\n",
    "missing_by_race = (\n",
    "    df_missing_indicators\n",
    "    .groupBy(\"course_cd\", \"race_date\", \"race_number\")\n",
    "    .agg(\n",
    "        F_sum(\"accel_missing\").alias(\"accel_missing_sum\"),\n",
    "        F_sum(\"stridefreq_missing\").alias(\"stridefreq_missing_sum\"),\n",
    "        F_sum(\"prev_speed_missing\").alias(\"prev_speed_missing_sum\"),\n",
    "        F_sum(\"dist_back_missing\").alias(\"dist_back_missing_sum\"),\n",
    "        F_sum(\"strides_missing\").alias(\"strides_missing_sum\"),\n",
    "        F_sum(\"weather_missing\").alias(\"weather_missing_sum\"),\n",
    "        count(\"*\").alias(\"race_row_count\")\n",
    "    )\n",
    "    .orderBy(\"accel_missing_sum\", ascending=False)\n",
    ")\n",
    "\n",
    "missing_by_race.show(50, truncate=False)\n",
    "\n",
    "# Similarly, you could group by (horse_id) or (course_cd, race_date, race_number, saddle_cloth_number).\n",
    "# For instance, if you suspect some *horses* always lack data:\n",
    "\n",
    "missing_by_horse = (\n",
    "    df_missing_indicators\n",
    "    .groupBy(\"horse_id\")\n",
    "    .agg(\n",
    "        F_sum(\"accel_missing\").alias(\"accel_missing_sum\"),\n",
    "        # ...\n",
    "        count(\"*\").alias(\"horse_row_count\")\n",
    "    )\n",
    "    .orderBy(\"accel_missing_sum\", ascending=False)\n",
    ")\n",
    "missing_by_horse.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4ff314aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, sum as F_sum, count, lit\n",
    "enriched_data = enriched_data.withColumn(\n",
    "    \"label\",\n",
    "    when(col(\"official_fin\") == 1, lit(0))                          # Win\n",
    "    .when(col(\"official_fin\") == 2, lit(1))                         # Place\n",
    "    .when(col(\"official_fin\") == 3, lit(2))                         # Show\n",
    "    .when(col(\"official_fin\") == 4, lit(3))                         # Fourth\n",
    "    .otherwise(lit(4))                                              # Outside top-4\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a0fe01e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    1|137802|\n",
      "|    3|137777|\n",
      "|    4|486781|\n",
      "|    2|137838|\n",
      "|    0|138485|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enriched_data.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af0daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc0d15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c28e388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b44d24ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct combos in sectionals: 79098\n",
      "Distinct combos in gpspoint: 139500\n"
     ]
    }
   ],
   "source": [
    "total_sectionals_horses = sectionals.select(\"course_cd\", \"race_date\", \n",
    "                                            \"race_number\", \"saddle_cloth_number\").distinct().count()\n",
    "\n",
    "total_gps_horses = gpspoint.select(\"course_cd\", \"race_date\", \n",
    "                                   \"race_number\", \"saddle_cloth_number\").distinct().count()\n",
    "\n",
    "print(\"Distinct combos in sectionals:\", total_sectionals_horses)\n",
    "print(\"Distinct combos in gpspoint:\", total_gps_horses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045c4877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, min as spark_min, sum as spark_sum, udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import timedelta\n",
    "\n",
    "# UDF to add seconds (including fractional seconds)\n",
    "def add_seconds(ts, seconds):\n",
    "    if ts is None or seconds is None:\n",
    "        return None\n",
    "    return ts + timedelta(seconds=seconds)\n",
    "\n",
    "add_seconds_udf = udf(add_seconds, TimestampType())\n",
    "\n",
    "# Define your race identification columns\n",
    "race_id_cols = [\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ea3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# 1) Compute earliest GPS time per race and horse\n",
    "######################################\n",
    "gps_earliest_df = gpspoint.groupBy(*race_id_cols).agg(\n",
    "    spark_min(\"time_stamp\").alias(\"earliest_time_stamp_gps\")\n",
    ")\n",
    "\n",
    "print(\"Earliest GPS time per race and horse computed.\")\n",
    "gps_earliest_df.printSchema()\n",
    "gps_earliest_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################################\n",
    "# 2) Sort the sectional data by gate_index\n",
    "######################################\n",
    "# Just ensure sectionals are ordered by gate_index. \n",
    "# We'll do cumulative sums after we join with earliest_time.\n",
    "sectionals_sorted = sectionals.orderBy(*race_id_cols, \"gate_index\")\n",
    "\n",
    "print(\"Sectionals sorted by gate_index.\")\n",
    "sectionals_sorted.printSchema()\n",
    "sectionals_sorted.select(*race_id_cols, \"gate_index\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c82c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################################\n",
    "# 3) Join the earliest GPS time with sectionals\n",
    "######################################\n",
    "sectionals_with_earliest = sectionals_sorted.join(\n",
    "    gps_earliest_df,\n",
    "    on=race_id_cols,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Joined sectionals with earliest GPS time:\")\n",
    "sectionals_with_earliest.printSchema()\n",
    "#sectionals_with_earliest.show(10, truncate=False)\n",
    "sectionals_with_earliest.select(*race_id_cols, \"gate_index\", \"earliest_time_stamp_gps\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dfc2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# 4) Compute sec_time_stamp in sectionals\n",
    "#\n",
    "# First, compute cumulative_sectional_time per race/horse ordered by gate_index.\n",
    "######################################\n",
    "window_spec = Window.partitionBy(*race_id_cols).orderBy(\"gate_index\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "sectionals_with_cum = sectionals_with_earliest.withColumn(\n",
    "    \"cumulative_sectional_time\",\n",
    "    spark_sum(\"sectionals_sectional_time\").over(window_spec)\n",
    ")\n",
    "\n",
    "# Now add earliest_time_stamp_gps to cumulative_sectional_time to get sec_time_stamp\n",
    "sectionals_with_sec_time = sectionals_with_cum.withColumn(\n",
    "    \"sec_time_stamp\",\n",
    "    add_seconds_udf(col(\"earliest_time_stamp_gps\"), col(\"cumulative_sectional_time\"))\n",
    ")\n",
    "\n",
    "print(\"Computed sec_time_stamp by adding cumulative_sectional_time to earliest GPS timestamp.\")\n",
    "sectionals_with_sec_time.printSchema()\n",
    "#sectionals_with_sec_time.show(10, truncate=False)\n",
    "# This next on works but removing earliest_time_stamp_gps for space reasons\n",
    "#sectionals_with_sec_time.select(*race_id_cols, \"gate_index\", \"earliest_time_stamp_gps\",\n",
    "#                               \"sec_time_stamp\", \"sectionals_sectional_time\").show(10, truncate=False)\n",
    "    \n",
    "sectionals_with_sec_time.select(*race_id_cols, \"gate_index\", \"sec_time_stamp\", \"sectionals_sectional_time\").show(10, truncate=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a60af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################\n",
    "# 5) Create a temporary view to inspect the data\n",
    "#\n",
    "# We'll select a subset of columns that are essential for inspection:\n",
    "# course_cd, race_date, race_number, saddle_cloth_number, gate_name, gate_index, sec_time_stamp\n",
    "######################################\n",
    "view_df = sectionals_with_sec_time.select(\n",
    "    \"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\", \"sectionals_gate_name\", \"sectionals_sectional_time\",\"gate_index\", \"sec_time_stamp\"\n",
    ").orderBy(*race_id_cols, \"gate_index\")\n",
    "\n",
    "view_name = \"sectionals_with_sec_time_view\"\n",
    "view_df.createOrReplaceTempView(view_name)\n",
    "\n",
    "print(f\"Temporary view '{view_name}' created. You can run SQL queries like:\")\n",
    "print(f\"SELECT * FROM {view_name} WHERE course_cd='XYZ' AND race_number=123 AND saddle_cloth_number='A1' ORDER BY gate_index;\")\n",
    "\n",
    "######################################\n",
    "# Verification Step:\n",
    "# At this point, you can run:\n",
    "#\n",
    "# spark.sql(f\"SELECT * FROM {view_name} WHERE course_cd='...' AND race_number=... AND saddle_cloth_number='...' ORDER BY gate_index\").show(50, truncate=False)\n",
    "#\n",
    "# to verify that sec_time_stamp increments by sectional_time as expected.\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bea6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT saddle_cloth_number, sectionals_gate_name, \\\n",
    "gate_index, \\\n",
    "sec_time_stamp, \\\n",
    "sectionals_sectional_time \\\n",
    "FROM {view_name} \\\n",
    "WHERE course_cd='LRL' \\\n",
    "AND race_date = '2022-07-30' \\\n",
    "AND race_number = 5 \\\n",
    "GROUP BY course_cd, race_date, race_number, saddle_cloth_number, sectionals_gate_name, \\\n",
    "gate_index, sec_time_stamp, gate_index, sectionals_sectional_time \\\n",
    "ORDER BY saddle_cloth_number, gate_index\").show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95410dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT course_cd, race_date, race_number, saddle_cloth_number, \\\n",
    "COUNT(*) AS num_gates \\\n",
    "FROM {view_name} \\\n",
    "GROUP BY course_cd, race_date, race_number, saddle_cloth_number \\\n",
    "ORDER BY course_cd, race_date, race_number, saddle_cloth_number\").show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#  6) To aggregate GPS data for the interval leading up to each gate, you need a \n",
    "# start_time and an end_time for that interval. A common approach:\n",
    "#  1. start_time for gate i: sec_time_stamp from the previous gate \n",
    "#    (or the earliest GPS timestamp if it’s the first gate).\n",
    "#  2. end_time for gate i: sec_time_stamp of gate i itself.\n",
    "#\n",
    "#  1A. Sort Each Horse’s Sectionals by gate_index\n",
    "#  In 5) above I already sorted with:\n",
    "# window_spec = Window.partitionBy(*race_id_cols).orderBy(\"gate_index\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "# \n",
    "# 1B. Use a lag Window Function to Get start_time\n",
    "#\n",
    "#  For each row (gate i), define the “start” as the sec_time_stamp of the previous gate i-1:\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, col, when\n",
    "\n",
    "race_id_cols = [\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"]\n",
    "\n",
    "# Window for each horse, ordered by gate_index\n",
    "w = Window.partitionBy(*race_id_cols).orderBy(\"gate_index\")\n",
    "\n",
    "sectionals_intervals = sectionals_with_sec_time \\\n",
    "    .withColumn(\"start_time\",\n",
    "        lag(\"sec_time_stamp\").over(w)\n",
    "    )\n",
    "\n",
    "# If start_time is null (i.e., this is the first gate), default to earliest_time_stamp_gps \n",
    "# or the same sec_time_stamp\n",
    "# whichever logic you prefer:\n",
    "\n",
    "sectionals_intervals = sectionals_intervals.withColumn(\n",
    "    \"start_time\",\n",
    "    when(col(\"start_time\").isNull(), col(\"earliest_time_stamp_gps\"))\n",
    "    .otherwise(col(\"start_time\"))\n",
    ")\n",
    "\n",
    "# The \"end_time\" is simply this row's sec_time_stamp\n",
    "sectionals_intervals = sectionals_intervals.withColumn(\"end_time\", col(\"sec_time_stamp\"))\n",
    "\n",
    "sectionals_intervals.select(\n",
    "    *race_id_cols, \"gate_index\", \"start_time\", \"end_time\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# \t1.\tJoin on (course_cd, race_date, race_number, saddle_cloth_number) \n",
    "#       so we only consider the correct horse in the correct race.\n",
    "# \t2.\tFilter where gpspoint.time_stamp is between start_time and end_time.\n",
    "##############################################\n",
    "\n",
    "interval_join = gpspoint.join(\n",
    "    sectionals_intervals,\n",
    "    on=race_id_cols,\n",
    "    how=\"left\"  # or 'left', if you want all intervals even if no GPS data\n",
    ").filter(\n",
    "    (col(\"time_stamp\") >= col(\"start_time\")) &\n",
    "    (col(\"time_stamp\") <= col(\"end_time\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac2d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# After the filter, you have all rows that satisfy:\n",
    "# \t•\t(course_cd, race_date, race_number, saddle_cloth_number) is the same\n",
    "# \t•\ttime_stamp is in [start_time, end_time]\n",
    "#\n",
    "# Now you can groupBy the unique ID of each gate interval. Typically, that’s:\n",
    "# \t•\t(course_cd, race_date, race_number, saddle_cloth_number, gate_index)\n",
    "#\n",
    "# Then compute your aggregates:\n",
    "# \t1.\tavg_speed: average of gpspoint.speed\n",
    "# \t2.\tavg_stride_freq: average of gpspoint.stride_frequency\n",
    "# \t3.\tgps_first_progress: the first progress in that interval\n",
    "# \t4.\tgps_last_progress: the last progress in that interval\n",
    "# \t5.\tgps_first_longitude, gps_first_latitude if desired\n",
    "# \t6.\tgps_last_longitude, gps_last_latitude\n",
    "# \t7.\tPossibly, max_speed, min_speed, etc.\n",
    "############################################################################################\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    mean as spark_mean, max as spark_max, min as spark_min, first, last, count\n",
    ")\n",
    "\n",
    "aggregated = interval_join.groupBy(\n",
    "    *race_id_cols, \"gate_index\"\n",
    ").agg(\n",
    "    spark_mean(\"speed\").alias(\"gps_section_avg_speed\"),\n",
    "    spark_mean(\"stride_frequency\").alias(\"gps_section_avg_stride_freq\"),\n",
    "    first(\"progress\").alias(\"gps_first_progress\"),\n",
    "    last(\"progress\").alias(\"gps_last_progress\"),\n",
    "    first(\"longitude\").alias(\"gps_first_longitude\"),\n",
    "    first(\"latitude\").alias(\"gps_first_latitude\"),\n",
    "    last(\"longitude\").alias(\"gps_last_longitude\"),\n",
    "    last(\"latitude\").alias(\"gps_last_latitude\"),\n",
    "    first(\"location\").alias(\"gps_first_location\"),\n",
    "    last(\"location\").alias(\"gps_last_location\"),   \n",
    "    # add more aggregates if needed\n",
    "    count(\"*\").alias(\"gps_num_points\")  # how many points fell in the interval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14546caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#\n",
    "# Rejoin Aggregates Back to Sectionals\n",
    "#\n",
    "############################################################################################\n",
    "final_df = sectionals_intervals.join(\n",
    "    aggregated,\n",
    "    on=[*race_id_cols, \"gate_index\"],  # same grouping key\n",
    "    how=\"left\"  # or 'inner' if you only want intervals that had GPS data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40492cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e41eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#\n",
    "# How Many Intervals Have No GPS Data?\n",
    "#\n",
    "############################################################################################\n",
    "\n",
    "missing_count = final_df.filter(\"gps_num_points IS NULL\").count()\n",
    "print(\"Number of intervals with no GPS data:\", missing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c577e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.count() - aggregated.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ca249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see a handful of intervals that have no GPS:\n",
    "final_df.select(\n",
    "    \"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\", \"gate_index\",\n",
    "    \"gps_num_points\", \"gps_section_avg_speed\", \"gps_section_avg_stride_freq\"\n",
    ").filter(\"gps_num_points IS NULL\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27dee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# You expect one row in final_df for each (course_cd, race_date, race_number, \n",
    "# saddle_cloth_number, gate_index) if your aggregator and join logic are correct. \n",
    "# Confirm that no duplicates crept in:\n",
    "############################################################################################\n",
    "\n",
    "duplicates_df = final_df.groupBy(\n",
    "    \"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\", \"gate_index\"\n",
    ").count().filter(\"count > 1\")\n",
    "\n",
    "duplicates_df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d31281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be helpful to see how many GPS points are typically aggregated per gate interval. \n",
    "# For example:\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "final_df.select(\"gps_num_points\") \\\n",
    "        .describe() \\\n",
    "        .show()\n",
    "\n",
    "# Or a quick distribution:\n",
    "final_df.groupBy(\"gps_num_points\").count().orderBy(col(\"gps_num_points\").asc()).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d75fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#\n",
    "# To verify if the aggregator columns (gps_section_avg_speed, \n",
    "# gps_section_avg_stride_freq, etc.) are missing in intervals that do have gps_num_points:\n",
    "#\n",
    "############################################################################################\n",
    "\n",
    "final_df.select([\n",
    "    \"gps_section_avg_speed\",\n",
    "    \"gps_section_avg_stride_freq\",\n",
    "    \"gps_first_progress\",\n",
    "    \"gps_last_progress\"\n",
    "]).summary().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, when, col\n",
    "\n",
    "race_id_cols = [\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"]\n",
    "\n",
    "# Sort by gate_index for each horse\n",
    "w = Window.partitionBy(*race_id_cols).orderBy(\"gate_index\")\n",
    "\n",
    "sectionals_intervals = sectionals_with_sec_time \\\n",
    "    .withColumn(\"start_time\", lag(\"sec_time_stamp\").over(w)) \\\n",
    "    .withColumn(\n",
    "        \"start_time\",\n",
    "        when(col(\"start_time\").isNull(), col(\"earliest_time_stamp_gps\"))   # First gate starts at earliest GPS time\n",
    "        .otherwise(col(\"start_time\"))\n",
    "    ) \\\n",
    "    .withColumn(\"end_time\", col(\"sec_time_stamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e949c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_join = gpspoint.join(\n",
    "    sectionals_intervals,\n",
    "    on=race_id_cols,  # (course_cd, race_date, race_number, saddle_cloth_number)\n",
    "    how=\"inner\"       # or 'left' if you also want intervals even if no GPS\n",
    ").filter(\n",
    "    (col(\"time_stamp\") >= col(\"start_time\")) &\n",
    "    (col(\"time_stamp\") <= col(\"end_time\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87ea640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    mean as spark_mean, first, last, count\n",
    ")\n",
    "\n",
    "aggregated = interval_join.groupBy(*race_id_cols, \"gate_index\").agg(\n",
    "    spark_mean(\"speed\").alias(\"gps_section_avg_speed\"),\n",
    "    spark_mean(\"stride_frequency\").alias(\"gps_section_avg_stride_freq\"),\n",
    "    first(\"progress\").alias(\"gps_first_progress\"),\n",
    "    last(\"progress\").alias(\"gps_last_progress\"),\n",
    "    first(\"longitude\").alias(\"gps_first_longitude\"),\n",
    "    first(\"latitude\").alias(\"gps_first_latitude\"),\n",
    "    last(\"longitude\").alias(\"gps_last_longitude\"),\n",
    "    last(\"latitude\").alias(\"gps_last_latitude\"),\n",
    "    count(\"*\").alias(\"gps_num_points\")   # e.g. number of GPS rows in that interval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327e1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = sectionals_intervals.join(\n",
    "    aggregated,\n",
    "    on=[*race_id_cols, \"gate_index\"],\n",
    "    how=\"left\"       # Use 'left' so intervals with no matches still appear\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0cbaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_gps_df = final_df.filter(col(\"gps_num_points\").isNull())\n",
    "print(\"Number of intervals with no GPS:\", no_gps_df.count())\n",
    "no_gps_df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589be457",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.filter(\"gps_num_points IS NULL\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52345ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_gps_count = final_df.filter(col(\"earliest_time_stamp_gps\").isNull()).count()\n",
    "print(\"Number of intervals with missing earliest_time_stamp_gps:\", missing_gps_count)\n",
    "\n",
    "final_df.filter(col(\"earliest_time_stamp_gps\").isNull()) \\\n",
    "        .select(*race_id_cols) \\\n",
    "        .distinct() \\\n",
    "        .show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.groupBy(\"gps_num_points\").count().orderBy(\"gps_num_points\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facbf590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your gps_num_points is NULL or 0, you want to mark gps_coverage=False, otherwise True:\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "\n",
    "final_df_with_flag = final_df.withColumn(\n",
    "    \"gps_coverage\",\n",
    "    when((col(\"gps_num_points\").isNotNull()) & (col(\"gps_num_points\") > 0), lit(True))\n",
    "    .otherwise(lit(False))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee311c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a small sample:\n",
    "\n",
    "final_df_with_flag.select(\n",
    "    \"gps_num_points\", \"gps_coverage\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c770425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sectionals_horses = sectionals.select(\"course_cd\", \"race_date\", \n",
    "                                    \"race_number\", \"saddle_cloth_number\").distinct().count()\n",
    "\n",
    "total_gps_horses = gpspoint.select(\"course_cd\", \"race_date\", \\\n",
    "                                \"race_number\", \"saddle_cloth_number\").distinct().count()\n",
    "\n",
    "print(\"Distinct combos in sectionals:\", total_sectionals_horses)\n",
    "print(\"Distinct combos in gpspoint:\", total_gps_horses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa511a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da1472c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b47cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb6ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fea9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133011b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "routes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f5455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_speed(enriched_df):\n",
    "    \"\"\"\n",
    "    Calculates the average speed for each horse during a race.\n",
    "    Uses sum of distance_m and sum of time_diff_s computed per row by calculate_instantaneous_speed.\n",
    "    \"\"\"\n",
    "    # Define the race columns and ordering\n",
    "    race_cols = [\"course_cd\", \"race_date\", \"race_number\", \"horse_id\"]\n",
    "\n",
    "    # Window specification partitioned by race and ordered by gate\n",
    "    window_spec = Window.partitionBy(*race_cols).orderBy(\"sectionals_gate_numeric\")\n",
    "\n",
    "    # Compute per-sectional average speed (sec_avg_spd)\n",
    "    enriched_df = enriched_df.withColumn(\n",
    "        \"sec_avg_spd\",\n",
    "        (col(\"sectionals_distance_ran\") / col(\"sectionals_sectional_time\"))\n",
    "    )\n",
    "\n",
    "    # Compute cumulative distance and cumulative time up to the current gate\n",
    "    enriched_df = enriched_df.withColumn(\n",
    "        \"cum_distance\",\n",
    "        spark_sum(\"sectionals_distance_ran\").over(window_spec)\n",
    "    ).withColumn(\n",
    "        \"cum_time\",\n",
    "        spark_sum(\"sectionals_sectional_time\").over(window_spec)\n",
    "    )\n",
    "\n",
    "    # Compute cumulative average speed (cum_avg_spd)\n",
    "    enriched_df = enriched_df.withColumn(\n",
    "        \"cum_avg_spd\",\n",
    "        (col(\"cum_distance\") / col(\"cum_time\"))\n",
    "    )\n",
    "\n",
    "    # Compute speed change (spd_chng) as difference between sec_avg_spd and cum_avg_spd\n",
    "    enriched_df = enriched_df.withColumn(\n",
    "        \"spd_chng\",\n",
    "        col(\"sec_avg_spd\") - col(\"cum_avg_spd\")\n",
    "    )    \n",
    "    \n",
    "    enriched_df.drop(\"cum_distance\", \"cum_time\")\n",
    "    \n",
    "    enriched_df = enriched_df.withColumn(\"sec_avg_spd\", spark_round(col(\"sec_avg_spd\"), 3)) \\\n",
    "                         .withColumn(\"cum_avg_spd\", spark_round(col(\"cum_avg_spd\"), 3)) \\\n",
    "                         .withColumn(\"spd_chng\", spark_round(col(\"spd_chng\"), 3))\n",
    "                    \n",
    "    return enriched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = calculate_average_speed(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe42b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.select(\"sectionals_gate_numeric\",\"sec_avg_spd\", \"cum_avg_spd\", \"spd_chng\").show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def plot_distances(enriched_df):\n",
    "    \"\"\"\n",
    "    Plots distances to running and winning lines for a selected race and horse.\n",
    "    \n",
    "    :param enriched_df: DataFrame enriched with metrics.\n",
    "    :param selected_race: Dictionary specifying the race and horse to plot.\n",
    "    \"\"\"\n",
    "    # Define the race and horse you want to visualize\n",
    "    selected_race = {\n",
    "        \"course_cd\": \"TGP\",               # Example racecourse code\n",
    "        \"race_date\": \"2024-09-15\",        # Example race date\n",
    "        \"race_number\": 2,                 # Example race number\n",
    "        \"horse_id\": 898247                    # Example horse ID\n",
    "    }\n",
    "\n",
    "    # Filter for the specific race and horse\n",
    "    filtered_df = enriched_df.filter(\n",
    "        (col(\"course_cd\") == selected_race[\"course_cd\"]) &\n",
    "        (col(\"race_date\") == selected_race[\"race_date\"]) &\n",
    "        (col(\"race_number\") == selected_race[\"race_number\"]) &\n",
    "        (col(\"horse_id\") == selected_race[\"horse_id\"])\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Select relevant columns and convert to Pandas\n",
    "    plot_df = filtered_df.select(\n",
    "        \"sectionals_gate_numeric\",\n",
    "        \"run_ln_str_dist_run\",\n",
    "        \"run_ln_end_dist_run\",\n",
    "        \"win_ln_str_dist_win\",\n",
    "        \"win_ln_end_dist_win\"\n",
    "    ).toPandas()\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Running Line Distances\n",
    "    plt.plot(plot_df['sectionals_gate_numeric'], plot_df['run_ln_str_dist_run'], marker='o', label='Running Line Start Distance', color='blue')\n",
    "    plt.plot(plot_df['sectionals_gate_numeric'], plot_df['run_ln_end_dist_run'], marker='o', label='Running Line End Distance', color='blue', linestyle='--')\n",
    "    \n",
    "    # Winning Line Distances\n",
    "    plt.plot(plot_df['sectionals_gate_numeric'], plot_df['win_ln_str_dist_win'], marker='s', label='Winning Line Start Distance', color='red')\n",
    "    plt.plot(plot_df['sectionals_gate_numeric'], plot_df['win_ln_end_dist_win'], marker='s', label='Winning Line End Distance', color='red', linestyle='--')\n",
    "    \n",
    "    plt.xlabel('Gate Order')\n",
    "    plt.ylabel('Distance to Line (m)')\n",
    "    plt.title(f'Distances to Running and Winning Lines for Horse ID {selected_race[\"horse_id\"]} in Race {selected_race[\"race_number\"]} on {selected_race[\"race_date\"]}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b49b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distances(distance_pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc97554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_to_lines(spark, df1, routes):\n",
    "    \"\"\"\n",
    "    Calculates distances from GPS points (start and end of sectionals) to both the running and winning lines.\n",
    "    \"\"\"\n",
    "    # Ensure coordinates are converted to geometry\n",
    "    routes = routes.withColumn(\n",
    "        \"coordinates_geom\",\n",
    "        when(col(\"coordinates\").rlike(\"^[0-9A-F]+$\"), expr(\"ST_GeomFromWKB(unhex(coordinates))\"))\n",
    "        .otherwise(expr(\"ST_GeomFromWKT(coordinates)\"))\n",
    "    )\n",
    "\n",
    "    # Verify schemas\n",
    "    #df1.printSchema()\n",
    "    #routes.printSchema()\n",
    "\n",
    "    # Create temporary views\n",
    "    df1.createOrReplaceTempView(\"df1\")\n",
    "    routes.createOrReplaceTempView(\"routes\")\n",
    "\n",
    "    # SQL query to calculate distances\n",
    "    distance_df = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            g.course_cd, \n",
    "            g.race_date, \n",
    "            g.race_number, \n",
    "            g.saddle_cloth_number,\n",
    "            g.sectionals_gate_numeric,\n",
    "            g.horse_id,\n",
    "            r.line_type,\n",
    "            ST_Distance(g.gps_first_location_geom, r.coordinates_geom) AS str_dist_to_line,\n",
    "            ST_Distance(g.gps_last_location_geom, r.coordinates_geom) AS end_dist_to_line\n",
    "        FROM \n",
    "            df1 g\n",
    "        JOIN \n",
    "            routes r \n",
    "        ON \n",
    "            g.course_cd = r.course_cd\n",
    "        WHERE \n",
    "            r.line_type IN ('RUNNING_LINE', 'WINNING_LINE')\n",
    "    \"\"\")\n",
    "\n",
    "    # Pivot distances by line_type\n",
    "    distance_pivot_df = distance_df.groupBy(\n",
    "        \"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\", \"horse_id\", \"sectionals_gate_numeric\"\n",
    "    ).pivot(\"line_type\", [\"RUNNING_LINE\", \"WINNING_LINE\"]).agg(\n",
    "        first(\"str_dist_to_line\").alias(\"str_dist_to_run_line\"),\n",
    "        first(\"end_dist_to_line\").alias(\"end_dist_to_run_line\"),\n",
    "        first(\"str_dist_to_line\").alias(\"str_dist_to_win_line\"),\n",
    "        first(\"end_dist_to_line\").alias(\"end_dist_to_win_line\")  # Corrected from `end_dis_to_line`\n",
    "    )\n",
    "    \n",
    "    # Dictionary to map old column names to new shorter names\n",
    "    rename_mapping = {\n",
    "        \"RUNNING_LINE_str_dist_to_run_line\": \"run_ln_str_dist_run\",\n",
    "        \"RUNNING_LINE_end_dist_to_run_line\": \"run_ln_end_dist_run\",\n",
    "        \"RUNNING_LINE_str_dist_to_win_line\": \"run_ln_str_dist_win\",\n",
    "        \"RUNNING_LINE_end_dist_to_win_line\": \"run_ln_end_dist_win\",\n",
    "        \"WINNING_LINE_str_dist_to_run_line\": \"win_ln_str_dist_win\",\n",
    "        \"WINNING_LINE_end_dist_to_run_line\": \"win_ln_end_dist_win\",\n",
    "        \"WINNING_LINE_str_dist_to_win_line\": \"win_line_str_dist_win\",\n",
    "        \"WINNING_LINE_end_dist_to_win_line\": \"win_line_end_dist_win\",\n",
    "    }\n",
    "\n",
    "    # Rename columns using withColumnRenamed\n",
    "    for old_name, new_name in rename_mapping.items():\n",
    "        distance_pivot_df = distance_pivot_df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "    # Print schema for debugging\n",
    "    distance_pivot_df.printSchema()\n",
    "    \n",
    "    return distance_pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24be749a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distance_pivot_df = calculate_distance_to_lines(spark, merged_df, routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9235747",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_pivot_df.select('sectionals_gate_numeric').show().sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define the race and horse you want to visualize\n",
    "selected_race = {\n",
    "    \"course_cd\": \"TGP\",               # Example racecourse code\n",
    "    \"race_date\": \"2024-09-15\",        # Example race date\n",
    "    \"race_number\": 2,                 # Example race number\n",
    "    \"horse_id\": 898247                    # Example horse ID\n",
    "}\n",
    "\n",
    "# Apply the filter\n",
    "filtered_df = distance_pivot_df.filter(\n",
    "    (col(\"course_cd\") == selected_race[\"course_cd\"]) &\n",
    "    (col(\"race_date\") == selected_race[\"race_date\"]) &\n",
    "    (col(\"race_number\") == selected_race[\"race_number\"]) &\n",
    "    (col(\"horse_id\") == selected_race[\"horse_id\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd7585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select the relevant columns\n",
    "plot_df = filtered_df.select(\n",
    "    \"sectionals_gate_numeric\",\n",
    "    \"run_ln_str_dist_run\",\n",
    "    \"run_ln_end_dist_run\",\n",
    "    \"run_ln_str_dist_win\",\n",
    "    \"run_ln_end_dist_win\",\n",
    "    \"win_ln_str_dist_win\",\n",
    "    \"win_ln_end_dist_win\"\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3356906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Optional: Enhance plot aesthetics\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9412f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot start and end distances to the running line\n",
    "plt.plot(plot_df['sectionals_gate_numeric'], plot_df['run_ln_str_dist_run'], marker='o', label='Running Line Start Distance')\n",
    "plt.plot(plot_df['sectionals_gate_numeric'], plot_df['run_ln_end_dist_run'], marker='o', label='Running Line End Distance')\n",
    "\n",
    "plt.xlabel('Sectional Gate Numeric')\n",
    "plt.ylabel('Distance to Running Line (m)')\n",
    "plt.title('Distances to Running Line per Sectional Gate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe198ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot start and end distances to the winning line\n",
    "plt.plot(plot_df['sectionals_gate_numeric'], plot_df['run_ln_str_dist_win'], marker='o', label='Winning Line Start Distance')\n",
    "plt.plot(plot_df['sectionals_gate_numeric'], plot_df['run_ln_end_dist_win'], marker='o', label='Winning Line End Distance')\n",
    "\n",
    "plt.xlabel('Sectional Gate Numeric')\n",
    "plt.ylabel('Distance to Winning Line (m)')\n",
    "plt.title('Distances to Winning Line per Sectional Gate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot start and end distances to the running line\n",
    "plt.plot(plot_df['sectionals_gate_numeric'], plot_df['run_ln_str_dist_run'], marker='o', label='Running Line Start Distance')\n",
    "plt.plot(plot_df['sectionals_gate_numeric'], plot_df['run_ln_end_dist_run'], marker='o', label='Running Line End Distance')\n",
    "\n",
    "plt.xlabel('Sectional Gate Numeric')\n",
    "plt.ylabel('Distance to Running Line (m)')\n",
    "plt.title('Distances to Running Line per Sectional Gate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c355196e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c103ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e84d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d454a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_temp_views(merged_df, routes):\n",
    "    \"\"\"\n",
    "    Registers DataFrames as temporary SQL views.\n",
    "\n",
    "    :param gps_df: DataFrame containing GPS data.\n",
    "    :param routes_df: DataFrame containing Routes data.\n",
    "    \"\"\"\n",
    "    merged_df.createOrReplaceTempView(\"merged_df\")\n",
    "    routes.createOrReplaceTempView(\"routes\")\n",
    "    print(\"DataFrames registered as temporary views: 'merged_df' and 'routes'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f6f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_to_lines(spark):\n",
    "    \"\"\"\n",
    "    Calculates distance from each GPS point to the running and winning lines.\n",
    "\n",
    "    :param spark: SparkSession object.\n",
    "    :return: DataFrame with distance metrics.\n",
    "    \"\"\"\n",
    "    distance_df = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            g.course_cd, \n",
    "            g.race_date, \n",
    "            g.race_number, \n",
    "            g.saddle_cloth_number, \n",
    "            g.horse_id,\n",
    "            r.line_type,\n",
    "            ST_Distance(g.geometry, r.route_geometry) AS distance_to_line_m\n",
    "        FROM \n",
    "            merged_df g\n",
    "        JOIN \n",
    "            routes r \n",
    "        ON \n",
    "            g.course_cd = r.course_cd\n",
    "        WHERE \n",
    "            r.line_type IN ('RUNNING_LINE', 'WINNING_LINE')\n",
    "    \"\"\")\n",
    "\n",
    "    # Pivot the distance metrics\n",
    "    from pyspark.sql.functions import first\n",
    "    distance_pivot_df = distance_df.groupBy(\n",
    "        \"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\", \"horse_id\"\n",
    "    ).pivot(\"line_type\", [\"RUNNING_LINE\", \"WINNING_LINE\"]).agg(first(\"distance_to_line_m\"))\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    distance_pivot_df = distance_pivot_df.withColumnRenamed(\"RUNNING_LINE\", \"distance_to_running_line_m\") \\\n",
    "                                         .withColumnRenamed(\"WINNING_LINE\", \"distance_to_winning_line_m\")\n",
    "\n",
    "    return distance_pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_distance_metrics(main_df, distance_pivot_df):\n",
    "    \"\"\"\n",
    "    Joins the main GPS DataFrame with distance metrics.\n",
    "\n",
    "    :param main_df: DataFrame containing main GPS data.\n",
    "    :param distance_pivot_df: DataFrame containing distance metrics.\n",
    "    :return: Enriched DataFrame with distance metrics.\n",
    "    \"\"\"\n",
    "    enriched_df = main_df.join(\n",
    "        distance_pivot_df,\n",
    "        on=[\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\", \"horse_id\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    return enriched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8526b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_integrate_distances(spark, parquet_dir):\n",
    "    # Load data\n",
    "    merged_df, routes = load_parquet_data(spark, parquet_dir)\n",
    "    \n",
    "    # Inspect data\n",
    "    inspect_data(merged_df, routes)\n",
    "    \n",
    "    # Register temporary views\n",
    "    register_temp_views(merged_df, routes)\n",
    "    \n",
    "    # Calculate distance metrics\n",
    "    distance_pivot_df = calculate_distance_to_lines(spark)\n",
    "    \n",
    "    # Integrate with main DataFrame\n",
    "    enriched_df = integrate_distance_metrics(merged_df, distance_pivot_df)\n",
    "    \n",
    "    # Show enriched data\n",
    "    print(\"Enriched Data with Distance Metrics:\")\n",
    "    enriched_df.show(5, truncate=False)\n",
    "    \n",
    "    # Optionally, write enriched data back to Parquet\n",
    "    enriched_df.write.mode(\"overwrite\").parquet(os.path.join(parquet_dir, \"enriched_gpspoint.parquet\"))\n",
    "    print(\"Enriched GPS data written to 'enriched_gpspoint.parquet'.\")\n",
    "    \n",
    "    return enriched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Initialize Spark session\n",
    "        jdbc_driver_path = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/jdbc/postgresql-42.7.4.jar\"\n",
    "        sedona_jar_abs_path = \"/home/exx/sedona/apache-sedona-1.7.0-bin/sedona-spark-shaded-3.4_2.12-1.7.0.jar\"\n",
    "        \n",
    "        # Paths to GeoTools JAR files\n",
    "        geotools_jar_paths = [\n",
    "            \"/home/exx/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pyspark/jars/geotools-wrapper-1.1.0-25.2.jar\",\n",
    "            \"/home/exx/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pyspark/jars/sedona-python-adapter-3.0_2.12-1.2.0-incubating.jar\",\n",
    "            \"/home/exx/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pyspark/jars/sedona-viz-3.0_2.12-1.2.0-incubating.jar\",\n",
    "        ]\n",
    "        \n",
    "        # Initialize Spark session\n",
    "        spark = initialize_spark(jdbc_driver_path, sedona_jar_abs_path, geotools_jar_paths)\n",
    "        \n",
    "        # Test Sedona integration\n",
    "        test_sedona_integration(spark)\n",
    "        \n",
    "        # Define paths\n",
    "        parquet_dir = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/\"\n",
    "        \n",
    "        # Create dummy Parquet files if needed\n",
    "        # create_dummy_parquet_files(parquet_dir, spark)\n",
    "        \n",
    "        # Calculate and integrate distance and speed metrics\n",
    "        enriched_df = calculate_and_integrate_metrics(spark, parquet_dir)\n",
    "        \n",
    "        # Additional metrics can be calculated here...\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing: {e}\")\n",
    "    finally:\n",
    "        if 'spark' in locals():\n",
    "            spark.stop()\n",
    "            print(\"Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011faaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, unix_timestamp, when, col\n",
    "\n",
    "def calculate_instantaneous_speed(spark, enriched_df):\n",
    "    \"\"\"\n",
    "    Calculates instantaneous speed between consecutive GPS points for each horse.\n",
    "\n",
    "    :param spark: SparkSession object.\n",
    "    :param enriched_df: DataFrame enriched with distance metrics.\n",
    "    :return: DataFrame with speed metrics.\n",
    "    \"\"\"\n",
    "    # Add timestamp column if not present\n",
    "    # Assuming you have a 'timestamp' column. If not, you'll need to add it.\n",
    "    # For this example, we'll assume it's present.\n",
    "    \n",
    "    # Define window specification\n",
    "    window_spec = Window.partitionBy(\"horse_id\").orderBy(\"race_date\", \"race_number\", \"saddle_cloth_number\", \"timestamp\")\n",
    "    \n",
    "    # Calculate previous geometry and timestamp\n",
    "    enriched_df = enriched_df.withColumn(\"prev_geom\", lag(\"geometry\").over(window_spec)) \\\n",
    "                             .withColumn(\"prev_timestamp\", lag(\"timestamp\").over(window_spec))\n",
    "    \n",
    "    # Calculate distance between current and previous points\n",
    "    enriched_df = enriched_df.withColumn(\"distance_m\", \n",
    "        when(col(\"prev_geom\").isNotNull(), \n",
    "             expr(\"ST_Distance(geometry, prev_geom)\")) \\\n",
    "        .otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Calculate time difference in seconds\n",
    "    enriched_df = enriched_df.withColumn(\"time_diff_s\", \n",
    "        when(col(\"prev_timestamp\").isNotNull(),\n",
    "             (unix_timestamp(col(\"timestamp\")) - unix_timestamp(col(\"prev_timestamp\")))\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Calculate speed in m/s\n",
    "    enriched_df = enriched_df.withColumn(\"speed_m_s\", \n",
    "        when(col(\"time_diff_s\") > 0, col(\"distance_m\") / col(\"time_diff_s\"))\n",
    "        .otherwise(0)\n",
    "    )\n",
    "    \n",
    "    return enriched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_integrate_metrics(spark, parquet_dir):\n",
    "    # Load data\n",
    "    merged_df, routes = load_parquet_data(spark, parquet_dir)\n",
    "    \n",
    "    # Inspect data\n",
    "    inspect_data(merged_df, routes)\n",
    "    \n",
    "    # Register temporary views\n",
    "    register_temp_views(merged_df, routes)\n",
    "    \n",
    "    # Calculate distance metrics\n",
    "    distance_pivot_df = calculate_distance_to_lines(spark)\n",
    "    \n",
    "    # Integrate with main DataFrame\n",
    "    enriched_df = integrate_distance_metrics(merged_df, distance_pivot_df)\n",
    "    \n",
    "    # Calculate instantaneous speed\n",
    "    enriched_df = calculate_instantaneous_speed(spark, enriched_df)\n",
    "    \n",
    "    # Show enriched data with speed metrics\n",
    "    print(\"Enriched Data with Distance and Speed Metrics:\")\n",
    "    enriched_df.show(5, truncate=False)\n",
    "    \n",
    "    # Optionally, write enriched data back to Parquet\n",
    "    enriched_df.write.mode(\"overwrite\").parquet(os.path.join(parquet_dir, \"enriched_gpspoint_with_speed.parquet\"))\n",
    "    print(\"Enriched GPS data with speed metrics written to 'enriched_gpspoint_with_speed.parquet'.\")\n",
    "    \n",
    "    return enriched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4f2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_speed(enriched_df):\n",
    "    \"\"\"\n",
    "    Calculates average speed for each horse in each race.\n",
    "\n",
    "    :param enriched_df: DataFrame enriched with distance and speed metrics.\n",
    "    :return: DataFrame with average speed per horse per race.\n",
    "    \"\"\"\n",
    "    average_speed_df = enriched_df.groupBy(\n",
    "        \"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\", \"horse_id\"\n",
    "    ).agg(\n",
    "        (sum(\"distance_m\") / sum(\"time_diff_s\")).alias(\"average_speed_m_s\")\n",
    "    )\n",
    "    \n",
    "    return average_speed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6fcd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "def calculate_and_integrate_metrics(spark, parquet_dir):\n",
    "    # Load data\n",
    "    merged_df, routes = load_parquet_data(spark, parquet_dir)\n",
    "    \n",
    "    # Inspect data\n",
    "    inspect_data(merged_df, routes)\n",
    "    \n",
    "    # Register temporary views\n",
    "    register_temp_views(merged_df, routes)\n",
    "    \n",
    "    # Calculate distance metrics\n",
    "    distance_pivot_df = calculate_distance_to_lines(spark)\n",
    "    \n",
    "    # Integrate with main DataFrame\n",
    "    enriched_df = integrate_distance_metrics(merged_df, distance_pivot_df)\n",
    "    \n",
    "    # Calculate instantaneous speed\n",
    "    enriched_df = calculate_instantaneous_speed(spark, enriched_df)\n",
    "    \n",
    "    # Calculate average speed\n",
    "    average_speed_df = calculate_average_speed(enriched_df)\n",
    "    \n",
    "    # Show average speed data\n",
    "    print(\"Average Speed per Horse per Race:\")\n",
    "    average_speed_df.show(5, truncate=False)\n",
    "    \n",
    "    # Optionally, write average speed data back to Parquet\n",
    "    average_speed_df.write.mode(\"overwrite\").parquet(os.path.join(parquet_dir, \"average_speed.parquet\"))\n",
    "    print(\"Average speed data written to 'average_speed.parquet'.\")\n",
    "    \n",
    "    return enriched_df, average_speed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88051997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag\n",
    "\n",
    "def calculate_instantaneous_acceleration(enriched_df):\n",
    "    \"\"\"\n",
    "    Calculates instantaneous acceleration between consecutive speed measurements.\n",
    "\n",
    "    :param enriched_df: DataFrame enriched with speed metrics.\n",
    "    :return: DataFrame with acceleration metrics.\n",
    "    \"\"\"\n",
    "    window_spec = Window.partitionBy(\"horse_id\").orderBy(\"timestamp\")\n",
    "    \n",
    "    # Lag the speed column to get previous speed\n",
    "    enriched_df = enriched_df.withColumn(\"prev_speed_m_s\", lag(\"speed_m_s\").over(window_spec))\n",
    "    \n",
    "    # Calculate acceleration (delta_speed / delta_time)\n",
    "    enriched_df = enriched_df.withColumn(\"acceleration_m_s2\", \n",
    "        when(col(\"time_diff_s\") > 0, \n",
    "             (col(\"speed_m_s\") - col(\"prev_speed_m_s\")) / col(\"time_diff_s\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    return enriched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabc876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_path_deviation(spark, enriched_df):\n",
    "    \"\"\"\n",
    "    Calculates the deviation of each horse's path from the running line.\n",
    "\n",
    "    :param spark: SparkSession object.\n",
    "    :param enriched_df: DataFrame enriched with distance metrics.\n",
    "    :return: DataFrame with path deviation metrics.\n",
    "    \"\"\"\n",
    "    deviation_df = enriched_df.withColumn(\n",
    "        \"deviation_m\",\n",
    "        col(\"distance_to_running_line_m\")\n",
    "    )\n",
    "    \n",
    "    return deviation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7994fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_integrate_metrics(spark, parquet_dir):\n",
    "    # Load data\n",
    "    merged_df, routes = load_parquet_data(spark, parquet_dir)\n",
    "    \n",
    "    # Inspect data\n",
    "    inspect_data(merged_df, routes)\n",
    "    \n",
    "    # Register temporary views\n",
    "    register_temp_views(merged_df, routes)\n",
    "    \n",
    "    # Calculate distance metrics\n",
    "    distance_pivot_df = calculate_distance_to_lines(spark)\n",
    "    \n",
    "    # Integrate with main DataFrame\n",
    "    enriched_df = integrate_distance_metrics(merged_df, distance_pivot_df)\n",
    "    \n",
    "    # Calculate instantaneous speed\n",
    "    enriched_df = calculate_instantaneous_speed(spark, enriched_df)\n",
    "    \n",
    "    # Calculate acceleration\n",
    "    enriched_df = calculate_instantaneous_acceleration(enriched_df)\n",
    "    \n",
    "    # Calculate average speed\n",
    "    average_speed_df = calculate_average_speed(enriched_df)\n",
    "    \n",
    "    # Calculate path deviation\n",
    "    deviation_df = calculate_path_deviation(spark, enriched_df)\n",
    "    \n",
    "    # Show metrics\n",
    "    print(\"Enriched Data with All Metrics:\")\n",
    "    enriched_df.show(5, truncate=False)\n",
    "    \n",
    "    print(\"Average Speed per Horse per Race:\")\n",
    "    average_speed_df.show(5, truncate=False)\n",
    "    \n",
    "    print(\"Path Deviation per GPS Point:\")\n",
    "    deviation_df.show(5, truncate=False)\n",
    "    \n",
    "    # Optionally, write metrics to Parquet\n",
    "    enriched_df.write.mode(\"overwrite\").parquet(os.path.join(parquet_dir, \"enriched_gpspoint_all_metrics.parquet\"))\n",
    "    average_speed_df.write.mode(\"overwrite\").parquet(os.path.join(parquet_dir, \"average_speed.parquet\"))\n",
    "    deviation_df.write.mode(\"overwrite\").parquet(os.path.join(parquet_dir, \"path_deviation.parquet\"))\n",
    "    \n",
    "    print(\"All metrics data written to respective Parquet files.\")\n",
    "    \n",
    "    return enriched_df, average_speed_df, deviation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1842c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_metrics(enriched_df):\n",
    "    \"\"\"\n",
    "    Validates the correctness of calculated metrics.\n",
    "\n",
    "    :param enriched_df: DataFrame enriched with metrics.\n",
    "    \"\"\"\n",
    "    enriched_df.select(\n",
    "        \"course_cd\",\n",
    "        \"race_date\",\n",
    "        \"race_number\",\n",
    "        \"saddle_cloth_number\",\n",
    "        \"horse_id\",\n",
    "        \"timestamp\",\n",
    "        \"distance_m\",\n",
    "        \"time_diff_s\",\n",
    "        \"speed_m_s\",\n",
    "        \"acceleration_m_s2\",\n",
    "        \"deviation_m\"\n",
    "    ).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee4067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_anomalies(enriched_df):\n",
    "    \"\"\"\n",
    "    Identifies and handles anomalies in speed and acceleration metrics.\n",
    "\n",
    "    :param enriched_df: DataFrame enriched with metrics.\n",
    "    :return: Cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # Example: Remove entries with speed > 20 m/s (arbitrary threshold)\n",
    "    cleaned_df = enriched_df.filter(col(\"speed_m_s\") <= 20)\n",
    "    \n",
    "    # Example: Remove entries with acceleration > 10 m/s^2\n",
    "    cleaned_df = cleaned_df.filter(col(\"acceleration_m_s2\") <= 10)\n",
    "    \n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d4b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_average_speed(average_speed_df):\n",
    "    \"\"\"\n",
    "    Plots average speed per horse.\n",
    "\n",
    "    :param average_speed_df: DataFrame with average speed metrics.\n",
    "    \"\"\"\n",
    "    # Convert to Pandas DataFrame\n",
    "    pandas_df = average_speed_df.toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='horse_id', y='average_speed_m_s', hue='race_number', data=pandas_df)\n",
    "    plt.title('Average Speed per Horse per Race')\n",
    "    plt.xlabel('Horse ID')\n",
    "    plt.ylabel('Average Speed (m/s)')\n",
    "    plt.legend(title='Race Number')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b809aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.sql.types import GeometryType\n",
    "\n",
    "def visualize_horse_trajectory(spark, horse_id, parquet_dir):\n",
    "    \"\"\"\n",
    "    Visualizes the trajectory of a specific horse.\n",
    "\n",
    "    :param spark: SparkSession object.\n",
    "    :param horse_id: ID of the horse to visualize.\n",
    "    :param parquet_dir: Directory where Parquet files are stored.\n",
    "    \"\"\"\n",
    "    # Load enriched data\n",
    "    enriched_df = spark.read.parquet(os.path.join(parquet_dir, \"enriched_gpspoint_all_metrics.parquet\"))\n",
    "    \n",
    "    # Filter for the specific horse\n",
    "    horse_df = enriched_df.filter(col(\"horse_id\") == horse_id).orderBy(\"timestamp\")\n",
    "    \n",
    "    # Collect geometries\n",
    "    geometries = horse_df.select(\"geometry\").rdd.map(lambda row: row[0]).collect()\n",
    "    \n",
    "    # Use GeoTools or other libraries to plot the trajectory\n",
    "    # This requires integrating with a geospatial library in Python\n",
    "    # Alternatively, export the data and use external tools like QGIS\n",
    "    print(f\"Collected {len(geometries)} geometries for horse {horse_id}.\")\n",
    "    \n",
    "    # Example: Export to WKT for external visualization\n",
    "    horse_df.select(\"timestamp\", \"geometry\").write.csv(os.path.join(parquet_dir, f\"horse_{horse_id}_trajectory.csv\"), header=True)\n",
    "    print(f\"Trajectory data for horse {horse_id} exported to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda2f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_dataframes(enriched_df, average_speed_df):\n",
    "    \"\"\"\n",
    "    Caches DataFrames that are accessed multiple times.\n",
    "\n",
    "    :param enriched_df: Enriched GPS DataFrame.\n",
    "    :param average_speed_df: Average Speed DataFrame.\n",
    "    \"\"\"\n",
    "    enriched_df.cache()\n",
    "    average_speed_df.cache()\n",
    "    print(\"DataFrames cached for improved performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6095f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "def optimize_joins(spark, main_df, lookup_df):\n",
    "    \"\"\"\n",
    "    Optimizes joins by broadcasting the lookup DataFrame.\n",
    "\n",
    "    :param spark: SparkSession object.\n",
    "    :param main_df: Main DataFrame to join.\n",
    "    :param lookup_df: Lookup DataFrame to broadcast.\n",
    "    :return: Joined DataFrame.\n",
    "    \"\"\"\n",
    "    joined_df = merged_df.join(broadcast(lookup_df), on=\"key_column\", how=\"left\")\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00924b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def initialize_environment_secure():\n",
    "    \"\"\"\n",
    "    Initializes the environment securely by loading credentials from environment variables.\n",
    "\n",
    "    :return: Tuple containing SparkSession and connection parameters.\n",
    "    \"\"\"\n",
    "    # Paths and configurations\n",
    "    config_path = '/home/exx/myCode/horse-racing/FoxRiverAIRacing/config.ini'\n",
    "    log_file = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/logs/SparkPy_load.log\"\n",
    "    jdbc_driver_path = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/jdbc/postgresql-42.7.4.jar\"\n",
    "    sedona_jar_abs_path = \"/home/exx/sedona/apache-sedona-1.7.0-bin/sedona-spark-shaded-3.4_2.12-1.7.0.jar\"\n",
    "    \n",
    "    # Paths to GeoTools JAR files\n",
    "    geotools_jar_paths = [\n",
    "        \"/home/exx/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pyspark/jars/geotools-wrapper-1.1.0-25.2.jar\",\n",
    "        \"/home/exx/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pyspark/jars/sedona-python-adapter-3.0_2.12-1.2.0-incubating.jar\",\n",
    "        \"/home/exx/anaconda3/envs/mamba_env/envs/tf_310/lib/python3.10/site-packages/pyspark/jars/sedona-viz-3.0_2.12-1.2.0-incubating.jar\",\n",
    "    ]\n",
    "    \n",
    "    # Load configuration\n",
    "    config = load_config(config_path)\n",
    "\n",
    "    # Database credentials from config and environment variables\n",
    "    db_host = config['database']['host']\n",
    "    db_port = config['database']['port']\n",
    "    db_name = config['database']['dbname']\n",
    "    db_user = config['database']['user']\n",
    "    db_password = os.getenv(\"DB_PASSWORD\")  # Ensure DB_PASSWORD is set\n",
    "    \n",
    "    if not db_password:\n",
    "        raise ValueError(\"Database password is missing. Set it in the DB_PASSWORD environment variable.\")\n",
    "    \n",
    "    # JDBC URL and properties\n",
    "    jdbc_url = f\"jdbc:postgresql://{db_host}:{db_port}/{db_name}\"\n",
    "    jdbc_properties = {\n",
    "        \"user\": db_user,\n",
    "        \"password\": db_password,\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "    \n",
    "    # Initialize Spark session\n",
    "    spark = initialize_spark(jdbc_driver_path, sedona_jar_abs_path, geotools_jar_paths)\n",
    "    \n",
    "    # Initialize logging\n",
    "    initialize_logging(log_file)\n",
    "    queries = sql_queries()\n",
    "    \n",
    "    return spark, jdbc_url, jdbc_properties, queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f1a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, sum as spark_sum, when, expr\n",
    "from pyspark.sql.window import Window\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "\n",
    "def load_config(config_path):\n",
    "    config = ConfigParser()\n",
    "    config.read(config_path)\n",
    "    return config\n",
    "\n",
    "def initialize_logging(log_file):\n",
    "    logging.basicConfig(\n",
    "        filename=log_file,\n",
    "        filemode='a',\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        level=logging.INFO\n",
    "    )\n",
    "    logging.info(\"Logging initialized.\")\n",
    "\n",
    "def initialize_spark(jdbc_driver_path, sedona_jar_path, geotools_jar_paths):\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Horse Racing Data Processing\") \\\n",
    "        .master(\"local[16]\") \\\n",
    "        .config(\"spark.driver.memory\", \"48g\") \\\n",
    "        .config(\"spark.executor.memory\", \"32g\") \\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"8g\") \\\n",
    "        .config(\"spark.default.parallelism\", \"256\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"256\") \\\n",
    "        .config(\"spark.jars\", f\"{jdbc_driver_path},{sedona_jar_path},{','.join(geotools_jar_paths)}\") \\\n",
    "        .config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    "        .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    "        .config(\"spark.kryo.buffer\", \"64k\") \\\n",
    "        .config(\"spark.kryo.buffer.max\", \"512m\") \\\n",
    "        .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"LEGACY\") \\\n",
    "        .config(\"spark.sql.parquet.int96RebaseModeInWrite\", \"LEGACY\") \\\n",
    "        .config(\"spark.local.dir\", \"/path/to/nvme/storage\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Register Sedona functions and types\n",
    "    SedonaRegistrator.registerAll(spark)\n",
    "    \n",
    "    # Set log level to ERROR\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    print(\"Spark session created successfully with Sedona and GeoTools integrated.\")\n",
    "    return spark\n",
    "\n",
    "def load_data_from_postgresql(spark, jdbc_url, table_name, jdbc_properties, output_path):\n",
    "    try:\n",
    "        df = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", jdbc_properties[\"user\"]) \\\n",
    "            .option(\"password\", jdbc_properties[\"password\"]) \\\n",
    "            .option(\"driver\", jdbc_properties[\"driver\"]) \\\n",
    "            .load()\n",
    "        \n",
    "        df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        logging.info(f\"Data from {table_name} loaded successfully to {output_path}.\")\n",
    "        print(f\"Data from {table_name} loaded successfully to {output_path}.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data from PostgreSQL: {e}\")\n",
    "        print(f\"Error loading data from PostgreSQL: {e}\")\n",
    "\n",
    "def load_parquet_data(spark, parquet_dir):\n",
    "    gps_df = spark.read.parquet(os.path.join(parquet_dir, \"gpspoint.parquet\"))\n",
    "    routes_df = spark.read.parquet(os.path.join(parquet_dir, \"routes.parquet\"))\n",
    "    return gps_df, routes_df\n",
    "\n",
    "def inspect_data(gps_df, routes_df):\n",
    "    print(\"GPS Data Sample:\")\n",
    "    gps_df.show(5, truncate=False)\n",
    "    \n",
    "    print(\"Routes Data Sample:\")\n",
    "    routes_df.show(5, truncate=False)\n",
    "\n",
    "def register_temp_views(gps_df, routes_df):\n",
    "    gps_df.createOrReplaceTempView(\"gps_points\")\n",
    "    routes_df.createOrReplaceTempView(\"routes\")\n",
    "    print(\"DataFrames registered as temporary views: 'gps_points' and 'routes'.\")\n",
    "\n",
    "def calculate_distance_to_lines(spark):\n",
    "    distance_df = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            g.course_cd, \n",
    "            g.race_date, \n",
    "            g.race_number, \n",
    "            g.saddle_cloth_number, \n",
    "            g.horse_id,\n",
    "            r.line_type,\n",
    "            ST_Distance(g.geometry, r.route_geometry) AS distance_to_line_m\n",
    "        FROM \n",
    "            gps_points g\n",
    "        JOIN \n",
    "            routes r \n",
    "        ON \n",
    "            g.course_cd = r.course_cd\n",
    "        WHERE \n",
    "            r.line_type IN ('RUNNING_LINE', 'WINNING_LINE')\n",
    "    \"\"\")\n",
    "\n",
    "    from pyspark.sql.functions import first\n",
    "    distance_pivot_df = distance_df.groupBy(\n",
    "        \"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\", \"horse_id\"\n",
    "    ).pivot(\"line_type\", [\"RUNNING_LINE\", \"WINNING_LINE\"]).agg(first(\"distance_to_line_m\"))\n",
    "\n",
    "    distance_pivot_df = distance_pivot_df.withColumnRenamed(\"RUNNING_LINE\", \"distance_to_running_line_m\") \\\n",
    "                                         .withColumnRenamed(\"WINNING_LINE\", \"distance_to_winning_line_m\")\n",
    "\n",
    "    return distance_pivot_df\n",
    "\n",
    "def integrate_distance_metrics(main_df, distance_pivot_df):\n",
    "    enriched_df = main_df.join(\n",
    "        distance_pivot_df,\n",
    "        on=[\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\", \"horse_id\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    return enriched_df\n",
    "\n",
    "def calculate_instantaneous_speed(spark, enriched_df):\n",
    "    window_spec = Window.partitionBy(\"horse_id\").orderBy(\"race_date\", \"race_number\", \"saddle_cloth_number\", \"timestamp\")\n",
    "    \n",
    "    enriched_df = enriched_df.withColumn(\"prev_geom\", lag(\"geometry\").over(window_spec)) \\\n",
    "                             .withColumn(\"prev_timestamp\", lag(\"timestamp\").over(window_spec))\n",
    "    \n",
    "    enriched_df = enriched_df.withColumn(\"distance_m\", \n",
    "        when(col(\"prev_geom\").isNotNull(), \n",
    "             expr(\"ST_Distance(geometry, prev_geom)\")) \\\n",
    "        .otherwise(0)\n",
    "    )\n",
    "    \n",
    "    enriched_df = enriched_df.withColumn(\"time_diff_s\", \n",
    "        when(col(\"prev_timestamp\").isNotNull(),\n",
    "             (unix_timestamp(col(\"timestamp\")) - unix_timestamp(col(\"prev_timestamp\")))\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    enriched_df = enriched_df.withColumn(\"speed_m_s\", \n",
    "        when(col(\"time_diff_s\") > 0, col(\"distance_m\") / col(\"time_diff_s\"))\n",
    "        .otherwise(0)\n",
    "    )\n",
    "    \n",
    "    return enriched_df\n",
    "\n",
    "def calculate_instantaneous_acceleration(enriched_df):\n",
    "    window_spec = Window.partitionBy(\"horse_id\").orderBy(\"timestamp\")\n",
    "    \n",
    "    enriched_df = enriched_df.withColumn(\"prev_speed_m_s\", lag(\"speed_m_s\").over(window_spec))\n",
    "    \n",
    "    enriched_df = enriched_df.withColumn(\"acceleration_m_s2\", \n",
    "        when(col(\"time_diff_s\") > 0, \n",
    "             (col(\"speed_m_s\") - col(\"prev_speed_m_s\")) / col(\"time_diff_s\")\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    return enriched_df\n",
    "\n",
    "def calculate_average_speed(enriched_df):\n",
    "    average_speed_df = enriched_df.groupBy(\n",
    "        \"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\", \"horse_id\"\n",
    "    ).agg(\n",
    "        (spark_sum(\"distance_m\") / spark_sum(\"time_diff_s\")).alias(\"average_speed_m_s\")\n",
    "    )\n",
    "    \n",
    "    return average_speed_df\n",
    "\n",
    "def calculate_path_deviation(spark, enriched_df):\n",
    "    deviation_df = enriched_df.withColumn(\n",
    "        \"deviation_m\",\n",
    "        col(\"distance_to_running_line_m\")\n",
    "    )\n",
    "    \n",
    "    return deviation_df\n",
    "\n",
    "def handle_anomalies(enriched_df):\n",
    "    cleaned_df = enriched_df.filter(col(\"speed_m_s\") <= 20) \\\n",
    "                             .filter(col(\"acceleration_m_s2\") <= 10)\n",
    "    return cleaned_df\n",
    "\n",
    "def validate_metrics(enriched_df):\n",
    "    enriched_df.select(\n",
    "        \"course_cd\",\n",
    "        \"race_date\",\n",
    "        \"race_number\",\n",
    "        \"saddle_cloth_number\",\n",
    "        \"horse_id\",\n",
    "        \"timestamp\",\n",
    "        \"distance_m\",\n",
    "        \"time_diff_s\",\n",
    "        \"speed_m_s\",\n",
    "        \"acceleration_m_s2\",\n",
    "        \"deviation_m\"\n",
    "    ).show(5, truncate=False)\n",
    "\n",
    "def visualize_average_speed(average_speed_df):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    pandas_df = average_speed_df.toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='horse_id', y='average_speed_m_s', hue='race_number', data=pandas_df)\n",
    "    plt.title('Average Speed per Horse per Race')\n",
    "    plt.xlabel('Horse ID')\n",
    "    plt.ylabel('Average Speed (m/s)')\n",
    "    plt.legend(title='Race Number')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize environment securely\n",
    "        spark, jdbc_url, jdbc_properties, parquet_dir, log_file = initialize_environment_secure()\n",
    "        \n",
    "        # Initialize Spark session\n",
    "        spark = initialize_spark(jdbc_driver_path, sedona_jar_abs_path, geotools_jar_paths)\n",
    "        \n",
    "        # Test Sedona integration\n",
    "        test_sedona_integration(spark)\n",
    "        \n",
    "        # Define paths\n",
    "        parquet_dir = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/\"\n",
    "        \n",
    "        # Create dummy Parquet files if needed\n",
    "        # create_dummy_parquet_files(parquet_dir, spark)\n",
    "        \n",
    "        # Calculate and integrate metrics\n",
    "        enriched_df, average_speed_df, deviation_df = calculate_and_integrate_metrics(spark, parquet_dir)\n",
    "        \n",
    "        # Validate metrics\n",
    "        validate_metrics(enriched_df)\n",
    "        \n",
    "        # Visualize average speed\n",
    "        visualize_average_speed(average_speed_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing: {e}\")\n",
    "    finally:\n",
    "        if 'spark' in locals():\n",
    "            spark.stop()\n",
    "            print(\"Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e600b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d35a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291a176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa477da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b0344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038ae59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55166586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4eb921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b5eff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019adaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed15e4-cdb0-49fa-ba8e-c15a7a04d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, avg, max as F_max, min as F_min, count, stddev, row_number, lag, lead,\n",
    "    when, lit, first, last, abs, sum as F_sum, udf\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "import math\n",
    "import logging\n",
    "\n",
    "# Initialize Spark session\n",
    "def initialize_spark():\n",
    "    jdbc_driver_path = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/jdbc/postgresql-42.7.4.jar\"\n",
    "    extra_class_path = jdbc_driver_path  # Ensure this is the correct path to your JDBC JAR\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"GPS Sectionals Analysis - Enhanced Aggregation\") \\\n",
    "        .config(\"spark.driver.extraClassPath\", extra_class_path) \\\n",
    "        .config(\"spark.executor.extraClassPath\", extra_class_path) \\\n",
    "        .config(\"spark.driver.memory\", \"64g\") \\\n",
    "        .config(\"spark.executor.memory\", \"32g\") \\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"8g\") \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", \"1000\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    logging.info(\"Spark session created successfully.\")\n",
    "    return spark\n",
    "\n",
    "# Define the haversine function and UDF\n",
    "def define_haversine_udf():\n",
    "    def haversine(lat1, lon1, lat2, lon2):\n",
    "        # Check for None values\n",
    "        if None in (lat1, lon1, lat2, lon2):\n",
    "            return 0.0\n",
    "        # Convert decimal degrees to radians\n",
    "        lon1, lat1, lon2, lat2 = map(\n",
    "            lambda x: math.radians(float(x)), [lon1, lat1, lon2, lat2]\n",
    "        )\n",
    "        # Haversine formula\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = math.sin(dlat / 2) ** 2 + \\\n",
    "            math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
    "        c = 2 * math.asin(math.sqrt(a))\n",
    "        # Radius of earth in meters\n",
    "        r = 6371000\n",
    "        return c * r\n",
    "    return udf(haversine, DoubleType())\n",
    "\n",
    "# Load data from the database\n",
    "def load_data(spark, course):\n",
    "    # Load sectionals data\n",
    "    sectionals_df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"sectionals\",\n",
    "        properties=jdbc_properties\n",
    "    ).filter(col(\"course_cd\") == course).select(\n",
    "        col(\"course_cd\"),\n",
    "        col(\"race_date\"),\n",
    "        col(\"race_number\"),\n",
    "        col(\"saddle_cloth_number\"),\n",
    "        col(\"gate_name\"),\n",
    "        col(\"length_to_finish\"),\n",
    "        col(\"sectional_time\"),\n",
    "        col(\"running_time\"),\n",
    "        col(\"distance_back\"),\n",
    "        col(\"distance_ran\"),\n",
    "        col(\"number_of_strides\")\n",
    "    )\n",
    "\n",
    "    # Load gpspoint data\n",
    "    gps_df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"gpspoint\",\n",
    "        properties=jdbc_properties\n",
    "    ).filter(col(\"course_cd\") == course).select(\n",
    "        col(\"course_cd\"),\n",
    "        col(\"race_date\"),\n",
    "        col(\"race_number\"),\n",
    "        col(\"saddle_cloth_number\"),\n",
    "        \"time_stamp\",\n",
    "        \"longitude\",\n",
    "        \"latitude\",\n",
    "        \"progress\",\n",
    "        \"speed\",\n",
    "        \"stride_frequency\"\n",
    "    )\n",
    "\n",
    "    # Load races data to get nominal race distance\n",
    "    races_df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"races\",\n",
    "        properties=jdbc_properties\n",
    "    ).filter(col(\"course_cd\") == course).select(\n",
    "        \"course_cd\",\n",
    "        \"race_date\",\n",
    "        \"race_number\",\n",
    "        col(\"distance\").alias(\"nominal_distance\"),\n",
    "        col(\"dist_unit\").alias(\"nominal_dist_unit\")\n",
    "    )\n",
    "    return sectionals_df, gps_df, races_df\n",
    "\n",
    "# Rename columns for clarity\n",
    "def rename_columns(sectionals_df, gps_df, races_df):\n",
    "    sectionals_df = sectionals_df.select(\n",
    "        col(\"course_cd\").alias(\"s_course_cd\"),\n",
    "        col(\"race_date\").alias(\"s_race_date\"),\n",
    "        col(\"race_number\").alias(\"s_race_number\"),\n",
    "        col(\"saddle_cloth_number\").alias(\"s_saddle_cloth_number\"),\n",
    "        \"gate_name\",\n",
    "        \"length_to_finish\",\n",
    "        \"sectional_time\",\n",
    "        \"running_time\",\n",
    "        \"distance_back\",\n",
    "        \"distance_ran\",\n",
    "        \"number_of_strides\"\n",
    "    )\n",
    "\n",
    "    gps_df = gps_df.select(\n",
    "        col(\"course_cd\").alias(\"g_course_cd\"),\n",
    "        col(\"race_date\").alias(\"g_race_date\"),\n",
    "        col(\"race_number\").alias(\"g_race_number\"),\n",
    "        col(\"saddle_cloth_number\").alias(\"g_saddle_cloth_number\"),\n",
    "        \"time_stamp\",\n",
    "        \"longitude\",\n",
    "        \"latitude\",\n",
    "        \"progress\",\n",
    "        \"speed\",\n",
    "        \"stride_frequency\"\n",
    "    )\n",
    "\n",
    "    races_df = races_df.select(\n",
    "        col(\"course_cd\").alias(\"r_course_cd\"),\n",
    "        col(\"race_date\").alias(\"r_race_date\"),\n",
    "        col(\"race_number\").alias(\"r_race_number\"),\n",
    "        col(\"nominal_distance\"),\n",
    "        col(\"nominal_dist_unit\")\n",
    "    )\n",
    "    return sectionals_df, gps_df, races_df\n",
    "\n",
    "# Join sectionals and GPS data\n",
    "def join_sectionals_gps(sectionals_df, gps_df):\n",
    "    gps_with_gates = sectionals_df.alias(\"s\").join(\n",
    "        gps_df.alias(\"g\"),\n",
    "        (col(\"s.s_course_cd\") == col(\"g.g_course_cd\")) &\n",
    "        (col(\"s.s_race_date\") == col(\"g.g_race_date\")) &\n",
    "        (col(\"s.s_race_number\") == col(\"g.g_race_number\")) &\n",
    "        (col(\"s.s_saddle_cloth_number\") == col(\"g.g_saddle_cloth_number\")),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    return gps_with_gates\n",
    "\n",
    "# Find the closest GPS point to each gate\n",
    "def find_closest_gps_points(gps_with_gates):\n",
    "    gps_with_gates = gps_with_gates.withColumn(\n",
    "        \"progress_diff\",\n",
    "        abs(col(\"g.progress\") - col(\"s.length_to_finish\"))\n",
    "    )\n",
    "\n",
    "    window_spec_gate = Window.partitionBy(\n",
    "        \"s.s_course_cd\",\n",
    "        \"s.s_race_date\",\n",
    "        \"s.s_race_number\",\n",
    "        \"s.s_saddle_cloth_number\",\n",
    "        \"gate_name\"\n",
    "    ).orderBy(\"progress_diff\")\n",
    "\n",
    "    gps_at_gates = gps_with_gates.withColumn(\n",
    "        \"row_number\",\n",
    "        row_number().over(window_spec_gate)\n",
    "    ).filter(col(\"row_number\") == 1).drop(\"row_number\", \"progress_diff\")\n",
    "    return gps_at_gates\n",
    "\n",
    "# Calculate speed changes and acceleration\n",
    "def calculate_speed_acceleration(gps_at_gates):\n",
    "    gate_order_window = Window.partitionBy(\n",
    "        \"s.s_course_cd\",\n",
    "        \"s.s_race_date\",\n",
    "        \"s.s_race_number\",\n",
    "        \"s.s_saddle_cloth_number\"\n",
    "    ).orderBy(\"gate_name\")  # Modify if gate_name is not sortable\n",
    "\n",
    "    gps_at_gates = gps_at_gates.withColumn(\n",
    "        \"previous_speed\",\n",
    "        lag(\"g.speed\").over(gate_order_window)\n",
    "    ).withColumn(\n",
    "        \"speed_change\",\n",
    "        col(\"g.speed\") - col(\"previous_speed\")\n",
    "    ).withColumn(\n",
    "        \"acceleration\",\n",
    "        when(col(\"previous_speed\").isNotNull(),\n",
    "             col(\"speed_change\") / col(\"previous_speed\")\n",
    "             ).otherwise(lit(0))\n",
    "    )\n",
    "    return gps_at_gates\n",
    "\n",
    "# Identify fastest and slowest gates\n",
    "def identify_fastest_slowest_gates(gps_at_gates):\n",
    "    speed_window = Window.partitionBy(\n",
    "        \"s.s_course_cd\",\n",
    "        \"s.s_race_date\",\n",
    "        \"s.s_race_number\",\n",
    "        \"s.s_saddle_cloth_number\"\n",
    "    )\n",
    "\n",
    "    gps_at_gates = gps_at_gates.withColumn(\n",
    "        \"max_speed\",\n",
    "        F_max(\"g.speed\").over(speed_window)\n",
    "    ).withColumn(\n",
    "        \"min_speed\",\n",
    "        F_min(\"g.speed\").over(speed_window)\n",
    "    ).withColumn(\n",
    "        \"is_fastest_gate\",\n",
    "        when(col(\"g.speed\") == col(\"max_speed\"), lit(1)).otherwise(lit(0))\n",
    "    ).withColumn(\n",
    "        \"is_slowest_gate\",\n",
    "        when(col(\"g.speed\") == col(\"min_speed\"), lit(1)).otherwise(lit(0))\n",
    "    )\n",
    "    return gps_at_gates\n",
    "\n",
    "# Calculate fatigue factor\n",
    "def calculate_fatigue_factor(gps_at_gates):\n",
    "    # Extract finish speed\n",
    "    finish_speed = gps_at_gates.filter(col(\"gate_name\") == \"Finish\").select(\n",
    "        col(\"s.s_course_cd\").alias(\"course_cd\"),\n",
    "        col(\"s.s_race_date\").alias(\"race_date\"),\n",
    "        col(\"s.s_race_number\").alias(\"race_number\"),\n",
    "        col(\"s.s_saddle_cloth_number\").alias(\"saddle_cloth_number\"),\n",
    "        col(\"g.speed\").alias(\"finish_speed\")\n",
    "    )\n",
    "\n",
    "    # Join finish speed back to gps_at_gates\n",
    "    gps_at_gates = gps_at_gates.join(\n",
    "        finish_speed,\n",
    "        on=[\n",
    "            gps_at_gates[\"s.s_course_cd\"] == finish_speed[\"course_cd\"],\n",
    "            gps_at_gates[\"s.s_race_date\"] == finish_speed[\"race_date\"],\n",
    "            gps_at_gates[\"s.s_race_number\"] == finish_speed[\"race_number\"],\n",
    "            gps_at_gates[\"s.s_saddle_cloth_number\"] == finish_speed[\"saddle_cloth_number\"]\n",
    "        ],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Calculate fatigue factor\n",
    "    gps_at_gates = gps_at_gates.withColumn(\n",
    "        \"fatigue_factor\",\n",
    "        (col(\"max_speed\") - col(\"finish_speed\")) / col(\"max_speed\")\n",
    "    )\n",
    "    return gps_at_gates\n",
    "\n",
    "# Prepare aggregated metrics per gate\n",
    "def prepare_aggregated_metrics(gps_at_gates):\n",
    "    aggregated_metrics = gps_at_gates.select(\n",
    "        col(\"s.s_course_cd\").alias(\"course_cd\"),\n",
    "        col(\"s.s_race_date\").alias(\"race_date\"),\n",
    "        col(\"s.s_race_number\").alias(\"race_number\"),\n",
    "        col(\"s.s_saddle_cloth_number\").alias(\"saddle_cloth_number\"),\n",
    "        \"gate_name\",\n",
    "        col(\"g.speed\").alias(\"speed\"),\n",
    "        \"acceleration\",\n",
    "        \"fatigue_factor\",\n",
    "        \"is_fastest_gate\",\n",
    "        \"is_slowest_gate\"\n",
    "    )\n",
    "\n",
    "    per_gate_metrics = aggregated_metrics.groupBy(\n",
    "        \"course_cd\",\n",
    "        \"race_date\",\n",
    "        \"race_number\",\n",
    "        \"saddle_cloth_number\",\n",
    "        \"gate_name\"\n",
    "    ).agg(\n",
    "        avg(\"speed\").alias(\"avg_speed\"),\n",
    "        avg(\"acceleration\").alias(\"avg_acceleration\"),\n",
    "        F_max(\"speed\").alias(\"max_speed\"),\n",
    "        F_min(\"speed\").alias(\"min_speed\"),\n",
    "        F_max(\"fatigue_factor\").alias(\"fatigue_factor\"),\n",
    "        F_max(\"is_fastest_gate\").alias(\"is_fastest_gate\"),\n",
    "        F_max(\"is_slowest_gate\").alias(\"is_slowest_gate\")\n",
    "    )\n",
    "    return per_gate_metrics\n",
    "\n",
    "# Calculate actual distance run and ground loss\n",
    "def calculate_ground_loss(gps_df, races_df, haversine_udf):\n",
    "    # Define window specification\n",
    "    window_spec_time = Window.partitionBy(\n",
    "        \"g_course_cd\", \"g_race_date\", \"g_race_number\", \"g_saddle_cloth_number\"\n",
    "    ).orderBy(\"time_stamp\")\n",
    "\n",
    "    # Get previous latitude and longitude\n",
    "    gps_df = gps_df.withColumn(\"prev_latitude\", lag(\"latitude\").over(window_spec_time))\n",
    "    gps_df = gps_df.withColumn(\"prev_longitude\", lag(\"longitude\").over(window_spec_time))\n",
    "\n",
    "    # Calculate segment distance using haversine formula\n",
    "    gps_df = gps_df.withColumn(\n",
    "        \"segment_distance\",\n",
    "        haversine_udf(\n",
    "            col(\"prev_latitude\"),\n",
    "            col(\"prev_longitude\"),\n",
    "            col(\"latitude\"),\n",
    "            col(\"longitude\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Fill null values\n",
    "    gps_df = gps_df.fillna({\"segment_distance\": 0})\n",
    "\n",
    "    # Calculate cumulative distance\n",
    "    gps_df = gps_df.withColumn(\n",
    "        \"cumulative_distance\",\n",
    "        F_sum(\"segment_distance\").over(window_spec_time)\n",
    "    )\n",
    "\n",
    "    # Get total distance per horse\n",
    "    total_distance_df = gps_df.groupBy(\n",
    "        \"g_course_cd\", \"g_race_date\", \"g_race_number\", \"g_saddle_cloth_number\"\n",
    "    ).agg(\n",
    "        F_max(\"cumulative_distance\").alias(\"total_distance_run\")\n",
    "    )\n",
    "\n",
    "    # Convert nominal distance to meters with proper scaling\n",
    "    races_df = races_df.withColumn(\n",
    "        \"nominal_distance_meters\",\n",
    "        when(col(\"nominal_dist_unit\") == 'F', (col(\"nominal_distance\") / 100) * 201.168)\n",
    "        .when(col(\"nominal_dist_unit\") == 'Y', col(\"nominal_distance\") * 0.9144)\n",
    "        .otherwise(lit(None))\n",
    "    )\n",
    "\n",
    "\n",
    "    # Join total_distance_df with races_df\n",
    "    distance_comparison_df = total_distance_df.join(\n",
    "        races_df,\n",
    "        (total_distance_df[\"g_course_cd\"] == races_df[\"r_course_cd\"]) &\n",
    "        (total_distance_df[\"g_race_date\"] == races_df[\"r_race_date\"]) &\n",
    "        (total_distance_df[\"g_race_number\"] == races_df[\"r_race_number\"]),\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Calculate ground loss\n",
    "    distance_comparison_df = distance_comparison_df.withColumn(\n",
    "        \"ground_loss\",\n",
    "        col(\"total_distance_run\") - col(\"nominal_distance_meters\")\n",
    "    )\n",
    "\n",
    "    # Join and select required columns\n",
    "    distance_comparison_df = total_distance_df.join(\n",
    "        races_df,\n",
    "        (total_distance_df[\"g_course_cd\"] == races_df[\"r_course_cd\"]) &\n",
    "        (total_distance_df[\"g_race_date\"] == races_df[\"r_race_date\"]) &\n",
    "        (total_distance_df[\"g_race_number\"] == races_df[\"r_race_number\"]),\n",
    "        how=\"left\"\n",
    "    ).withColumn(\n",
    "        \"ground_loss\",\n",
    "        col(\"total_distance_run\") - col(\"nominal_distance_meters\")\n",
    "    ).select(\n",
    "        col(\"g_course_cd\").alias(\"course_cd\"),\n",
    "        col(\"g_race_date\").alias(\"race_date\"),\n",
    "        col(\"g_race_number\").alias(\"race_number\"),\n",
    "        col(\"g_saddle_cloth_number\").alias(\"saddle_cloth_number\"),\n",
    "        \"total_distance_run\",\n",
    "        \"ground_loss\"\n",
    "    )\n",
    "\n",
    "    return distance_comparison_df\n",
    "\n",
    "# Integrate ground loss into final metrics\n",
    "def integrate_ground_loss(per_gate_metrics, distance_comparison_df):\n",
    "    final_metrics = per_gate_metrics.join(\n",
    "        distance_comparison_df,\n",
    "        on=[\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    return final_metrics\n",
    "    \n",
    "# Write final metrics to the database\n",
    "def write_to_database(final_metrics):\n",
    "    # Ensure all necessary columns are included\n",
    "    required_columns = [\n",
    "        'course_cd', 'race_date', 'race_number', 'saddle_cloth_number',\n",
    "        'gate_name', 'avg_speed', 'avg_acceleration', 'max_speed', 'min_speed',\n",
    "        'fatigue_factor', 'is_fastest_gate', 'is_slowest_gate',\n",
    "        'total_distance_run', 'ground_loss'\n",
    "    ]\n",
    "\n",
    "    # Check if all required columns are present\n",
    "    missing_columns = [col for col in required_columns if col not in final_metrics.columns]\n",
    "    if missing_columns:\n",
    "        logging.error(f\"Missing columns in final_metrics: {missing_columns}\")\n",
    "        return\n",
    "\n",
    "    # Write final metrics to the database\n",
    "    final_metrics.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"gps_aggregated_results_with_gates\",\n",
    "        mode=\"append\",\n",
    "        properties=jdbc_properties\n",
    "    )\n",
    "    \n",
    "# Main processing function for each course\n",
    "def process_course(course, spark, haversine_udf):\n",
    "    print(f\"Processing course: {course}\")\n",
    "\n",
    "    # Load data\n",
    "    sectionals_df, gps_df, races_df = load_data(spark, course)\n",
    "    logging.info(f\"Loading data from database for course: {course}\")\n",
    "    # Rename columns\n",
    "    sectionals_df, gps_df, races_df = rename_columns(sectionals_df, gps_df, races_df)\n",
    "\n",
    "    # Join sectionals and GPS data\n",
    "    gps_with_gates = join_sectionals_gps(sectionals_df, gps_df)\n",
    "\n",
    "    # Find closest GPS points to gates\n",
    "    gps_at_gates = find_closest_gps_points(gps_with_gates)\n",
    "\n",
    "    # Calculate speed changes and acceleration\n",
    "    gps_at_gates = calculate_speed_acceleration(gps_at_gates)\n",
    "\n",
    "    # Identify fastest and slowest gates\n",
    "    gps_at_gates = identify_fastest_slowest_gates(gps_at_gates)\n",
    "\n",
    "    # Calculate fatigue factor\n",
    "    gps_at_gates = calculate_fatigue_factor(gps_at_gates)\n",
    "\n",
    "    # Prepare aggregated metrics\n",
    "    per_gate_metrics = prepare_aggregated_metrics(gps_at_gates)\n",
    "\n",
    "    # Calculate ground loss\n",
    "    distance_comparison_df = calculate_ground_loss(gps_df, races_df, haversine_udf)\n",
    "\n",
    "    per_gate_metrics.printSchema()\n",
    "    logging.info(per_gate_metrics.printSchema())\n",
    "    distance_comparison_df.printSchema()\n",
    "    logging.info(distance_comparison_df.printSchema())\n",
    "    # Integrate ground loss into final metrics\n",
    "    final_metrics = integrate_ground_loss(per_gate_metrics, distance_comparison_df)\n",
    "    \n",
    "    final_metrics = integrate_ground_loss(per_gate_metrics, distance_comparison_df)\n",
    "    final_metrics.printSchema()\n",
    "    logging.info(final_metrics.printSchema())\n",
    "\n",
    "    # Write final metrics to database\n",
    "    write_to_database(final_metrics)\n",
    "\n",
    "    print(f\"Completed processing for course: {course}\")\n",
    "\n",
    "def main():\n",
    "    # Initialize Spark session\n",
    "    spark = initialize_spark()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    # Define haversine UDF\n",
    "    haversine_udf = define_haversine_udf()\n",
    "\n",
    "    # List of courses\n",
    "    courses = ['CNL', 'SAR', 'PIM', 'TSA', 'BEL', 'MVR', 'TWO', 'CLS', 'KEE', 'TAM',\n",
    "               'TTP', 'TKD', 'ELP', 'PEN', 'HOU', 'DMR', 'TLS', 'AQU', 'MTH', 'TGP',\n",
    "               'TGG', 'CBY', 'LRL', 'TED', 'IND', 'CTD', 'ASD', 'TCD', 'LAD', 'MED',\n",
    "               'TOP', 'HOO']\n",
    "\n",
    "    # Process each course\n",
    "    for course in courses:\n",
    "        try:\n",
    "            process_course(course, spark, haversine_udf)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing course {course}: {e}\")\n",
    "\n",
    "    # Stop Spark session\n",
    "    spark.stop()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f75611-2333-4205-8bce-0d9280c55440",
   "metadata": {},
   "source": [
    "1. Modular Functions\n",
    "\n",
    ">    •\tinitialize_spark(): Sets up the Spark session with the necessary configurations.\n",
    "> \n",
    ">    •\tdefine_haversine_udf(): Defines the Haversine function to calculate distances between GPS points and registers it as a UDF.\n",
    "> \n",
    ">\t•\t**load_data()**: Loads the sectionals, gpspoint, and races data from the database for a given course.\n",
    "> \n",
    "> \t•\trename_columns(): Aliases columns in the DataFrames for clarity and to avoid naming conflicts.\n",
    "> \n",
    "> \t•\tjoin_sectionals_gps(): Joins the sectionals and gpspoint DataFrames on the race and horse identifiers.\n",
    "> \n",
    ">\t•\tfind_closest_gps_points(): Finds the closest GPS point to each gate for each horse.\n",
    "> \n",
    "> \t•\tcalculate_speed_acceleration(): Calculates speed changes and acceleration between gates.\n",
    "> \n",
    ">\t•\tidentify_fastest_slowest_gates(): Identifies the fastest and slowest gates for each horse.\n",
    "> \n",
    ">\t•\tcalculate_fatigue_factor(): Computes the fatigue factor for each horse based on their maximum speed and finish speed.\n",
    "> \n",
    ">\t•\tprepare_aggregated_metrics(): Aggregates the metrics per gate and per horse.\n",
    "> \n",
    ">\t•\tcalculate_ground_loss(): Calculates the actual distance run by each horse and computes the ground loss compared to the nominal race distance.\n",
    "> \n",
    ">\t•\tintegrate_ground_loss(): Integrates the ground loss metric into the final aggregated metrics.\n",
    ">\t•\twrite_to_database(): Writes the final metrics to the database.\n",
    "> \n",
    ">\t•\tprocess_course(): Orchestrates the processing steps for a single course.\n",
    "> \n",
    ">\t•\tmain(): The main function that initializes the Spark session, processes each course, and stops the Spark session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714c416-49e6-4ab7-899e-8a36276825dd",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "\n",
    "\t1.\tVisualization:\n",
    "\t•\tPlot speed and stride frequency trends across gates for specific horses to validate the analysis.\n",
    "\t2.\tTesting:\n",
    "\t•\tApply this to a few more courses to confirm that the calculations are meaningful and robust across different races.\n",
    "\t3.\tAnalysis:\n",
    "\t•\tCompare fatigue factors or speed trends between winners and non-winners to derive insights about race dynamics.\n",
    "\n",
    "These enhancements should provide valuable insights and improve the predictive capabilities of your analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffe146d-1e27-4d44-a282-06623496d1f0",
   "metadata": {},
   "source": [
    "\n",
    "# Additional steps to derive fatigue factors, sectional efficiency, etc.\n",
    "\n",
    "Innovative Applications\n",
    "\n",
    "\t1.\tPredictive Fatigue Model: Train a model using TPD data to predict fatigue thresholds for horses.\n",
    "\t2.\tRace Simulation: Use historical TPD data to simulate how horses might perform in upcoming races.\n",
    "\t3.\tDynamic Betting Insights: Provide real-time insights into how race conditions or competitor performance might influence outcomes.\n",
    "\n",
    "Spark’s distributed computing will allow you to process the large dataset efficiently and scale as needed. By creatively combining TPD and EQB data, you can uncover insights that traditional analysis might overlook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
