{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db68106",
   "metadata": {},
   "source": [
    "# LSTM: Sequence-aware â€œFoxSpeedScoreâ€ Generator\n",
    "\n",
    "Train your LSTM to analyze a horseâ€™s past performance sequence and predict a FoxSpeedScore that reflects its projected strength today.\n",
    "\n",
    "âœ… You can then: â€¢ Rank horses by this score for simple win/place bets â€¢ Or use it as a ranking feature in your CatBoost/YetiRank model\n",
    "\n",
    "â¸»\n",
    "\n",
    "ðŸ” How Accurate is Ranking by Speed Score Alone?\n",
    "\n",
    "ðŸ”¥ The Good:\n",
    "\n",
    "If your LSTM is trained on well-normalized, consistent historical speed signals: â€¢ Relative ordering can be quite meaningful â€¢ Especially in smaller fields or when youâ€™re identifying top 3â€“4 finishers â€¢ Even if absolute values are off, ranking is often more robust\n",
    "\n",
    "â„ï¸ The Limitations: â€¢ LSTM alone wonâ€™t account for todayâ€™s track conditions, surface bias, jockey/trainer changes, distance, class jump/drop â€¢ It may favor horses that ran fast recently but are now outclassed or mispositioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0d976bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "os.environ.pop(\"CUDA_VISIBLE_DEVICES\", None)\n",
    "\n",
    "# Setup Environment\n",
    "import time\n",
    "from optuna.importance import MeanDecreaseImpurityImportanceEvaluator\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import optuna.visualization as viz\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import (col, count, row_number, abs, unix_timestamp, mean, \n",
    "                                   when, lit, min as F_min, max as F_max , upper, trim,\n",
    "                                   row_number, mean as F_mean, countDistinct, last, first, when)\n",
    "from src.data_preprocessing.data_prep1.data_utils import initialize_environment\n",
    "from src.data_preprocessing.data_prep1.data_loader import load_data_from_postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff7422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "spark, jdbc_url, jdbc_properties, parquet_dir, log_file = initialize_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c779b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gps_sql_queries():\n",
    "    queries = {\n",
    "        \"gps_horse\": \"\"\"\n",
    "            SELECT g.course_cd, g.race_date,g.race_number,\n",
    "            REGEXP_REPLACE(TRIM(UPPER(saddle_cloth_number)), '\\s+$', '') AS saddle_cloth_number, time_stamp, \n",
    "            longitude, latitude, speed, progress, stride_frequency, g.post_time, location,\n",
    "            re.axciskey, h.horse_id, re.official_fin, h.horse_name\n",
    "            FROM gpspoint g\n",
    "            JOIN results_entries re on g.course_cd = re.course_cd\n",
    "                AND g.race_date = re.race_date\n",
    "                AND g.race_number = re.race_number\n",
    "                AND g.saddle_cloth_number = re.program_num\n",
    "            JOIN horse h on re.axciskey = h.axciskey\n",
    "            WHERE speed is not null\n",
    "            AND progress is not null\n",
    "            AND stride_frequency is not null\n",
    "            \"\"\",\n",
    "        \"sectionals\": \"\"\"\n",
    "            SELECT s.course_cd, s.race_date, s.race_number, \n",
    "            REGEXP_REPLACE(TRIM(UPPER(saddle_cloth_number)), '\\s+$', '') AS saddle_cloth_number, \n",
    "            s.gate_name, s.gate_numeric, s.length_to_finish, s.sectional_time, s.running_time, \n",
    "            s.distance_back, s.distance_ran, s.number_of_strides, s.post_time, re.official_fin\n",
    "            FROM sectionals s\n",
    "            JOIN results_entries re on s.course_cd = re.course_cd\n",
    "                AND s.race_date = re.race_date\n",
    "                AND s.race_number = re.race_number\n",
    "                AND s.saddle_cloth_number = re.program_num\n",
    "            JOIN horse h on re.axciskey = h.axciskey \n",
    "            WHERE length_to_finish is not null\n",
    "            AND sectional_time is not null\n",
    "            AND running_time is not null\n",
    "            AND distance_back is not null\n",
    "            AND distance_ran is not null\n",
    "            AND distance_ran is not null\n",
    "            \"\"\"\n",
    "    }\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7d1447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- course_cd: string (nullable = true)\n",
      " |-- race_date: date (nullable = true)\n",
      " |-- race_number: integer (nullable = true)\n",
      " |-- saddle_cloth_number: string (nullable = true)\n",
      " |-- time_stamp: timestamp (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- speed: double (nullable = true)\n",
      " |-- progress: double (nullable = true)\n",
      " |-- stride_frequency: double (nullable = true)\n",
      " |-- post_time: timestamp (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- axciskey: string (nullable = true)\n",
      " |-- horse_id: integer (nullable = true)\n",
      " |-- official_fin: integer (nullable = true)\n",
      " |-- horse_name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- course_cd: string (nullable = true)\n",
      " |-- race_date: date (nullable = true)\n",
      " |-- race_number: integer (nullable = true)\n",
      " |-- saddle_cloth_number: string (nullable = true)\n",
      " |-- gate_name: string (nullable = true)\n",
      " |-- gate_numeric: double (nullable = true)\n",
      " |-- length_to_finish: double (nullable = true)\n",
      " |-- sectional_time: double (nullable = true)\n",
      " |-- running_time: double (nullable = true)\n",
      " |-- distance_back: double (nullable = true)\n",
      " |-- distance_ran: double (nullable = true)\n",
      " |-- number_of_strides: double (nullable = true)\n",
      " |-- post_time: timestamp (nullable = true)\n",
      " |-- official_fin: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "queries = gps_sql_queries()\n",
    "dfs = load_data_from_postgresql(spark, jdbc_url, jdbc_properties, queries, parquet_dir)\n",
    "        # Suppose we have a dictionary of queries\n",
    "for name, df in dfs.items():\n",
    "    logging.info(f\"DataFrame '{name}' loaded. Schema:\")\n",
    "    df.printSchema()\n",
    "    if name == \"gps_horse\":\n",
    "        gps_horse_df = df\n",
    "    elif name == \"sectionals\":\n",
    "        sectionals_df = df    \n",
    "    else:\n",
    "        logging.error(f\"Unknown DataFrame name: {name}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d54ab74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "gps_horse_df.write.mode(\"overwrite\").parquet(f\"{parquet_dir}/gps_horse_df\")\n",
    "sectionals_df.write.mode(\"overwrite\").parquet(f\"{parquet_dir}/sectionals_df\")\n",
    "logging.info(f\"Data written to Parquet in {time.time() - start_time:.2f} seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f2d5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, lpad, date_format\n",
    "\n",
    "gps_horse_df = gps_horse_df.withColumn(\n",
    "    \"race_id\",\n",
    "    concat_ws(\n",
    "        \"_\",\n",
    "        col(\"course_cd\"),\n",
    "        date_format(col(\"race_date\"), \"yyyyMMdd\"),\n",
    "        lpad(col(\"race_number\").cast(\"string\"), 2, \"0\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2e36d",
   "metadata": {},
   "source": [
    "## Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff39033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, to_date\n",
    "\n",
    "# Ensure race_date is in proper date format\n",
    "gps_horse_df = gps_horse_df.withColumn(\"race_date\", to_date(col(\"race_date\")))\n",
    "\n",
    "# Define a Window partitioned by horse_id, ordered by race_date and post_time\n",
    "window_spec = Window.partitionBy(\"horse_id\").orderBy(col(\"race_date\"), col(\"post_time\"))\n",
    "\n",
    "# Assign a row number to ensure strict chronological order\n",
    "gps_horse_df = gps_horse_df.withColumn(\"race_ordinal\", row_number().over(window_spec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea62df",
   "metadata": {},
   "source": [
    "## Padding Sequences to a Fixed Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31f4d5",
   "metadata": {},
   "source": [
    "###  Step 1: Select Relevant Columns & Order by Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a7950f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [\"speed\", \"progress\", \"stride_frequency\"]  # Add or adjust as needed\n",
    "\n",
    "# Filter nulls (optional, depending on your quality)\n",
    "gps_horse_df_filtered = gps_horse_df.dropna(subset=features + [\"race_ordinal\", \"horse_id\"])\n",
    "\n",
    "# Group and collect sequences\n",
    "from pyspark.sql.functions import collect_list, struct\n",
    "\n",
    "sequence_df = gps_horse_df_filtered \\\n",
    "    .select(\"horse_id\", \"race_ordinal\", *features) \\\n",
    "    .orderBy(\"horse_id\", \"race_ordinal\") \\\n",
    "    .withColumn(\"features\", struct(*[col(c) for c in features])) \\\n",
    "    .groupBy(\"horse_id\") \\\n",
    "    .agg(collect_list(\"features\").alias(\"sequence\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d1f22f",
   "metadata": {},
   "source": [
    "###  PySpark Code to Count GPS Points per Horse per Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9528e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+------------------+-----------------+\n",
      "|summary|        race_id|          horse_id|       seq_length|\n",
      "+-------+---------------+------------------+-----------------+\n",
      "|  count|         353595|            353595|           353595|\n",
      "|   mean|           null|251063.94700716922|96.16060464655892|\n",
      "| stddev|           null|335519.12547872664|22.24771164873177|\n",
      "|    min|AQU_20221229_01|                 1|                1|\n",
      "|    max|TWO_20241215_13|           2279735|              305|\n",
      "+-------+---------------+------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+----------+------+\n",
      "|race_id        |horse_id|seq_length|bucket|\n",
      "+---------------+--------+----------+------+\n",
      "|AQU_20221229_01|6303    |96        |short |\n",
      "|AQU_20221229_01|6303    |96        |short |\n",
      "|AQU_20221229_01|6303    |96        |short |\n",
      "|AQU_20221229_01|6303    |96        |short |\n",
      "|AQU_20221229_01|6303    |96        |short |\n",
      "|AQU_20221229_01|6303    |96        |short |\n",
      "|AQU_20221229_01|6303    |96        |short |\n",
      "|AQU_20221229_01|6303    |96        |short |\n",
      "|AQU_20221229_01|6303    |96        |short |\n",
      "|AQU_20221229_01|6303    |96        |short |\n",
      "+---------------+--------+----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=====================================================>  (22 + 1) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|bucket|   count|\n",
      "+------+--------+\n",
      "|  long|  360411|\n",
      "|medium|17958421|\n",
      "| short|15683077|\n",
      "+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Compute sequence lengths per horse per race\n",
    "df_seq_len = gps_horse_df.groupBy(\"race_id\", \"horse_id\").agg(F.count(\"*\").alias(\"seq_length\"))\n",
    "\n",
    "# Show distribution statistics to validate\n",
    "df_seq_len.describe().show()\n",
    "\n",
    "# Step 2: Define length buckets\n",
    "df_buckets = df_seq_len.withColumn(\n",
    "    \"bucket\",\n",
    "    F.when(F.col(\"seq_length\") <= 100, \"short\")\n",
    "     .when(F.col(\"seq_length\") <= 150, \"medium\")\n",
    "     .otherwise(\"long\")\n",
    ")\n",
    "\n",
    "# Step 3: Join back with original data\n",
    "df_binned = gps_horse_df.join(df_buckets, [\"race_id\", \"horse_id\"])\n",
    "\n",
    "# Show sample results\n",
    "df_binned.select(\"race_id\", \"horse_id\", \"seq_length\", \"bucket\").show(10, truncate=False)\n",
    "\n",
    "# Step 4: Get overall distribution of bucket counts\n",
    "df_binned.groupBy(\"bucket\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9b482",
   "metadata": {},
   "source": [
    "###  Filter by Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6f6b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df = df_binned.filter(F.col(\"bucket\") == \"short\")\n",
    "medium_df = df_binned.filter(F.col(\"bucket\") == \"medium\")\n",
    "long_df = df_binned.filter(F.col(\"bucket\") == \"long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3c771",
   "metadata": {},
   "source": [
    "## Assign Target/Relevance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9fe311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_piecewise_log_labels_spark(df, alpha = 30.0, beta = 4.0):\n",
    "    from pyspark.sql import functions as F\n",
    "    df_out = df.withColumn(\n",
    "        \"relevance\",\n",
    "        F.when(F.col(\"official_fin\") == 1, 70.0)\n",
    "         .when(F.col(\"official_fin\") == 2, 56.0)\n",
    "         .when(F.col(\"official_fin\") == 3, 44.0)\n",
    "         .when(F.col(\"official_fin\") == 4, 34.0)\n",
    "         .otherwise(F.lit(alpha) / F.log(F.lit(beta) + F.col(\"official_fin\")))\n",
    "    ).withColumn(\n",
    "        \"top4_label\",\n",
    "        F.when(F.col(\"official_fin\") <= 4, F.lit(1)).otherwise(F.lit(0))\n",
    "    )\n",
    "    \n",
    "    return df_out  # â† ðŸ”¥ This line is critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9f1919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df = assign_piecewise_log_labels_spark(short_df)\n",
    "medium_df = assign_piecewise_log_labels_spark(medium_df)\n",
    "long_df = assign_piecewise_log_labels_spark(long_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd773956",
   "metadata": {},
   "source": [
    "### Convert to Sequence Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cdb1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct, collect_list\n",
    "\n",
    "def make_sequences(df, features):\n",
    "    return df.select(\"race_id\", \"horse_id\", \"race_ordinal\", *features) \\\n",
    "        .orderBy(\"race_id\", \"horse_id\", \"race_ordinal\") \\\n",
    "        .withColumn(\"features\", struct(*[col(f) for f in features])) \\\n",
    "        .groupBy(\"race_id\", \"horse_id\") \\\n",
    "        .agg(collect_list(\"features\").alias(\"sequence\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcfedb6",
   "metadata": {},
   "source": [
    "#### Apply this to each bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd2b755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"speed\", \"progress\", \"stride_frequency\"]\n",
    "short_seq = make_sequences(short_df, features)\n",
    "medium_seq = make_sequences(medium_df, features)\n",
    "long_seq = make_sequences(long_df, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36140c0f",
   "metadata": {},
   "source": [
    "#### Convert to Pandas + Pad\n",
    "\n",
    "Convert to Pandas and pad each list to a fixed length:\n",
    "\n",
    "\tâ€¢\tshort â†’ pad to 100\n",
    "\tâ€¢\tmedium â†’ pad to 150\n",
    "\tâ€¢\tlong â†’ pad to max (or truncate at 200â€“250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77e201e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in official_fin per (race_id, horse_id)\n",
    "short_seq = short_seq.join(\n",
    "    short_df.select(\"race_id\", \"horse_id\", \"relevance\").dropDuplicates([\"race_id\", \"horse_id\"]),\n",
    "    on=[\"race_id\", \"horse_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "medium_seq = medium_seq.join(\n",
    "    medium_df.select(\"race_id\", \"horse_id\", \"relevance\").dropDuplicates([\"race_id\", \"horse_id\"]),\n",
    "    on=[\"race_id\", \"horse_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "long_seq = long_seq.join(\n",
    "    long_df.select(\"race_id\", \"horse_id\", \"relevance\").dropDuplicates([\"race_id\", \"horse_id\"]),\n",
    "    on=[\"race_id\", \"horse_id\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ecff4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def pad_sequence(seq, target_len):\n",
    "    \"\"\"\n",
    "    Pad or truncate a horse's race history sequence to a fixed length.\n",
    "    Each element of the sequence is a dictionary with keys like 'speed', 'progress', 'stride_frequency'.\n",
    "    \"\"\"\n",
    "    pad_val = {\"speed\": 0.0, \"progress\": 0.0, \"stride_frequency\": 0.0}\n",
    "    seq = seq[:target_len]\n",
    "    return seq + [pad_val] * (target_len - len(seq))\n",
    "\n",
    "def prepare_unified_lstm_dataset(short_pd, medium_pd, long_pd, target_len=150):\n",
    "    \"\"\"\n",
    "    Combine short, medium, and long sequence DataFrames into a single training set for LSTM regression.\n",
    "\n",
    "    Each row represents a horse in a race and contains:\n",
    "      - a sequence of past performance features (speed, progress, stride_frequency)\n",
    "      - a target 'relevance' score representing the outcome of that race\n",
    "\n",
    "    Returns:\n",
    "        X_all: 3D np.array [samples, time_steps, features]\n",
    "        y_all: 1D np.array [samples]\n",
    "        combined_pd: original DataFrame including padded_seq and metadata\n",
    "    \"\"\"\n",
    "    # Combine datasets and remove duplicates for same race_id + horse_id\n",
    "    combined_pd = pd.concat([short_pd, medium_pd, long_pd], ignore_index=True)\n",
    "    combined_pd = combined_pd.drop_duplicates(subset=[\"race_id\", \"horse_id\"])\n",
    "    combined_pd = combined_pd[combined_pd[\"relevance\"].notna()]\n",
    "\n",
    "    # Pad each sequence to target length\n",
    "    combined_pd[\"padded_seq\"] = combined_pd[\"sequence\"].apply(lambda x: pad_sequence(x, target_len))\n",
    "\n",
    "    # Extract the 3 feature values for each timestep\n",
    "    X_all = np.array([\n",
    "        [[d[\"speed\"], d[\"progress\"], d[\"stride_frequency\"]] for d in seq]\n",
    "        for seq in combined_pd[\"padded_seq\"]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    y_all = combined_pd[\"relevance\"].values.astype(np.float32)\n",
    "\n",
    "    return X_all, y_all, combined_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaa1401",
   "metadata": {},
   "source": [
    "### The following code:\n",
    "    1.\tCombines short, medium, long buckets\n",
    "    \n",
    "\t2.\tApplies the assign_piecewise_log_labels_spark function\n",
    "    \n",
    "\t3.\tPrepares one record per horse/race, with sequence and relevance\n",
    "    \n",
    "\t4.\tPads all sequences to the same length\n",
    "    \n",
    "\t5.\tOutputs: X_all, y_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a832515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SEQ_LEN = 150\n",
    "\n",
    "short_pd = short_seq.toPandas()\n",
    "medium_pd = medium_seq.toPandas()\n",
    "long_pd = long_seq.toPandas()\n",
    "\n",
    "X_all, y_all, full_pd = prepare_unified_lstm_dataset(short_pd, medium_pd, long_pd, target_len=TARGET_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f640df92",
   "metadata": {},
   "source": [
    "âœ… Shape Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8b959fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353595, 150, 3)\n",
      "(353595,)\n",
      "(353595, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_all.shape)   # (num_samples, sequence_length, features) = (353595, 150, 3)\n",
    "print(y_all.shape)   # (num_samples,) â€” 1 FoxSpeedScore per sample\n",
    "print(full_pd.shape) # (num_samples, 5) â€” includes race_id, horse_id, sequence, relevance, padded_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495be1c3",
   "metadata": {},
   "source": [
    "### âœ… PyTorch Dataset + DataLoader Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2152c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = y_all / y_all.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4d35eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HorseRaceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0db3bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_dataloaders(X_all, y_all, batch_size=64, val_split=0.2, shuffle=True):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_all, y_all, test_size=val_split, random_state=42\n",
    "    )\n",
    "\n",
    "    train_dataset = HorseRaceDataset(X_train, y_train)\n",
    "    val_dataset = HorseRaceDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "97e72e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, train_dataset, val_dataset = build_lstm_dataloaders(X_all, y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4759366",
   "metadata": {},
   "source": [
    "âœ… Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b1b6b2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch shape: torch.Size([64, 150, 3])\n",
      "y batch shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for batch_X, batch_y in train_loader:\n",
    "    print(\"X batch shape:\", batch_X.shape)  # torch.Size([64, 100, 3])\n",
    "    print(\"y batch shape:\", batch_y.shape)  # torch.Size([64])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "738f514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (353595, 150, 3)\n",
      "float32 (353595,)\n"
     ]
    }
   ],
   "source": [
    "print(X_all.dtype, X_all.shape)  # float32 (164445, 100, 3)\n",
    "print(y_all.dtype, y_all.shape)  # float32 (164445,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b5362a",
   "metadata": {},
   "source": [
    "# ðŸ§  Basic LSTM Model for Sequence Classification\n",
    "\n",
    "This model assumes:\n",
    "\n",
    "\tâ€¢\tInput shape: (batch_size, sequence_length=100, num_features=3)\n",
    "\tâ€¢\tOutput: binary classification (e.g. win = 1, not win = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df7dd1",
   "metadata": {},
   "source": [
    "### âœ… HorseRaceLSTM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a7d5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HorseRaceLSTM(nn.Module):\n",
    "    def __init__(self, input_size=3, hidden_size=64, num_layers=1, dropout=0.2):\n",
    "        super(HorseRaceLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a0222bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 2\n",
      "Device name 0: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"Device name 0:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d5d41b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = HorseRaceLSTM(input_size=3, hidden_size=64, num_layers=2)\n",
    "model.to(device)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b147ef",
   "metadata": {},
   "source": [
    "### ðŸ§ª Mini Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "215190e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-normalization check:\n",
      "Min: 0.0 Max: 2112.3\n",
      "Any NaNs? False\n"
     ]
    }
   ],
   "source": [
    "print(\"Post-normalization check:\")\n",
    "print(\"Min:\", X_short.min(), \"Max:\", X_short.max())\n",
    "print(\"Any NaNs?\", np.isnan(X_short).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b0c80a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device, verbose=False):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        if verbose:\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad = param.grad\n",
    "                    print(f\"{name}: mean={grad.mean().item():.6f}, std={grad.std().item():.6f}\")\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c5419c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lstm(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f988868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device, verbose=False)\n",
    "    val_loss = evaluate_lstm(model, val_loader, loss_fn, device, verbose=False)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53b4139",
   "metadata": {},
   "source": [
    "Would you like help writing an evaluation script to test how often your top 1â€“3 LSTM-ranked horses actually finished in the top 3?\n",
    "\n",
    "This is how you start to trust your model and quantify edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c26786d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top projected finishers: [(215667, 0.825968325138092), (4168, 0.8017905950546265), (417398, 0.801307201385498), (151216, 0.8009016513824463), (432162, 0.8001235127449036)]\n"
     ]
    }
   ],
   "source": [
    "projected_scores = {}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, row in short_pd.iterrows():\n",
    "        x = torch.tensor(X_short[i], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        score = model(x).item()\n",
    "        projected_scores[row[\"horse_id\"]] = score\n",
    "\n",
    "# Sort horses\n",
    "ranked = sorted(projected_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top projected finishers:\", ranked[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "30449154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def evaluate_regression(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            preds = model(X_batch)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    mae = mean_absolute_error(all_labels, all_preds)\n",
    "    r2 = r2_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"MSE:  {mse:.4f}\")\n",
    "    print(f\"MAE:  {mae:.4f}\")\n",
    "    print(f\"RÂ²:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2444deda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0674\n",
      "MAE:  0.2110\n",
      "RÂ²:   0.2808\n"
     ]
    }
   ],
   "source": [
    "evaluate_regression(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "40d6f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/models/LSTM/foxspeedscore_lstm_20250321.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dead31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HorseRaceLSTM(input_size=3, hidden_size=64, num_layers=2)\n",
    "model.load_state_dict(torch.load(\"horse_lstm_short.pt\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7e335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for horse in race_horses:\n",
    "        x = horse_sequence_tensor.unsqueeze(0).to(device)  # shape: [1, seq_len, features]\n",
    "        fox_score = model(x).item()\n",
    "        projected_scores[horse_id] = fox_score\n",
    "\n",
    "# Rank by score\n",
    "ranked_horses = sorted(projected_scores.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b738b2db",
   "metadata": {},
   "source": [
    "Would you like me to help rewire your train_one_epoch() and data prep code to support this regression setup with relevance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7aa137ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_racewise_ranking(df, score_col=\"pred_score\", label_col=\"official_fin\", top_k=3):\n",
    "    \"\"\"\n",
    "    Evaluates how often the model's top-k predicted horses actually finished in the top-k.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Must include 'race_id', 'horse_id', model predictions, and actual finish\n",
    "        score_col (str): Column name for model's predicted score\n",
    "        label_col (str): Column name for true finish (1 = winner, 2 = second, ...)\n",
    "        top_k (int): Number of top horses to evaluate (e.g., top 3 or 4)\n",
    "\n",
    "    Returns:\n",
    "        dict: Hit@K, MRR, AvgWinnerRank, Total Races\n",
    "    \"\"\"\n",
    "    hit_at_k = 0\n",
    "    reciprocal_ranks = []\n",
    "    true_ranks = []\n",
    "\n",
    "    for race_id, group in df.groupby(\"race_id\"):\n",
    "        group_sorted = group.sort_values(by=score_col, ascending=False).reset_index(drop=True)\n",
    "\n",
    "        top_k_predicted = group_sorted.head(top_k)\n",
    "        actual_top_k = group[group[label_col] <= top_k]\n",
    "\n",
    "        # Hit@K: any of predicted top-k finished top-k\n",
    "        hit = any(horse_id in actual_top_k[\"horse_id\"].values for horse_id in top_k_predicted[\"horse_id\"].values)\n",
    "        hit_at_k += int(hit)\n",
    "\n",
    "        # MRR: rank of actual winner\n",
    "        winner_row = group[group[label_col] == 1]\n",
    "        if not winner_row.empty:\n",
    "            winner_id = winner_row[\"horse_id\"].values[0]\n",
    "            winner_rank = group_sorted[group_sorted[\"horse_id\"] == winner_id].index[0] + 1\n",
    "            reciprocal_ranks.append(1.0 / winner_rank)\n",
    "            true_ranks.append(winner_rank)\n",
    "\n",
    "    num_races = df[\"race_id\"].nunique()\n",
    "    return {\n",
    "        f\"Hit@{top_k}\": round(hit_at_k / num_races, 4) if num_races else 0,\n",
    "        \"MRR\": round(np.mean(reciprocal_ranks), 4) if reciprocal_ranks else 0,\n",
    "        \"AvgWinnerRank\": round(np.mean(true_ranks), 2) if true_ranks else None,\n",
    "        \"Total Races\": num_races\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6239c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_scores(model, X_all, device, batch_size=1024):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_all), batch_size):\n",
    "            batch = X_all[i:i+batch_size]\n",
    "            batch_tensor = torch.tensor(batch, dtype=torch.float32).to(device)\n",
    "            batch_preds = model(batch_tensor).cpu().numpy()\n",
    "            preds.extend(batch_preds)\n",
    "\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4d287fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scores = predict_scores(model, X_all, device)\n",
    "race_results_df = full_pd.copy()\n",
    "race_results_df[\"pred_score\"] = pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b1d43b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Assuming you have it in the Spark side\n",
    "officials = short_df.select(\"race_id\", \"horse_id\", \"official_fin\").dropDuplicates()\n",
    "officials_pd = officials.toPandas()\n",
    "\n",
    "# Merge with full_pd\n",
    "race_results_df = full_pd.merge(officials_pd, on=[\"race_id\", \"horse_id\"], how=\"left\")\n",
    "race_results_df[\"pred_score\"] = pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "47bd268d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hit@3': 0.558, 'MRR': 0.6944, 'AvgWinnerRank': 2.05, 'Total Races': 45535}\n"
     ]
    }
   ],
   "source": [
    "ranking_metrics = evaluate_racewise_ranking(\n",
    "    race_results_df,\n",
    "    score_col=\"pred_score\",\n",
    "    label_col=\"official_fin\",  # Must be actual 1, 2, 3... positions\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(ranking_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0001d1e3",
   "metadata": {},
   "source": [
    "# âœ… Immediate Goal: Write FoxSpeedScores to foxspeedscore_model_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "75e33303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Build Pandas DF with prediction results\n",
    "output_pd = full_pd[[\"race_id\", \"horse_id\"]].copy()\n",
    "output_pd[\"score\"] = pred_scores\n",
    "output_pd[\"sequence_len\"] = full_pd[\"sequence\"].apply(len)\n",
    "output_pd[\"model_version\"] = \"lstm_v1\"\n",
    "output_pd[\"run_timestamp\"] = datetime.utcnow()\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "output_df = spark.createDataFrame(output_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "06f9a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "staging_table = \"foxspeedscore_model_output\"\n",
    "(\n",
    "        output_df.write.format(\"jdbc\")\n",
    "        .option(\"url\", jdbc_url)\n",
    "        .option(\"dbtable\", staging_table)\n",
    "        .option(\"user\", jdbc_properties[\"user\"])\n",
    "        .option(\"driver\", jdbc_properties[\"driver\"])\n",
    "        .mode(\"overwrite\")\n",
    "        .save()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749a943c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
