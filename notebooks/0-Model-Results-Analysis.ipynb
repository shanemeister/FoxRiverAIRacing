{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfcfd860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 23:06:44.143777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-12 23:06:44.152126: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-12 23:06:44.154568: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-12 23:06:44.161113: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Setup Environment\n",
    "import time\n",
    "from optuna.importance import MeanDecreaseImpurityImportanceEvaluator\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten\n",
    "import joblib # Used for encoding horse_id\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import optuna.visualization as viz\n",
    "from catboost import CatBoostRanker, CatBoostRegressor, CatBoostClassifier, Pool\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import (col, count, row_number, abs, unix_timestamp, mean, \n",
    "                                   when, lit, min as F_min, max as F_max , upper, trim,\n",
    "                                   row_number, mean as F_mean, countDistinct, last, first, when)\n",
    "from src.data_preprocessing.data_prep1.data_utils import initialize_environment \n",
    "# Set global references to None\n",
    "spark = None\n",
    "master_results_df = None\n",
    "race_df = None\n",
    "df = None\n",
    "training_data = None\n",
    "train_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d6e9c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "spark, jdbc_url, jdbc_properties, parquet_dir, log_file = initialize_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "537064b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/train_df\")\n",
    "global_speed_score = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/global_speed_score.parquet\")\n",
    "horse_embedding = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/horse_embedding_data-20250312_1948.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "463a66e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load historical and future data from Spark.\n",
    "historical_df_spark = train_df.filter(F.col(\"data_flag\") == \"historical\")\n",
    "future_df = train_df.filter(F.col(\"data_flag\") == \"future\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99906d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383609"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd38e65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "806"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d30917b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_df_spark = (\n",
    "    historical_df_spark\n",
    "    .withColumn(\"race_date_str\", F.date_format(\"race_date\", \"yyyy-MM-dd\"))\n",
    "    .withColumn(\n",
    "        \"group_id\",\n",
    "        F.concat(\n",
    "            F.col(\"course_cd\"),\n",
    "            F.lit(\"_\"),\n",
    "            F.col(\"race_date_str\"),\n",
    "            F.lit(\"_\"),\n",
    "            F.col(\"race_number\").cast(\"string\")\n",
    "            )\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25f2dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_sizes_df = (\n",
    "    historical_df_spark.groupBy(\"group_id\")\n",
    "      .agg(F.count(\"*\").alias(\"num_horses\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef5fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676617c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d081c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d63b244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 75:=>                                                    (3 + 125) / 128]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+\n",
      "|num_horses|num_races|pct_of_races|\n",
      "+----------+---------+------------+\n",
      "|1         |13586    |16.13       |\n",
      "|2         |11317    |13.43       |\n",
      "|3         |9798     |11.63       |\n",
      "|4         |9020     |10.71       |\n",
      "|5         |9385     |11.14       |\n",
      "|6         |9836     |11.68       |\n",
      "|7         |8089     |9.6         |\n",
      "|8         |5699     |6.77        |\n",
      "|9         |3651     |4.33        |\n",
      "|10        |2229     |2.65        |\n",
      "|11        |1016     |1.21        |\n",
      "|12        |553      |0.66        |\n",
      "|13        |37       |0.04        |\n",
      "|14        |19       |0.02        |\n",
      "|16        |1        |0.0         |\n",
      "|17        |1        |0.0         |\n",
      "|18        |1        |0.0         |\n",
      "+----------+---------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 2) Group by the computed \"num_horses\" to find how many races have that count\n",
    "race_size_distribution_df = (\n",
    "    race_sizes_df\n",
    "    .groupBy(\"num_horses\")\n",
    "    .count()  # how many races have that particular horse count\n",
    "    .withColumnRenamed(\"count\", \"num_races\")\n",
    ")\n",
    "\n",
    "# 3) Optionally compute total number of races, then compute percentages\n",
    "total_races = race_size_distribution_df.agg(F.sum(\"num_races\")).collect()[0][0]\n",
    "\n",
    "race_size_distribution_df = (\n",
    "    race_size_distribution_df\n",
    "    .withColumn(\"pct_of_races\", F.round((F.col(\"num_races\") / F.lit(total_races)) * 100, 2))\n",
    "    .orderBy(\"num_horses\")\n",
    ")\n",
    "\n",
    "race_size_distribution_df.show(200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1080c7da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104bcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed637fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eccf3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1603b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc14f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47723aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd145387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f71789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b2919e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba98ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "for col in future_df.columns:\n",
    "    null_count = future_df.filter(F.col(col).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(f\"{col}: {null_count} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7744657",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in historical_df_spark.columns:\n",
    "    null_count = historical_df_spark.filter(F.col(col).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(f\"{col}: {null_count} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cec8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_speed_score.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910580a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate historical and future data\n",
    "historical_df = global_speed_score.filter(F.col(\"data_flag\") == \"historical\")\n",
    "future_df = global_speed_score.filter(F.col(\"data_flag\") == \"future\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320883db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType, DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Get all numeric columns (float and double)\n",
    "numeric_cols = [field.name for field in global_speed_score.schema.fields \n",
    "                if field.dataType.typeName() in ['double', 'float']]\n",
    "\n",
    "cols_with_issues = []\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    # Count NaN values\n",
    "    nan_count = global_speed_score.filter(F.isnan(F.col(col_name))).count()\n",
    "    # Count positive infinity values\n",
    "    pos_inf_count = global_speed_score.filter(F.col(col_name) == float(\"inf\")).count()\n",
    "    # Count negative infinity values\n",
    "    neg_inf_count = global_speed_score.filter(F.col(col_name) == float(\"-inf\")).count()\n",
    "    total_issues = nan_count + pos_inf_count + neg_inf_count\n",
    "    if total_issues > 0:\n",
    "        cols_with_issues.append({\n",
    "            \"column\": col_name,\n",
    "            \"total_issues\": total_issues,\n",
    "            \"nan_count\": nan_count,\n",
    "            \"pos_inf_count\": pos_inf_count,\n",
    "            \"neg_inf_count\": neg_inf_count\n",
    "        })\n",
    "\n",
    "print(\"Numeric columns with NaN or Infinity values:\")\n",
    "for info in cols_with_issues:\n",
    "    print(f\"{info['column']}: total issues={info['total_issues']} (NaN: {info['nan_count']}, +Inf: {info['pos_inf_count']}, -Inf: {info['neg_inf_count']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0266374",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_hist = global_speed_score.filter(F.col(\"data_flag\") == \"historical\").count()\n",
    "count_fut = global_speed_score.filter(F.col(\"data_flag\") == \"future\").count()\n",
    "count_total = global_speed_score.count()\n",
    "\n",
    "print(f\"Final DF total count: {count_total}\")\n",
    "print(f\"Final DF count for historical: {count_hist}\")\n",
    "print(f\"Final DF count for future: {count_fut}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic descriptive stats for global_speed_score\n",
    "global_speed_score_stats = global_speed_score.select(\"global_speed_score_iq\").describe()\n",
    "global_speed_score_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48424b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_speed_score_quantiles = global_speed_score.select(\"global_speed_score_iq\").summary()\n",
    "global_speed_score_quantiles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ca295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show count, mean, stddev, min, 25%, 50%, 75%, and max\n",
    "global_speed_score_quantiles = global_speed_score.select(\"global_speed_score_iq\").summary()\n",
    "global_speed_score_quantiles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089d96bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_bins = (\n",
    "    global_speed_score.withColumn(\"score_bin\", F.floor(F.col(\"global_speed_score_iq\") / 10) * 10)\n",
    "      .groupBy(\"score_bin\")\n",
    "      .count()\n",
    "      .orderBy(\"score_bin\")\n",
    ")\n",
    "\n",
    "df_bins.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d102294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for standardized_score with bin size = 0.5\n",
    "df_bins_std = (\n",
    "    global_speed_score.withColumn(\"score_bin\", F.floor(F.col(\"global_speed_score_iq\") / 0.5) * 0.5)\n",
    "      .groupBy(\"score_bin\")\n",
    "      .count()\n",
    "      .orderBy(\"score_bin\")\n",
    ")\n",
    "df_bins_std.show(200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f54b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the column to an RDD and use the histogram function\n",
    "hist_result = (\n",
    "    global_speed_score.select(\"global_speed_score_iq\")\n",
    "      .rdd\n",
    "      .flatMap(lambda x: x)  # flatten out the column values\n",
    "      .histogram(10)         # 10 bins by default\n",
    ")\n",
    "\n",
    "# hist_result returns a tuple (bins, counts)\n",
    "# bins: list of bin boundaries\n",
    "# counts: list of counts in each bin\n",
    "\n",
    "bins = hist_result[0]\n",
    "counts = hist_result[1]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x=bins[:-1], height=counts, width=(bins[1] - bins[0]) * 0.9)\n",
    "plt.xlabel(\"global_speed_score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of global_speed_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e60a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the column to an RDD and use the histogram function\n",
    "hist_result = (\n",
    "    global_speed_score.select(\"global_speed_score_iq\")\n",
    "      .rdd\n",
    "      .flatMap(lambda x: x)  # flatten out the column values\n",
    "      .histogram(10)         # 10 bins by default\n",
    ")\n",
    "\n",
    "# hist_result returns a tuple (bins, counts)\n",
    "# bins: list of bin boundaries\n",
    "# counts: list of counts in each bin\n",
    "\n",
    "bins = hist_result[0]\n",
    "counts = hist_result[1]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x=bins[:-1], height=counts, width=(bins[1] - bins[0]) * 0.9)\n",
    "plt.xlabel(\"global_speed_score_iq\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of global_speed_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf29a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = global_speed_score.select(\"global_speed_score_iq\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(pdf[\"global_speed_score_iq\"], bins=50, edgecolor='black')\n",
    "plt.xlabel(\"global_speed_score_iq\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of global_speed_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88697d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# If the DataFrame is huge, take a sample to avoid OOM on driver\n",
    "pdf = (global_speed_score\n",
    "       .select(\"global_speed_score_iq\")\n",
    "       .dropna()\n",
    "       .sample(withReplacement=False, fraction=0.01, seed=42)  # e.g., 1% sample\n",
    "       .toPandas()\n",
    "      )\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(pdf[\"global_speed_score_iq\"], bins=350, edgecolor='black')\n",
    "plt.title(\"Distribution of global_speed_score_iq (sampled)\")\n",
    "plt.xlabel(\"global_speed_score_iq\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3a8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "high_scores_df = (\n",
    "    global_speed_score\n",
    "    .select(\"horse_id\", \"horse_name\", \"global_speed_score_iq\")\n",
    "    .filter(\n",
    "        (F.col(\"global_speed_score_iq\") >= 110) &\n",
    "        (F.col(\"global_speed_score_iq\") <= 140)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show them\n",
    "high_scores_df.show(truncate=False)\n",
    "\n",
    "# Or collect to the driver if you want to iterate in Python\n",
    "results = high_scores_df.collect()\n",
    "for row in results:\n",
    "    print(row[\"horse_id\"], row[\"horse_name\"], row[\"global_speed_score_iq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7ff02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by what should be your PK columns\n",
    "dup_check_df = (\n",
    "    global_speed_score.groupBy(\"course_cd\", \"race_date\", \"race_number\", \"horse_id\")\n",
    "      .count()\n",
    "      .filter(\"count > 1\")  # means there's more than one row for that key\n",
    ")\n",
    "\n",
    "dup_check_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e6454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c045a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca5dc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ab951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0a025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f160d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a0fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b0ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac99da36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a89f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bfc186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b683cf44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3b77a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f05b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24652160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of races\n",
    "# Count the number of unique races\n",
    "race_count = (\n",
    "    global_speed_score\n",
    "    .select(\"course_cd\", \"race_date\", \"race_number\")\n",
    "    .distinct()  # Get unique race combinations\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# Print the number of unique races\n",
    "print(f\"Total number of races: {race_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd9bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Check if the column exists, and drop it if it does\n",
    "if \"horse_count\" in global_speed_score.columns:\n",
    "    global_speed_score = global_speed_score.drop(\"horse_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c67785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of horses in each race\n",
    "race_horse_counts = (\n",
    "    global_speed_score\n",
    "    .groupBy(\"course_cd\", \"race_date\", \"race_number\")\n",
    "    .agg(F.count(\"saddle_cloth_number\").alias(\"horse_count\"))  # Count horses per race\n",
    ")\n",
    "\n",
    "# Join the count back to the main DataFrame\n",
    "global_speed_score = global_speed_score.join(race_horse_counts, on=[\"course_cd\", \"race_date\", \"race_number\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdab2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the race to filter (modify these values as needed)\n",
    "selected_course = \"TOP\"\n",
    "selected_date = \"2025-03-01\"  # Adjust as needed\n",
    "selected_race = 2  # Adjust as needed\n",
    "\n",
    "# Filter and sort the DataFrame\n",
    "filtered_race = (\n",
    "    global_speed_score\n",
    "    .filter(\n",
    "        (F.col(\"course_cd\") == selected_course) &\n",
    "        (F.col(\"race_date\") == selected_date) &\n",
    "        (F.col(\"race_number\") == selected_race)\n",
    "    )\n",
    "    .select(\n",
    "        \"course_cd\", \n",
    "        \"race_date\", \n",
    "        \"race_number\", \n",
    "        \"horse_name\", \n",
    "        \"saddle_cloth_number\", \n",
    "        \"horse_count\"\n",
    "    )\n",
    "    .orderBy(F.col(\"saddle_cloth_number\").asc())  # Sort by saddle_cloth_number\n",
    ")\n",
    "\n",
    "# Show the filtered race details\n",
    "filtered_race.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c6e5ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame for the specific race.\n",
    "race_df = speed_score.filter(\n",
    "    (F.col(\"course_cd\") == \"TGP\") &\n",
    "    (F.col(\"race_date\") == F.lit(\"2025-02-23\").cast(\"date\")) &\n",
    "    (F.col(\"race_number\") == 2)\n",
    ")\n",
    "\n",
    "# Select and order the columns of interest.\n",
    "race_df.select(\"saddle_cloth_number\", \"horse_name\", \"course_cd\", \"race_date\", \"race_number\") \\\n",
    "       .orderBy(\"saddle_cloth_number\") \\\n",
    "       .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d92b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_embedding.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc112c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Define the list of TPD tracks you want to filter for\n",
    "tpd_tracks = [\n",
    "    'CNL', 'SAR', 'PIM', 'TSA', 'BEL', 'MVR', 'TWO', 'CLS', 'KEE', 'TAM', 'TTP', 'TKD', \n",
    "    'ELP', 'PEN', 'HOU', 'DMR', 'TLS', 'AQU', 'MTH', 'TGP', 'TGG', 'CBY', 'LRL', \n",
    "    'TED', 'IND', 'CTD', 'ASD', 'TCD', 'LAD', 'TOP'\n",
    "]\n",
    "\n",
    "# Convert saddle_cloth_number to Integer (if stored as string)\n",
    "horse_embedding = horse_embedding.withColumn(\n",
    "    \"saddle_cloth_number\", F.col(\"saddle_cloth_number\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# Define window partitioned by race and ordered by saddle_cloth_number\n",
    "race_window = Window.partitionBy(\"course_cd\", \"race_date\", \"race_number\").orderBy(\"saddle_cloth_number\")\n",
    "\n",
    "# Add a column for expected saddle_cloth_number (incremental index starting at 1)\n",
    "horse_embedding = horse_embedding.withColumn(\n",
    "    \"expected_number\",\n",
    "    F.row_number().over(race_window)\n",
    ")\n",
    "\n",
    "# Identify races where saddle_cloth_number != expected_number\n",
    "races_with_gaps = (\n",
    "    horse_embedding\n",
    "    .filter(F.col(\"saddle_cloth_number\") != F.col(\"expected_number\"))\n",
    "    .filter(F.col(\"course_cd\").isin(tpd_tracks))  # <-- Track filter applied here\n",
    "    .select(\"course_cd\", \"race_date\", \"race_number\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# Show races with non-contiguous saddle cloth numbers from the specified tracks\n",
    "races_with_gaps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c48c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Define the specific track(s) you want to filter for\n",
    "selected_tracks = ['TOP']  # Change this to filter for different tracks\n",
    "\n",
    "# Convert saddle_cloth_number to Integer (if stored as string)\n",
    "horse_embedding = horse_embedding.withColumn(\n",
    "    \"saddle_cloth_number\", F.col(\"saddle_cloth_number\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# Define window partitioned by race and ordered by saddle_cloth_number\n",
    "race_window = Window.partitionBy(\"course_cd\", \"race_date\", \"race_number\").orderBy(\"saddle_cloth_number\")\n",
    "\n",
    "# Add a column for expected saddle_cloth_number (incremental index starting at 1)\n",
    "horse_embedding = horse_embedding.withColumn(\n",
    "    \"expected_number\",\n",
    "    F.row_number().over(race_window)\n",
    ")\n",
    "\n",
    "# Identify races where saddle_cloth_number != expected_number\n",
    "races_with_gaps = (\n",
    "    horse_embedding\n",
    "    .filter(F.col(\"saddle_cloth_number\") != F.col(\"expected_number\"))\n",
    "    .filter(F.col(\"course_cd\").isin(selected_tracks))  # <-- Filter for specific track(s)\n",
    "    .select(\"course_cd\", \"race_date\", \"race_number\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# Show races with non-contiguous saddle cloth numbers for the selected track(s)\n",
    "races_with_gaps.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c331788",
   "metadata": {},
   "source": [
    "Approach\n",
    "\n",
    ">1.\tFind races where saddle cloth numbers are non-contiguous (already identified in races_with_gaps).\n",
    "\n",
    ">2.\tUse that list to filter the main dataset (horse_embedding) and retrieve the horses that are present in those races."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Convert saddle_cloth_number to Integer (if stored as string)\n",
    "horse_embedding = horse_embedding.withColumn(\n",
    "    \"saddle_cloth_number\", F.col(\"saddle_cloth_number\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# Define window partitioned by race and ordered by saddle_cloth_number\n",
    "race_window = Window.partitionBy(\"course_cd\", \"race_date\", \"race_number\").orderBy(\"saddle_cloth_number\")\n",
    "\n",
    "# Add a column for expected saddle_cloth_number (incremental index starting at 1)\n",
    "horse_embedding = horse_embedding.withColumn(\n",
    "    \"expected_number\",\n",
    "    F.row_number().over(race_window)\n",
    ")\n",
    "\n",
    "# Identify races where saddle_cloth_number != expected_number\n",
    "races_with_gaps = (\n",
    "    horse_embedding\n",
    "    .filter(F.col(\"saddle_cloth_number\") != F.col(\"expected_number\"))\n",
    "    .select(\"course_cd\", \"race_date\", \"race_number\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# Now, retrieve all horses in these races so we can see what saddle cloth numbers are present\n",
    "horses_in_missing_races = (\n",
    "    horse_embedding\n",
    "    .join(races_with_gaps, [\"course_cd\", \"race_date\", \"race_number\"])  # Join to get only races with gaps\n",
    "    .select(\"course_cd\", \"race_date\", \"race_number\", \"horse_name\", \"saddle_cloth_number\")\n",
    "    .orderBy(\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\")  # Order for readability\n",
    ")\n",
    "\n",
    "# Show results\n",
    "horses_in_missing_races.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe16951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for the specific race.\n",
    "race_df = horse_enhanced.filter(\n",
    "    (F.col(\"course_cd\") == \"AQU\") &\n",
    "    (F.col(\"race_date\") == F.lit(\"2025-02-23\").cast(\"date\")) &\n",
    "    (F.col(\"race_number\") == 10)\n",
    ")\n",
    "\n",
    "# Select and order the columns of interest.\n",
    "race_df.select(\"saddle_cloth_number\", \"horse_name\", \"course_cd\", \"race_date\", \"race_number\") \\\n",
    "       .orderBy(\"saddle_cloth_number\") \\\n",
    "       .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d61d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
