{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a43fc8b",
   "metadata": {},
   "source": [
    "# XGBoost Model Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1463424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# Setup Environment\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn import set_config\n",
    "from src.data_preprocessing.data_prep1.data_loader import load_data_from_postgresql\n",
    "from src.data_preprocessing.data_prep1.sql_queries import sql_queries\n",
    "import pyspark.sql.functions as F\n",
    "import xgboost as xgb\n",
    "from sklearn import set_config\n",
    "from pyspark.sql.functions import (col, count, row_number, abs, unix_timestamp, mean, \n",
    "                                   when, lit, min as F_min, max as F_max , upper, trim,\n",
    "                                   row_number, mean as F_mean, countDistinct, last, first, when)\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession\n",
    "from src.data_preprocessing.data_prep1.sql_queries import sql_queries\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame, Window\n",
    "from src.data_preprocessing.data_prep1.data_utils import (save_parquet, gather_statistics, \n",
    "                initialize_environment, load_config, initialize_spark, \n",
    "                identify_and_impute_outliers, \n",
    "                identify_and_remove_outliers, identify_missing_and_outliers)\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Set global references to None\n",
    "spark = None\n",
    "master_results_df = None\n",
    "race_df = None\n",
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73671a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark, jdbc_url, jdbc_properties, parquet_dir, log_file = initialize_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c57a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset has already been cleaned up in the LGB notebook and saved as a starting point\n",
    "# It now just need to be converted to Panadas and run in the GBDT variant model (LGB, XGB, CatBoost)\n",
    "race_df = spark.read.parquet(os.path.join(parquet_dir, \"race_df_p2.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ccd5955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324041"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce2ed82",
   "metadata": {},
   "source": [
    "# Switching to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e3b985b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "race_df = race_df.toPandas()\n",
    "# Quick info about the DataFrame\n",
    "#print(df.info())\n",
    "#print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df8b562-bdbd-4ac0-8f58-38984594b4bd",
   "metadata": {},
   "source": [
    "## Set the race_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "412c7f9a-7b84-4b22-bc58-6b21556363c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_df[\"race_id\"] = (\n",
    "    race_df[\"course_cd\"].astype(str) + \"_\" +\n",
    "    race_df[\"race_date\"].astype(str) + \"_\" +\n",
    "    race_df[\"race_number\"].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6cc11f6-457e-47a5-acde-168d99c9cabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 8 9 ... 9 9 6]\n"
     ]
    }
   ],
   "source": [
    "group_array = race_df.groupby(\"race_id\").size().values  # array of group sizes\n",
    "print(group_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32bb9d4c-ea48-4388-8f93-1014ab908724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the largest official_fin is 20 (some races can have 20 horses),\n",
    "# then label = (21 - official_fin).\n",
    "# So official_fin=1 => label=20, official_fin=2 =>19, etc.\n",
    "# If your max is 14, you can do (15 - official_fin).  Just ensure \"best\" horse has largest label.\n",
    "race_df[\"rank\"] = 21 - race_df[\"official_fin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "484e5647-2ac5-4b15-94aa-e4db454bc5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"official_fin\" in race_df.columns:\n",
    "    race_df.drop(columns=[\"official_fin\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4586abd9-0d51-46b5-a2f4-2d9644011f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = [\"course_cd\", \"sex\", \"equip\", \"surface\", \"trk_cond\", \"weather\", \"med\", \n",
    "            \"race_type\", \"stk_clm_md\", \"turf_mud_mark\", \"layoff_cat\"]\n",
    "for c in cat_cols:\n",
    "    lbl = LabelEncoder()\n",
    "    race_df[c] = lbl.fit_transform(race_df[c].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1346135c-4374-4e3e-9c15-11809c23838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_df = race_df.sort_values(\"race_id\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55a5726e-5135-42e0-8142-236bf380f56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    # Basic numeric columns\n",
    "    \"purse\",\n",
    "    \"wps_pool\",\n",
    "    \"weight\",\n",
    "    \"claimprice\",\n",
    "    \"power\",\n",
    "    \"morn_odds\",\n",
    "    \"distance_meters\",\n",
    "    \"avgspd\",\n",
    "    \"class_rating\",\n",
    "    \"todays_cls\",\n",
    "    \"net_sentiment\",\n",
    "    \"avg_spd_sd\",\n",
    "    \"ave_cl_sd\",\n",
    "    \"hi_spd_sd\",\n",
    "    \"pstyerl\",\n",
    "\n",
    "    # Cumulative performance stats\n",
    "    \"all_starts\",\n",
    "    \"all_win\",\n",
    "    \"all_place\",\n",
    "    \"all_show\",\n",
    "    \"all_fourth\",\n",
    "    \"all_earnings\",\n",
    "    \"cond_starts\",\n",
    "    \"cond_win\",\n",
    "    \"cond_place\",\n",
    "    \"cond_show\",\n",
    "    \"cond_fourth\",\n",
    "    \"cond_earnings\",\n",
    "\n",
    "    # Recent form metrics\n",
    "    \"avg_fin_3\",\n",
    "    \"avg_beaten_3\",\n",
    "    \"avg_speed_3\",\n",
    "    \"avg_fin_5\",\n",
    "    \"avg_beaten_5\",\n",
    "    \"avg_speed_5\",\n",
    "    \"speed_improvement\",\n",
    "    \"days_off\",\n",
    "\n",
    "    # Sectionals / GPS\n",
    "    \"avgtime_gate1\",\n",
    "    \"avgtime_gate2\",\n",
    "    \"avgtime_gate3\",\n",
    "    \"avgtime_gate4\",\n",
    "    \"total_distance_ran\",\n",
    "    \"running_time\",\n",
    "    \"speed_q1\",\n",
    "    \"speed_q2\",\n",
    "    \"speed_q3\",\n",
    "    \"speed_q4\",\n",
    "    \"total_dist_covered\",\n",
    "    \"avg_acceleration\",\n",
    "    \"net_progress_gain\",\n",
    "    \"gps_avg_stride_length\",\n",
    "\n",
    "    # Jockey/Trainer stats\n",
    "    \"jock_win_percent\",\n",
    "    \"jock_itm_percent\",\n",
    "    \"trainer_win_percent\",\n",
    "    \"trainer_itm_percent\",\n",
    "    \"jt_win_percent\",\n",
    "    \"jt_itm_percent\",\n",
    "    \"jock_win_track\",\n",
    "    \"jock_itm_track\",\n",
    "    \"trainer_win_track\",\n",
    "    \"trainer_itm_track\",\n",
    "    \"jt_win_track\",\n",
    "    \"jt_itm_track\",\n",
    "\n",
    "    # Other\n",
    "    \"age_at_race_day\",\n",
    "    \"is_first_race\",\n",
    "]\n",
    "\n",
    "\n",
    "X_all = race_df[features].values\n",
    "y_all = race_df['rank'].values\n",
    "race_ids = race_df['race_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d78c10-ccaf-4b39-bf2c-24567137c5cb",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e288757-5fb0-477b-bcb1-e8649af4ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "unique_races = race_df['race_id'].unique()\n",
    "unique_races = shuffle(unique_races, random_state=42)\n",
    "\n",
    "train_ratio = 0.8\n",
    "cut = int(len(unique_races) * train_ratio)\n",
    "train_races = set(unique_races[:cut])\n",
    "valid_races = set(unique_races[cut:])\n",
    "\n",
    "# Create a boolean mask\n",
    "train_mask = race_df['race_id'].isin(train_races)\n",
    "valid_mask  = race_df['race_id'].isin(valid_races)\n",
    "\n",
    "# Now slice\n",
    "X_train = X_all[train_mask]\n",
    "y_train = y_all[train_mask]\n",
    "race_id_train = race_ids[train_mask]\n",
    "\n",
    "X_valid = X_all[valid_mask]\n",
    "y_valid = y_all[valid_mask]\n",
    "race_id_valid = race_ids[valid_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40e1f523-3912-4825-b082-db9efbecf519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_group_array(race_id_array):\n",
    "    \"\"\"\n",
    "    Returns an array of group sizes in the order of race_id_array’s actual row order.\n",
    "    Expects that race_id_array is sorted or lumps each race contiguously.\n",
    "    \"\"\"\n",
    "    # Approach 1: Rely on the data already being grouped in contiguous rows\n",
    "    # If your data is not guaranteed to be sorted by race_id, you can sort it first.\n",
    "    # But let's assume from the train_mask/valid_mask approach that the relative order\n",
    "    # is consistent. We can just accumulate counts.\n",
    "\n",
    "    # A simpler approach: group the data by unique race_id in the order they appear\n",
    "    # and store the size for each chunk.\n",
    "    # We'll do a loop approach:\n",
    "\n",
    "    groups = []\n",
    "    current_race = None\n",
    "    current_count = 0\n",
    "\n",
    "    group_sequence = []\n",
    "\n",
    "    for rid in race_id_array:\n",
    "        if rid != current_race:\n",
    "            # if we have an existing group, push it\n",
    "            if current_race is not None:\n",
    "                groups.append(current_count)\n",
    "            current_race = rid\n",
    "            current_count = 1\n",
    "        else:\n",
    "            current_count += 1\n",
    "    # push the last group\n",
    "    if current_race is not None and current_count > 0:\n",
    "        groups.append(current_count)\n",
    "\n",
    "    return np.array(groups, dtype=np.int32)\n",
    "\n",
    "group_train = make_group_array(race_id_train)\n",
    "group_valid  = make_group_array(race_id_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842de6d",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0aced663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# 1) Create DMatrices for train/valid with group data\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtrain.set_group(group_train)\n",
    "\n",
    "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "dvalid.set_group(group_valid)\n",
    "\n",
    "# 2) A search space (grid) for hyperparameters\n",
    "param_grid = {\n",
    "    \"eta\": [0.01, 0.1],\n",
    "    \"max_depth\": [4, 6],\n",
    "    \"min_child_weight\": [10, 30],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# 3) Base parameters for GPU-based ranking with ndcg@5\n",
    "base_params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"objective\": \"rank:pairwise\",    # or \"rank:ndcg\"\n",
    "    \"eval_metric\": [\"ndcg@5\"],       # focusing on ndcg@5\n",
    "    \"tree_method\": \"hist\",           # recommended if using GPU\n",
    "    \"device\": \"cuda\",                # specify GPU device\n",
    "    \"eta\": 0.1,                      # will be overwritten by param_grid iteration\n",
    "    \"max_depth\": 6,                  # will be overwritten\n",
    "    \"verbosity\": 1\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2a0649f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-ndcg@5:0.78572\tvalid-ndcg@5:0.78787\n",
      "[50]\ttrain-ndcg@5:0.87393\tvalid-ndcg@5:0.87438\n",
      "[100]\ttrain-ndcg@5:0.89574\tvalid-ndcg@5:0.89498\n",
      "[150]\ttrain-ndcg@5:0.90743\tvalid-ndcg@5:0.90583\n",
      "[200]\ttrain-ndcg@5:0.91485\tvalid-ndcg@5:0.91375\n",
      "[250]\ttrain-ndcg@5:0.92086\tvalid-ndcg@5:0.91887\n",
      "[300]\ttrain-ndcg@5:0.92516\tvalid-ndcg@5:0.92345\n",
      "[350]\ttrain-ndcg@5:0.92872\tvalid-ndcg@5:0.92701\n",
      "[400]\ttrain-ndcg@5:0.93205\tvalid-ndcg@5:0.93047\n",
      "[450]\ttrain-ndcg@5:0.93494\tvalid-ndcg@5:0.93305\n",
      "[500]\ttrain-ndcg@5:0.93724\tvalid-ndcg@5:0.93509\n",
      "[550]\ttrain-ndcg@5:0.93913\tvalid-ndcg@5:0.93735\n",
      "[600]\ttrain-ndcg@5:0.94105\tvalid-ndcg@5:0.93891\n",
      "[650]\ttrain-ndcg@5:0.94266\tvalid-ndcg@5:0.94080\n",
      "[700]\ttrain-ndcg@5:0.94449\tvalid-ndcg@5:0.94227\n",
      "[750]\ttrain-ndcg@5:0.94582\tvalid-ndcg@5:0.94352\n",
      "[800]\ttrain-ndcg@5:0.94708\tvalid-ndcg@5:0.94468\n",
      "[850]\ttrain-ndcg@5:0.94828\tvalid-ndcg@5:0.94590\n",
      "[900]\ttrain-ndcg@5:0.94941\tvalid-ndcg@5:0.94699\n",
      "[950]\ttrain-ndcg@5:0.95038\tvalid-ndcg@5:0.94790\n",
      "[999]\ttrain-ndcg@5:0.95135\tvalid-ndcg@5:0.94880\n",
      "[0]\ttrain-ndcg@5:0.78572\tvalid-ndcg@5:0.78787\n",
      "[50]\ttrain-ndcg@5:0.87504\tvalid-ndcg@5:0.87471\n",
      "[100]\ttrain-ndcg@5:0.89704\tvalid-ndcg@5:0.89586\n",
      "[150]\ttrain-ndcg@5:0.90850\tvalid-ndcg@5:0.90682\n",
      "[200]\ttrain-ndcg@5:0.91592\tvalid-ndcg@5:0.91406\n",
      "[250]\ttrain-ndcg@5:0.92112\tvalid-ndcg@5:0.91965\n",
      "[300]\ttrain-ndcg@5:0.92551\tvalid-ndcg@5:0.92347\n",
      "[350]\ttrain-ndcg@5:0.92896\tvalid-ndcg@5:0.92712\n",
      "[400]\ttrain-ndcg@5:0.93213\tvalid-ndcg@5:0.92984\n",
      "[450]\ttrain-ndcg@5:0.93492\tvalid-ndcg@5:0.93252\n",
      "[500]\ttrain-ndcg@5:0.93743\tvalid-ndcg@5:0.93515\n",
      "[550]\ttrain-ndcg@5:0.93941\tvalid-ndcg@5:0.93692\n",
      "[600]\ttrain-ndcg@5:0.94119\tvalid-ndcg@5:0.93871\n",
      "[650]\ttrain-ndcg@5:0.94279\tvalid-ndcg@5:0.94022\n",
      "[700]\ttrain-ndcg@5:0.94434\tvalid-ndcg@5:0.94183\n",
      "[750]\ttrain-ndcg@5:0.94583\tvalid-ndcg@5:0.94311\n",
      "[800]\ttrain-ndcg@5:0.94712\tvalid-ndcg@5:0.94429\n",
      "[850]\ttrain-ndcg@5:0.94825\tvalid-ndcg@5:0.94524\n",
      "[900]\ttrain-ndcg@5:0.94950\tvalid-ndcg@5:0.94631\n",
      "[950]\ttrain-ndcg@5:0.95056\tvalid-ndcg@5:0.94740\n",
      "[999]\ttrain-ndcg@5:0.95155\tvalid-ndcg@5:0.94827\n",
      "[0]\ttrain-ndcg@5:0.78572\tvalid-ndcg@5:0.78787\n",
      "[50]\ttrain-ndcg@5:0.87393\tvalid-ndcg@5:0.87438\n",
      "[100]\ttrain-ndcg@5:0.89574\tvalid-ndcg@5:0.89498\n",
      "[150]\ttrain-ndcg@5:0.90743\tvalid-ndcg@5:0.90583\n",
      "[200]\ttrain-ndcg@5:0.91485\tvalid-ndcg@5:0.91375\n",
      "[250]\ttrain-ndcg@5:0.92086\tvalid-ndcg@5:0.91887\n",
      "[300]\ttrain-ndcg@5:0.92516\tvalid-ndcg@5:0.92345\n",
      "[350]\ttrain-ndcg@5:0.92872\tvalid-ndcg@5:0.92701\n",
      "[400]\ttrain-ndcg@5:0.93205\tvalid-ndcg@5:0.93047\n",
      "[450]\ttrain-ndcg@5:0.93494\tvalid-ndcg@5:0.93305\n",
      "[500]\ttrain-ndcg@5:0.93722\tvalid-ndcg@5:0.93499\n",
      "[550]\ttrain-ndcg@5:0.93918\tvalid-ndcg@5:0.93716\n",
      "[600]\ttrain-ndcg@5:0.94103\tvalid-ndcg@5:0.93894\n",
      "[650]\ttrain-ndcg@5:0.94261\tvalid-ndcg@5:0.94067\n",
      "[700]\ttrain-ndcg@5:0.94430\tvalid-ndcg@5:0.94227\n",
      "[750]\ttrain-ndcg@5:0.94578\tvalid-ndcg@5:0.94336\n",
      "[800]\ttrain-ndcg@5:0.94705\tvalid-ndcg@5:0.94452\n",
      "[850]\ttrain-ndcg@5:0.94821\tvalid-ndcg@5:0.94573\n",
      "[900]\ttrain-ndcg@5:0.94937\tvalid-ndcg@5:0.94686\n",
      "[950]\ttrain-ndcg@5:0.95045\tvalid-ndcg@5:0.94773\n",
      "[999]\ttrain-ndcg@5:0.95125\tvalid-ndcg@5:0.94862\n",
      "[0]\ttrain-ndcg@5:0.78572\tvalid-ndcg@5:0.78787\n",
      "[50]\ttrain-ndcg@5:0.87504\tvalid-ndcg@5:0.87471\n",
      "[100]\ttrain-ndcg@5:0.89704\tvalid-ndcg@5:0.89586\n",
      "[150]\ttrain-ndcg@5:0.90850\tvalid-ndcg@5:0.90682\n",
      "[200]\ttrain-ndcg@5:0.91592\tvalid-ndcg@5:0.91406\n",
      "[250]\ttrain-ndcg@5:0.92112\tvalid-ndcg@5:0.91965\n",
      "[300]\ttrain-ndcg@5:0.92551\tvalid-ndcg@5:0.92347\n",
      "[350]\ttrain-ndcg@5:0.92896\tvalid-ndcg@5:0.92712\n",
      "[400]\ttrain-ndcg@5:0.93213\tvalid-ndcg@5:0.92984\n",
      "[450]\ttrain-ndcg@5:0.93492\tvalid-ndcg@5:0.93252\n",
      "[500]\ttrain-ndcg@5:0.93743\tvalid-ndcg@5:0.93512\n",
      "[550]\ttrain-ndcg@5:0.93938\tvalid-ndcg@5:0.93704\n",
      "[600]\ttrain-ndcg@5:0.94129\tvalid-ndcg@5:0.93870\n",
      "[650]\ttrain-ndcg@5:0.94290\tvalid-ndcg@5:0.94039\n",
      "[700]\ttrain-ndcg@5:0.94441\tvalid-ndcg@5:0.94180\n",
      "[750]\ttrain-ndcg@5:0.94580\tvalid-ndcg@5:0.94321\n",
      "[800]\ttrain-ndcg@5:0.94716\tvalid-ndcg@5:0.94450\n",
      "[850]\ttrain-ndcg@5:0.94849\tvalid-ndcg@5:0.94555\n",
      "[900]\ttrain-ndcg@5:0.94968\tvalid-ndcg@5:0.94651\n",
      "[950]\ttrain-ndcg@5:0.95070\tvalid-ndcg@5:0.94746\n",
      "[999]\ttrain-ndcg@5:0.95172\tvalid-ndcg@5:0.94828\n",
      "[0]\ttrain-ndcg@5:0.81375\tvalid-ndcg@5:0.81576\n",
      "[50]\ttrain-ndcg@5:0.88165\tvalid-ndcg@5:0.87913\n",
      "[100]\ttrain-ndcg@5:0.90315\tvalid-ndcg@5:0.89988\n",
      "[150]\ttrain-ndcg@5:0.91596\tvalid-ndcg@5:0.91186\n",
      "[200]\ttrain-ndcg@5:0.92482\tvalid-ndcg@5:0.91930\n",
      "[250]\ttrain-ndcg@5:0.93126\tvalid-ndcg@5:0.92540\n",
      "[300]\ttrain-ndcg@5:0.93604\tvalid-ndcg@5:0.93049\n",
      "[350]\ttrain-ndcg@5:0.93996\tvalid-ndcg@5:0.93421\n",
      "[400]\ttrain-ndcg@5:0.94326\tvalid-ndcg@5:0.93676\n",
      "[450]\ttrain-ndcg@5:0.94609\tvalid-ndcg@5:0.93936\n",
      "[500]\ttrain-ndcg@5:0.94845\tvalid-ndcg@5:0.94112\n",
      "[550]\ttrain-ndcg@5:0.95050\tvalid-ndcg@5:0.94300\n",
      "[600]\ttrain-ndcg@5:0.95220\tvalid-ndcg@5:0.94421\n",
      "[0]\ttrain-ndcg@5:0.81426\tvalid-ndcg@5:0.81611\n",
      "[50]\ttrain-ndcg@5:0.88233\tvalid-ndcg@5:0.87912\n",
      "[100]\ttrain-ndcg@5:0.90416\tvalid-ndcg@5:0.89969\n",
      "[150]\ttrain-ndcg@5:0.91716\tvalid-ndcg@5:0.91157\n",
      "[200]\ttrain-ndcg@5:0.92573\tvalid-ndcg@5:0.92010\n",
      "[250]\ttrain-ndcg@5:0.93202\tvalid-ndcg@5:0.92589\n",
      "[300]\ttrain-ndcg@5:0.93641\tvalid-ndcg@5:0.93024\n",
      "[350]\ttrain-ndcg@5:0.94060\tvalid-ndcg@5:0.93383\n",
      "[400]\ttrain-ndcg@5:0.94400\tvalid-ndcg@5:0.93676\n",
      "[450]\ttrain-ndcg@5:0.94669\tvalid-ndcg@5:0.93936\n",
      "[500]\ttrain-ndcg@5:0.94895\tvalid-ndcg@5:0.94159\n",
      "[550]\ttrain-ndcg@5:0.95086\tvalid-ndcg@5:0.94314\n",
      "[600]\ttrain-ndcg@5:0.95278\tvalid-ndcg@5:0.94486\n",
      "[650]\ttrain-ndcg@5:0.95431\tvalid-ndcg@5:0.94601\n",
      "[700]\ttrain-ndcg@5:0.95593\tvalid-ndcg@5:0.94720\n",
      "[750]\ttrain-ndcg@5:0.95750\tvalid-ndcg@5:0.94838\n",
      "[800]\ttrain-ndcg@5:0.95878\tvalid-ndcg@5:0.94968\n",
      "[850]\ttrain-ndcg@5:0.95989\tvalid-ndcg@5:0.95056\n",
      "[900]\ttrain-ndcg@5:0.96095\tvalid-ndcg@5:0.95141\n",
      "[915]\ttrain-ndcg@5:0.96127\tvalid-ndcg@5:0.95164\n",
      "[0]\ttrain-ndcg@5:0.81380\tvalid-ndcg@5:0.81583\n",
      "[50]\ttrain-ndcg@5:0.88199\tvalid-ndcg@5:0.87889\n",
      "[100]\ttrain-ndcg@5:0.90359\tvalid-ndcg@5:0.89977\n",
      "[150]\ttrain-ndcg@5:0.91660\tvalid-ndcg@5:0.91161\n",
      "[200]\ttrain-ndcg@5:0.92526\tvalid-ndcg@5:0.91984\n",
      "[250]\ttrain-ndcg@5:0.93142\tvalid-ndcg@5:0.92554\n",
      "[300]\ttrain-ndcg@5:0.93625\tvalid-ndcg@5:0.93027\n",
      "[350]\ttrain-ndcg@5:0.94007\tvalid-ndcg@5:0.93401\n",
      "[400]\ttrain-ndcg@5:0.94354\tvalid-ndcg@5:0.93697\n",
      "[450]\ttrain-ndcg@5:0.94630\tvalid-ndcg@5:0.93935\n",
      "[500]\ttrain-ndcg@5:0.94869\tvalid-ndcg@5:0.94135\n",
      "[550]\ttrain-ndcg@5:0.95072\tvalid-ndcg@5:0.94311\n",
      "[600]\ttrain-ndcg@5:0.95268\tvalid-ndcg@5:0.94469\n",
      "[650]\ttrain-ndcg@5:0.95433\tvalid-ndcg@5:0.94635\n",
      "[700]\ttrain-ndcg@5:0.95580\tvalid-ndcg@5:0.94760\n",
      "[750]\ttrain-ndcg@5:0.95733\tvalid-ndcg@5:0.94873\n",
      "[800]\ttrain-ndcg@5:0.95846\tvalid-ndcg@5:0.94980\n",
      "[839]\ttrain-ndcg@5:0.95938\tvalid-ndcg@5:0.95069\n",
      "[0]\ttrain-ndcg@5:0.81427\tvalid-ndcg@5:0.81616\n",
      "[50]\ttrain-ndcg@5:0.88213\tvalid-ndcg@5:0.87932\n",
      "[100]\ttrain-ndcg@5:0.90455\tvalid-ndcg@5:0.89981\n",
      "[150]\ttrain-ndcg@5:0.91727\tvalid-ndcg@5:0.91135\n",
      "[200]\ttrain-ndcg@5:0.92573\tvalid-ndcg@5:0.92032\n",
      "[250]\ttrain-ndcg@5:0.93215\tvalid-ndcg@5:0.92617\n",
      "[300]\ttrain-ndcg@5:0.93695\tvalid-ndcg@5:0.93062\n",
      "[350]\ttrain-ndcg@5:0.94067\tvalid-ndcg@5:0.93409\n",
      "[400]\ttrain-ndcg@5:0.94393\tvalid-ndcg@5:0.93692\n",
      "[450]\ttrain-ndcg@5:0.94668\tvalid-ndcg@5:0.93889\n",
      "[500]\ttrain-ndcg@5:0.94893\tvalid-ndcg@5:0.94087\n",
      "[550]\ttrain-ndcg@5:0.95088\tvalid-ndcg@5:0.94265\n",
      "[600]\ttrain-ndcg@5:0.95266\tvalid-ndcg@5:0.94407\n",
      "[650]\ttrain-ndcg@5:0.95432\tvalid-ndcg@5:0.94554\n",
      "[700]\ttrain-ndcg@5:0.95589\tvalid-ndcg@5:0.94665\n",
      "[750]\ttrain-ndcg@5:0.95732\tvalid-ndcg@5:0.94827\n",
      "[800]\ttrain-ndcg@5:0.95869\tvalid-ndcg@5:0.94918\n",
      "[850]\ttrain-ndcg@5:0.95989\tvalid-ndcg@5:0.95028\n",
      "[900]\ttrain-ndcg@5:0.96098\tvalid-ndcg@5:0.95123\n",
      "[950]\ttrain-ndcg@5:0.96201\tvalid-ndcg@5:0.95202\n",
      "[954]\ttrain-ndcg@5:0.96208\tvalid-ndcg@5:0.95202\n",
      "[0]\ttrain-ndcg@5:0.78572\tvalid-ndcg@5:0.78787\n",
      "[50]\ttrain-ndcg@5:0.93037\tvalid-ndcg@5:0.92931\n",
      "[100]\ttrain-ndcg@5:0.94658\tvalid-ndcg@5:0.94413\n",
      "[150]\ttrain-ndcg@5:0.95469\tvalid-ndcg@5:0.95235\n",
      "[200]\ttrain-ndcg@5:0.95990\tvalid-ndcg@5:0.95704\n",
      "[250]\ttrain-ndcg@5:0.96395\tvalid-ndcg@5:0.96039\n",
      "[300]\ttrain-ndcg@5:0.96678\tvalid-ndcg@5:0.96303\n",
      "[350]\ttrain-ndcg@5:0.96915\tvalid-ndcg@5:0.96548\n",
      "[400]\ttrain-ndcg@5:0.97104\tvalid-ndcg@5:0.96712\n",
      "[450]\ttrain-ndcg@5:0.97279\tvalid-ndcg@5:0.96842\n",
      "[500]\ttrain-ndcg@5:0.97426\tvalid-ndcg@5:0.96995\n",
      "[505]\ttrain-ndcg@5:0.97439\tvalid-ndcg@5:0.96992\n",
      "[0]\ttrain-ndcg@5:0.78572\tvalid-ndcg@5:0.78787\n",
      "[50]\ttrain-ndcg@5:0.93035\tvalid-ndcg@5:0.92816\n",
      "[100]\ttrain-ndcg@5:0.94671\tvalid-ndcg@5:0.94457\n",
      "[150]\ttrain-ndcg@5:0.95507\tvalid-ndcg@5:0.95212\n",
      "[200]\ttrain-ndcg@5:0.96023\tvalid-ndcg@5:0.95752\n",
      "[250]\ttrain-ndcg@5:0.96425\tvalid-ndcg@5:0.96077\n",
      "[300]\ttrain-ndcg@5:0.96725\tvalid-ndcg@5:0.96330\n",
      "[350]\ttrain-ndcg@5:0.96969\tvalid-ndcg@5:0.96548\n",
      "[400]\ttrain-ndcg@5:0.97170\tvalid-ndcg@5:0.96705\n",
      "[450]\ttrain-ndcg@5:0.97346\tvalid-ndcg@5:0.96858\n",
      "[500]\ttrain-ndcg@5:0.97486\tvalid-ndcg@5:0.96989\n",
      "[509]\ttrain-ndcg@5:0.97515\tvalid-ndcg@5:0.96985\n",
      "[0]\ttrain-ndcg@5:0.78572\tvalid-ndcg@5:0.78787\n",
      "[50]\ttrain-ndcg@5:0.93054\tvalid-ndcg@5:0.92928\n",
      "[100]\ttrain-ndcg@5:0.94617\tvalid-ndcg@5:0.94469\n",
      "[150]\ttrain-ndcg@5:0.95485\tvalid-ndcg@5:0.95216\n",
      "[200]\ttrain-ndcg@5:0.95972\tvalid-ndcg@5:0.95657\n",
      "[250]\ttrain-ndcg@5:0.96390\tvalid-ndcg@5:0.96047\n",
      "[300]\ttrain-ndcg@5:0.96677\tvalid-ndcg@5:0.96325\n",
      "[350]\ttrain-ndcg@5:0.96920\tvalid-ndcg@5:0.96545\n",
      "[400]\ttrain-ndcg@5:0.97121\tvalid-ndcg@5:0.96704\n",
      "[414]\ttrain-ndcg@5:0.97178\tvalid-ndcg@5:0.96724\n",
      "[0]\ttrain-ndcg@5:0.78572\tvalid-ndcg@5:0.78787\n",
      "[50]\ttrain-ndcg@5:0.93035\tvalid-ndcg@5:0.92816\n",
      "[100]\ttrain-ndcg@5:0.94636\tvalid-ndcg@5:0.94391\n",
      "[150]\ttrain-ndcg@5:0.95483\tvalid-ndcg@5:0.95142\n",
      "[200]\ttrain-ndcg@5:0.96044\tvalid-ndcg@5:0.95670\n",
      "[250]\ttrain-ndcg@5:0.96441\tvalid-ndcg@5:0.96039\n",
      "[300]\ttrain-ndcg@5:0.96728\tvalid-ndcg@5:0.96307\n",
      "[350]\ttrain-ndcg@5:0.96975\tvalid-ndcg@5:0.96513\n",
      "[400]\ttrain-ndcg@5:0.97167\tvalid-ndcg@5:0.96687\n",
      "[447]\ttrain-ndcg@5:0.97331\tvalid-ndcg@5:0.96789\n",
      "[0]\ttrain-ndcg@5:0.81375\tvalid-ndcg@5:0.81576\n",
      "[50]\ttrain-ndcg@5:0.94083\tvalid-ndcg@5:0.93448\n",
      "[100]\ttrain-ndcg@5:0.95653\tvalid-ndcg@5:0.94845\n",
      "[150]\ttrain-ndcg@5:0.96466\tvalid-ndcg@5:0.95513\n",
      "[200]\ttrain-ndcg@5:0.97015\tvalid-ndcg@5:0.95938\n",
      "[250]\ttrain-ndcg@5:0.97393\tvalid-ndcg@5:0.96220\n",
      "[300]\ttrain-ndcg@5:0.97698\tvalid-ndcg@5:0.96475\n",
      "[350]\ttrain-ndcg@5:0.97908\tvalid-ndcg@5:0.96617\n",
      "[400]\ttrain-ndcg@5:0.98097\tvalid-ndcg@5:0.96758\n",
      "[450]\ttrain-ndcg@5:0.98252\tvalid-ndcg@5:0.96849\n",
      "[454]\ttrain-ndcg@5:0.98256\tvalid-ndcg@5:0.96848\n",
      "[0]\ttrain-ndcg@5:0.81426\tvalid-ndcg@5:0.81611\n",
      "[50]\ttrain-ndcg@5:0.94093\tvalid-ndcg@5:0.93394\n",
      "[100]\ttrain-ndcg@5:0.95715\tvalid-ndcg@5:0.94802\n",
      "[150]\ttrain-ndcg@5:0.96538\tvalid-ndcg@5:0.95482\n",
      "[200]\ttrain-ndcg@5:0.97063\tvalid-ndcg@5:0.95847\n",
      "[250]\ttrain-ndcg@5:0.97431\tvalid-ndcg@5:0.96151\n",
      "[300]\ttrain-ndcg@5:0.97695\tvalid-ndcg@5:0.96364\n",
      "[350]\ttrain-ndcg@5:0.97937\tvalid-ndcg@5:0.96561\n",
      "[400]\ttrain-ndcg@5:0.98128\tvalid-ndcg@5:0.96678\n",
      "[450]\ttrain-ndcg@5:0.98288\tvalid-ndcg@5:0.96841\n",
      "[500]\ttrain-ndcg@5:0.98415\tvalid-ndcg@5:0.96922\n",
      "[524]\ttrain-ndcg@5:0.98464\tvalid-ndcg@5:0.96946\n",
      "[0]\ttrain-ndcg@5:0.81380\tvalid-ndcg@5:0.81583\n",
      "[50]\ttrain-ndcg@5:0.94047\tvalid-ndcg@5:0.93321\n",
      "[100]\ttrain-ndcg@5:0.95637\tvalid-ndcg@5:0.94791\n",
      "[150]\ttrain-ndcg@5:0.96434\tvalid-ndcg@5:0.95497\n",
      "[200]\ttrain-ndcg@5:0.96972\tvalid-ndcg@5:0.95924\n",
      "[250]\ttrain-ndcg@5:0.97359\tvalid-ndcg@5:0.96258\n",
      "[300]\ttrain-ndcg@5:0.97635\tvalid-ndcg@5:0.96465\n",
      "[350]\ttrain-ndcg@5:0.97860\tvalid-ndcg@5:0.96622\n",
      "[400]\ttrain-ndcg@5:0.98045\tvalid-ndcg@5:0.96777\n",
      "[450]\ttrain-ndcg@5:0.98203\tvalid-ndcg@5:0.96871\n",
      "[487]\ttrain-ndcg@5:0.98306\tvalid-ndcg@5:0.96930\n",
      "[0]\ttrain-ndcg@5:0.81427\tvalid-ndcg@5:0.81616\n",
      "[50]\ttrain-ndcg@5:0.94056\tvalid-ndcg@5:0.93404\n",
      "[100]\ttrain-ndcg@5:0.95686\tvalid-ndcg@5:0.94858\n",
      "[150]\ttrain-ndcg@5:0.96473\tvalid-ndcg@5:0.95543\n",
      "[200]\ttrain-ndcg@5:0.97004\tvalid-ndcg@5:0.95948\n",
      "[250]\ttrain-ndcg@5:0.97379\tvalid-ndcg@5:0.96228\n",
      "[300]\ttrain-ndcg@5:0.97672\tvalid-ndcg@5:0.96440\n",
      "[350]\ttrain-ndcg@5:0.97908\tvalid-ndcg@5:0.96629\n",
      "[400]\ttrain-ndcg@5:0.98096\tvalid-ndcg@5:0.96761\n",
      "[450]\ttrain-ndcg@5:0.98238\tvalid-ndcg@5:0.96892\n",
      "[487]\ttrain-ndcg@5:0.98349\tvalid-ndcg@5:0.96945\n",
      "Best Score (valid-ndcg@5): 0.9700206338995528\n",
      "Best Params: {'booster': 'gbtree', 'objective': 'rank:pairwise', 'eval_metric': ['ndcg@5'], 'tree_method': 'hist', 'device': 'cuda', 'eta': 0.1, 'max_depth': 4, 'verbosity': 1, 'min_child_weight': 10, 'subsample': 0.8}\n",
      "Best Iteration: 495\n"
     ]
    }
   ],
   "source": [
    "# 4) Prepare a watchlist for training/early stopping\n",
    "watchlist = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
    "\n",
    "best_score = float(\"-inf\")\n",
    "best_params = None\n",
    "best_iteration = None\n",
    "\n",
    "# 5) Iterate over all combinations in param_grid\n",
    "for eta in param_grid[\"eta\"]:\n",
    "    for max_depth in param_grid[\"max_depth\"]:\n",
    "        for min_child_weight in param_grid[\"min_child_weight\"]:\n",
    "            for subsample in param_grid[\"subsample\"]:\n",
    "\n",
    "                # Copy base_params, then update with current hyperparams\n",
    "                trial_params = base_params.copy()\n",
    "                trial_params.update({\n",
    "                    \"eta\": eta,\n",
    "                    \"max_depth\": max_depth,\n",
    "                    \"min_child_weight\": min_child_weight,\n",
    "                    \"subsample\": subsample,\n",
    "                })\n",
    "\n",
    "                # 6) Train with early stopping\n",
    "                model = xgb.train(\n",
    "                    params=trial_params,\n",
    "                    dtrain=dtrain,\n",
    "                    num_boost_round=1000,\n",
    "                    evals=watchlist,\n",
    "                    early_stopping_rounds=10,\n",
    "                    verbose_eval=50  # or False/True/50 if you want logs\n",
    "                )\n",
    "\n",
    "                # 7) model.best_score typically corresponds to the \"valid-ndcg@5\" \n",
    "                #    if \"ndcg@5\" is your only eval_metric. \n",
    "                this_score = model.best_score  # Usually the highest valid-ndcg@5\n",
    "\n",
    "                # Update if better\n",
    "                if this_score > best_score:\n",
    "                    best_score = this_score\n",
    "                    best_params = trial_params\n",
    "                    best_iteration = model.best_iteration\n",
    "\n",
    "# 8) Print final results\n",
    "print(\"Best Score (valid-ndcg@5):\", best_score)\n",
    "print(\"Best Params:\", best_params)\n",
    "print(\"Best Iteration:\", best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1c2d4ea-0e97-4745-9fa9-b4862f2dd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Combine train + valid arrays into a single full set\n",
    "X_full = np.concatenate([X_train, X_valid], axis=0)\n",
    "y_full = np.concatenate([y_train, y_valid], axis=0)\n",
    "group_full = np.concatenate([group_train, group_valid], axis=0)\n",
    "\n",
    "# 3) Create an XGBoost DMatrix for the *full* data\n",
    "dfull = xgb.DMatrix(X_full, label=y_full)\n",
    "dfull.set_group(group_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3486d1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-ndcg@5:0.78428\tvalid-ndcg@5:0.78697\n",
      "[50]\ttrain-ndcg@5:0.92983\tvalid-ndcg@5:0.93018\n",
      "[100]\ttrain-ndcg@5:0.94600\tvalid-ndcg@5:0.94570\n",
      "[150]\ttrain-ndcg@5:0.95381\tvalid-ndcg@5:0.95399\n",
      "[200]\ttrain-ndcg@5:0.95899\tvalid-ndcg@5:0.95963\n",
      "[250]\ttrain-ndcg@5:0.96314\tvalid-ndcg@5:0.96375\n",
      "[300]\ttrain-ndcg@5:0.96579\tvalid-ndcg@5:0.96653\n",
      "[350]\ttrain-ndcg@5:0.96820\tvalid-ndcg@5:0.96912\n",
      "[400]\ttrain-ndcg@5:0.97037\tvalid-ndcg@5:0.97117\n",
      "[450]\ttrain-ndcg@5:0.97203\tvalid-ndcg@5:0.97273\n",
      "[500]\ttrain-ndcg@5:0.97364\tvalid-ndcg@5:0.97457\n",
      "[550]\ttrain-ndcg@5:0.97499\tvalid-ndcg@5:0.97576\n",
      "[600]\ttrain-ndcg@5:0.97615\tvalid-ndcg@5:0.97675\n",
      "[642]\ttrain-ndcg@5:0.97705\tvalid-ndcg@5:0.97744\n",
      "Final model best iteration: 632\n",
      "Final model best score: 0.9774410556976562\n"
     ]
    }
   ],
   "source": [
    "# 4) Re-train a final model on the best parameters (train+valid, or just train)\n",
    "#    We'll do a final train on (dtrain + dvalid) if you want. Or just on dtrain.\n",
    "\n",
    "# Example: final train on just the same training set for demonstration\n",
    "# If you want to combine train+valid => you'd DMatrix(np.concatenate(...)).\n",
    "# But let's keep it simple:\n",
    "\n",
    "# Best Score (valid-ndcg@5): 0.9700206338995528\n",
    "# Best Params: {'booster': 'gbtree', 'objective': 'rank:pairwise', 'eval_metric': ['ndcg@5'], \n",
    "# 'tree_method': 'hist', 'device': 'cuda', 'eta': 0.1, 'max_depth': 4, 'verbosity': 1, \n",
    "# 'min_child_weight': 10, 'subsample': 0.8}\n",
    "# Best Iteration: 495\n",
    "\n",
    "best_params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"objective\": \"rank:pairwise\",    # or \"rank:ndcg\"\n",
    "    \"eval_metric\": [\"ndcg@5\"],       # focusing on ndcg@5\n",
    "    \"tree_method\": \"hist\",           # or \"hist\" with \"device\":\"cuda\"\n",
    "    \"device\": \"cuda\",\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_child_weight\": 10,      # <--- newly added\n",
    "    \"subsample\": 0.8,            # <--- newly added\n",
    "    \"verbosity\": 1\n",
    "}\n",
    "\n",
    "# Then train with those parameters:\n",
    "final_model = xgb.train(\n",
    "    params=best_params,\n",
    "    dtrain=dfull,\n",
    "    num_boost_round=1000,\n",
    "    evals=watchlist,\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "print(\"Final model best iteration:\", final_model.best_iteration)\n",
    "print(\"Final model best score:\", final_model.best_score)\n",
    "\n",
    "# 5) Save the final model to disk, if desired:\n",
    "final_model.save_model(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/src/models/XGB_ranking_best_model.json\")\n",
    "\n",
    "# 6) Predict on your validation set or a new test set\n",
    "preds = final_model.predict(dvalid)\n",
    "# 'preds' are ranking scores. Sort each group (race) descending to get top picks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb16452a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
