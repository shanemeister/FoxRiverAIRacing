{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277e9121-0ed4-44f3-b848-ba98dc6ce795",
   "metadata": {},
   "source": [
    "# Proposed Ensemble Models\n",
    "\n",
    "Given the constraints and objectives, I recommend considering the following models for the ensemble:\n",
    "\t\n",
    "    1.\tModel 1: LSTM Network on Raw GPS Data\n",
    "    \n",
    ">•\tInput Data: Sequences of raw GPS data (speed, progress, stride_frequency, etc.).\n",
    "\n",
    ">•\tArchitecture: An LSTM network designed to capture temporal dependencies and patterns in the sequential data.\n",
    "\n",
    ">•\tAdvantage: LSTMs are well-suited for time-series data and can learn complex temporal dynamics without the need for hand-engineered features like acceleration.\n",
    "\n",
    "    2.\tModel 2: 1D Convolutional Neural Network (1D-CNN)\n",
    "\t\n",
    ">•\tInput Data: The same raw GPS sequences as in Model 1.\n",
    "\n",
    ">•\tArchitecture: A 1D-CNN that applies convolutional filters across the time dimension to detect local patterns.\n",
    "\n",
    ">•\tAdvantage: CNNs can capture spatial hierarchies and are effective in recognizing patterns in sequences, potentially identifying features like sudden changes in speed or stride frequency.\n",
    "\n",
    "    3.\tModel 3: Transformer-based Model\n",
    "\t\n",
    ">•\tInput Data: Raw GPS sequences and possibly sectionals data.\n",
    "\n",
    ">•\tArchitecture: A Transformer model that uses self-attention mechanisms to weigh the importance of different parts of the sequence.\n",
    "\n",
    ">•\tAdvantage: Transformers can model long-range dependencies and focus on the most relevant parts of the sequence for prediction.\n",
    "\n",
    "## Additional Models (Optional):\n",
    "\n",
    "    4.\tModel 4: Gated Recurrent Unit (GRU) Network\n",
    "\n",
    ">•\tSimilar to LSTMs but with a simpler architecture, GRUs can be more efficient and may perform better on certain datasets.\n",
    "\n",
    ">•\tModel 5: Temporal Convolutional Network (TCN)\n",
    "\n",
    ">•\tTCNs are designed for sequential data and can capture long-term dependencies using causal convolutions and residual connections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c90ab6",
   "metadata": {},
   "source": [
    "### Validate GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14e5426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3eb8b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 2\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available:\", len(physical_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee863758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "for device in physical_devices:\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afcdb84",
   "metadata": {},
   "source": [
    "#### Make sure JAVA HOME is set to version 11 -- source .zshrc in same folder as you start jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e351e3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/java-11-openjdk\n",
      "openjdk 11.0.25 2024-10-15 LTS\n",
      "OpenJDK Runtime Environment (Red_Hat-11.0.25.0.9-1) (build 11.0.25+9-LTS)\n",
      "OpenJDK 64-Bit Server VM (Red_Hat-11.0.25.0.9-1) (build 11.0.25+9-LTS, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!echo $JAVA_HOME\n",
    "!java --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144788c4-1601-45af-97bd-2832f1d4d22d",
   "metadata": {},
   "source": [
    "# The LSTM Network on Raw GPS Data\n",
    "\n",
    "Initially I desired to merge the GPS data with Sectionals, but the timestamp and gate_name intervals of each respectively made it difficult to align the data in sequences -- something that is needed for Long-Short Term Memory models. Therefore, it was decided to go with an ensemble approach. There will be additional models that incorporate Equibase data as well, but for the time being, the focus will be on Total Performance GPS data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f0647b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 12:47:15,451 - INFO - Environment setup initialized.\n",
      "2024-12-07 12:47:15,454 - INFO - Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min as spark_min, sum as spark_sum, from_utc_timestamp, expr\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Load configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('/home/exx/myCode/horse-racing/FoxRiverAIRacing/config.ini')\n",
    "\n",
    "# Database credentials from config\n",
    "db_host = config['database']['host']\n",
    "db_port = config['database']['port']\n",
    "db_name = config['database']['dbname']\n",
    "db_user = config['database']['user']\n",
    "db_password = os.getenv(\"DB_PASSWORD\", \"SparkPy24!\")  # Ensure DB_PASSWORD is set\n",
    "\n",
    "# Validate database password\n",
    "if not db_password:\n",
    "    raise ValueError(\"Database password is missing. Set it in the DB_PASSWORD environment variable.\")\n",
    "\n",
    "# JDBC URL and properties\n",
    "jdbc_url = f\"jdbc:postgresql://{db_host}:{db_port}/{db_name}\"\n",
    "jdbc_properties = {\n",
    "    \"user\": db_user,\n",
    "    \"password\": db_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Path to JDBC driver\n",
    "jdbc_driver_path = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/jdbc/postgresql-42.7.4.jar\"\n",
    "\n",
    "# Configure logging\n",
    "log_file = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/logs/SparkPy_load.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logging.info(\"Environment setup initialized.\")\n",
    "\n",
    "# Initialize Spark session\n",
    "def initialize_spark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Horse Racing Data Processing\") \\\n",
    "        .config(\"spark.driver.extraClassPath\", jdbc_driver_path) \\\n",
    "        .config(\"spark.executor.extraClassPath\", jdbc_driver_path) \\\n",
    "        .config(\"spark.driver.memory\", \"64g\") \\\n",
    "        .config(\"spark.executor.memory\", \"32g\") \\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"8g\") \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", \"1000\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"LEGACY\") \\\n",
    "        .config(\"spark.sql.legacy.parquet.int96RebaseModeInWrite\", \"LEGACY\") \\\n",
    "        .getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    logging.info(\"Spark session created successfully.\")\n",
    "    return spark\n",
    "\n",
    "# Initialize Spark\n",
    "spark = initialize_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "518deb19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 20:45:54,482 - INFO - Loading results data from PostgreSQL...\n",
      "2024-12-06 20:45:55,637 - INFO - Saving results DataFrame to Parquet at /home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/results.parquet...\n",
      "2024-12-06 20:45:58,430 - INFO - results data loaded and saved successfully.    \n",
      "2024-12-06 20:45:58,431 - INFO - Loading sectionals data from PostgreSQL...\n",
      "2024-12-06 20:45:58,451 - INFO - Saving sectionals DataFrame to Parquet at /home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/sectionals.parquet...\n",
      "2024-12-06 20:46:07,526 - INFO - sectionals data loaded and saved successfully. \n",
      "2024-12-06 20:46:07,527 - INFO - Loading gpspoint data from PostgreSQL...\n",
      "2024-12-06 20:46:07,547 - INFO - Saving gpspoint DataFrame to Parquet at /home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/gpspoint.parquet...\n",
      "2024-12-06 20:47:41,554 - INFO - gpspoint data loaded and saved successfully.   \n",
      "2024-12-06 20:47:41,554 - INFO - Reloading Parquet files into Spark DataFrames for transformation...\n",
      "2024-12-06 20:47:41,747 - INFO - Parquet files reloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Data Loading and Transformation\n",
    "\n",
    "from pyspark.sql.functions import col, min as spark_min, sum as spark_sum, unix_timestamp, to_timestamp, expr\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define SQL queries without trailing semicolons\n",
    "queries = {\n",
    "    \"results\": \"\"\"\n",
    "        SELECT vre.course_cd, vre.race_date, vre.race_number, vre.program_num AS saddle_cloth_number, vre.post_pos,\n",
    "               h.horse_id, vre.official_fin, vre.finish_time, vre.speed_rating, vr.todays_cls,\n",
    "               vr.previous_surface, vr.previous_class, vr.net_sentiment\n",
    "        FROM v_results_entries vre\n",
    "        JOIN v_runners vr \n",
    "            ON vre.course_cd = vr.course_cd\n",
    "            AND vre.race_date = vr.race_date\n",
    "            AND vre.race_number = vr.race_number\n",
    "            AND vre.program_num = vr.saddle_cloth_number\n",
    "        JOIN horse h \n",
    "            ON vre.axciskey = h.axciskey\n",
    "        WHERE vre.breed = 'TB'\n",
    "        GROUP BY vre.course_cd, vre.race_date, vre.race_number, vre.program_num, vre.post_pos,\n",
    "                 h.horse_id, vre.official_fin, vre.finish_time, vre.speed_rating, vr.todays_cls,\n",
    "                 vr.previous_surface, vr.previous_class, vr.net_sentiment\n",
    "    \"\"\",\n",
    "    \"sectionals\": \"\"\"\n",
    "        SELECT course_cd, race_date, race_number, saddle_cloth_number, gate_name, \n",
    "               gate_numeric, length_to_finish, sectional_time, running_time, \n",
    "               distance_back, distance_ran, number_of_strides\n",
    "        FROM v_sectionals\n",
    "    \"\"\",\n",
    "    \"gpspoint\": \"\"\"\n",
    "        SELECT course_cd, race_date, race_number, saddle_cloth_number, time_stamp, \n",
    "               longitude, latitude, speed, progress, stride_frequency, post_time, location\n",
    "        FROM v_gpspoint\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Path to save Parquet files\n",
    "parquet_dir = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/\"\n",
    "os.makedirs(parquet_dir, exist_ok=True)\n",
    "\n",
    "# Load data directly from PostgreSQL into Spark DataFrames and save as Parquet\n",
    "dfs = {}\n",
    "for name, query in queries.items():\n",
    "    logging.info(f\"Loading {name} data from PostgreSQL...\")\n",
    "    try:\n",
    "        df = spark.read.jdbc(url=jdbc_url, table=f\"({query}) AS subquery\", properties=jdbc_properties)\n",
    "        output_path = os.path.join(parquet_dir, f\"{name}.parquet\")\n",
    "        logging.info(f\"Saving {name} DataFrame to Parquet at {output_path}...\")\n",
    "        df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        dfs[name] = df\n",
    "        logging.info(f\"{name} data loaded and saved successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {name} data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Reload Parquet files into Spark DataFrames for processing\n",
    "logging.info(\"Reloading Parquet files into Spark DataFrames for transformation...\")\n",
    "results_df = spark.read.parquet(os.path.join(parquet_dir, \"results.parquet\"))\n",
    "sectionals_df = spark.read.parquet(os.path.join(parquet_dir, \"sectionals.parquet\"))\n",
    "gps_df = spark.read.parquet(os.path.join(parquet_dir, \"gpspoint.parquet\"))\n",
    "logging.info(\"Parquet files reloaded successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdbe02de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import timedelta\n",
    "\n",
    "# Define the UDF to add seconds (including fractional seconds) to a timestamp\n",
    "def add_seconds(ts, seconds):\n",
    "    if ts is None or seconds is None:\n",
    "        return None\n",
    "    return ts + timedelta(seconds=seconds)\n",
    "\n",
    "# Register the UDF\n",
    "add_seconds_udf = udf(add_seconds, TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "72aa4ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of gps_df time_stamp column:\n",
      "+---------------------+\n",
      "|time_stamp           |\n",
      "+---------------------+\n",
      "|2023-10-15 16:37:05.2|\n",
      "|2023-12-10 17:23:03.2|\n",
      "|2023-12-10 17:23:04.2|\n",
      "|2023-12-10 17:23:05.2|\n",
      "|2023-12-10 17:23:06.2|\n",
      "|2023-12-10 17:23:08.2|\n",
      "|2023-10-15 16:37:06.2|\n",
      "|2023-12-10 17:23:10.2|\n",
      "|2023-12-10 17:23:11.2|\n",
      "|2023-12-10 17:23:12.2|\n",
      "+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    unix_timestamp,\n",
    "    expr,\n",
    "    min as spark_min,\n",
    "    sum as spark_sum,\n",
    "    date_format\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import udf\n",
    "from datetime import timedelta\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HorseRacingDataProcessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Clear all cached data\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Reload the DataFrames from Parquet files\n",
    "gps_df = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/gpspoint.parquet\")\n",
    "sectionals_df = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/sectionals.parquet\")\n",
    "\n",
    "# Convert time_stamp to timestamp type\n",
    "gps_df = gps_df.withColumn(\"time_stamp\", col(\"time_stamp\").cast(\"timestamp\"))\n",
    "\n",
    "# Print a sample of the time_stamp column to check for millisecond precision\n",
    "print(\"Sample of gps_df time_stamp column:\")\n",
    "gps_df.select(\"time_stamp\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fe91ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the earliest 'time_stamp' for each race\n",
    "race_id_cols = [\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"]\n",
    "\n",
    "first_time_df = gps_df.groupBy(*race_id_cols).agg(\n",
    "    spark_min(\"time_stamp\").alias(\"earliest_time_stamp\")\n",
    ")\n",
    "\n",
    "# Step 2: Join 'first_time_df' with 'sectionals_df' to associate each sectional with the race's start time\n",
    "sectionals_df = sectionals_df.join(\n",
    "    first_time_df,\n",
    "    on=race_id_cols,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Step 3: Sort 'sectionals_df' by 'gate_numeric' to ensure correct order of gates\n",
    "sectionals_df = sectionals_df.orderBy(*race_id_cols, \"gate_numeric\")\n",
    "\n",
    "# Step 4: Define the window specification for cumulative sum\n",
    "window_spec = Window.partitionBy(*race_id_cols).orderBy(\"gate_numeric\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "# Step 5: Compute cumulative sum of 'sectional_time' for each race\n",
    "sectionals_df = sectionals_df.withColumn(\n",
    "    \"cumulative_sectional_time\",\n",
    "    spark_sum(\"sectional_time\").over(window_spec)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a5d4211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of sectionals_df with sec_time_stamp:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+-------------------+------------+---------+--------------+----------------------+\n",
      "|course_cd|race_date |race_number|saddle_cloth_number|gate_numeric|gate_name|sectional_time|sec_time_stamp        |\n",
      "+---------+----------+-----------+-------------------+------------+---------+--------------+----------------------+\n",
      "|AQU      |2023-01-05|1          |6                  |0.5         |0.5f     |6.89          |2023-01-05 17:53:06.09|\n",
      "|AQU      |2023-01-05|1          |6                  |1.0         |1f       |6.66          |2023-01-05 17:53:12.75|\n",
      "|AQU      |2023-01-05|1          |6                  |1.5         |1.5f     |7.25          |2023-01-05 17:53:20   |\n",
      "|AQU      |2023-01-05|1          |6                  |2.0         |2f       |6.15          |2023-01-05 17:53:26.15|\n",
      "|AQU      |2023-01-05|1          |6                  |2.5         |2.5f     |7.07          |2023-01-05 17:53:33.22|\n",
      "|AQU      |2023-01-05|1          |6                  |3.0         |3f       |6.64          |2023-01-05 17:53:39.86|\n",
      "|AQU      |2023-01-05|1          |6                  |3.5         |3.5f     |6.5           |2023-01-05 17:53:46.36|\n",
      "|AQU      |2023-01-05|1          |6                  |4.0         |4f       |6.18          |2023-01-05 17:53:52.54|\n",
      "|AQU      |2023-01-05|1          |6                  |4.5         |4.5f     |6.44          |2023-01-05 17:53:58.98|\n",
      "|AQU      |2023-01-05|1          |6                  |5.0         |5f       |6.14          |2023-01-05 17:54:05.12|\n",
      "+---------+----------+-----------+-------------------+------------+---------+--------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Define the UDF to add seconds (including fractional seconds) to a timestamp\n",
    "def add_seconds(ts, seconds):\n",
    "    if ts is None or seconds is None:\n",
    "        return None\n",
    "    return ts + timedelta(seconds=seconds)\n",
    "\n",
    "# Register the UDF\n",
    "add_seconds_udf = udf(add_seconds, TimestampType())\n",
    "\n",
    "# Step 7: Create 'sec_time_stamp' by adding 'cumulative_sectional_time' to 'earliest_time_stamp' using the UDF\n",
    "sectionals_df = sectionals_df.withColumn(\n",
    "    \"sec_time_stamp\",\n",
    "    add_seconds_udf(col(\"earliest_time_stamp\"), col(\"cumulative_sectional_time\"))\n",
    ")\n",
    "\n",
    "# Step 8: Drop intermediate columns if no longer needed\n",
    "sectionals_df = sectionals_df.drop(\"earliest_time_stamp\", \"cumulative_sectional_time\")\n",
    "\n",
    "# Show a sample of the results\n",
    "print(\"Sample of sectionals_df with sec_time_stamp:\")\n",
    "sectionals_df.select(\n",
    "    \"course_cd\",\n",
    "    \"race_date\",\n",
    "    \"race_number\",\n",
    "    \"saddle_cloth_number\",\n",
    "    \"gate_numeric\",\n",
    "    \"gate_name\",\n",
    "    \"sectional_time\",\n",
    "    \"sec_time_stamp\"\n",
    ").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8865462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, proceed with the join to create matched_df\n",
    "\n",
    "from pyspark.sql.functions import abs\n",
    "\n",
    "# Step 9: Convert 'time_stamp' and 'sec_time_stamp' to milliseconds since epoch to preserve sub-second precision\n",
    "gps_with_ms = gps_df.withColumn(\n",
    "    \"time_stamp_ms\",\n",
    "    (col(\"time_stamp\").cast(\"double\") * 1000).cast(\"long\")\n",
    ")\n",
    "\n",
    "sectionals_with_ms = sectionals_df.withColumn(\n",
    "    \"sec_time_stamp_ms\",\n",
    "    (col(\"sec_time_stamp\").cast(\"double\") * 1000).cast(\"long\")\n",
    ")\n",
    "\n",
    "# Step 10: Define the join condition with time window (±1000 milliseconds)\n",
    "join_condition = (\n",
    "    (gps_with_ms.course_cd == sectionals_with_ms.course_cd) &\n",
    "    (gps_with_ms.race_date == sectionals_with_ms.race_date) &\n",
    "    (gps_with_ms.race_number == sectionals_with_ms.race_number) &\n",
    "    (gps_with_ms.saddle_cloth_number == sectionals_with_ms.saddle_cloth_number) &\n",
    "    (abs(gps_with_ms.time_stamp_ms - sectionals_with_ms.sec_time_stamp_ms) <= 500)\n",
    ")\n",
    "\n",
    "# Step 11: Perform the left join based on the join condition\n",
    "matched_df = gps_with_ms.join(\n",
    "    sectionals_with_ms,\n",
    "    on=join_condition,\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    gps_with_ms[\"*\"],\n",
    "    sectionals_with_ms[\"sec_time_stamp\"],\n",
    "    sectionals_with_ms[\"gate_numeric\"],\n",
    "    sectionals_with_ms[\"gate_name\"],\n",
    "    sectionals_with_ms[\"sectional_time\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "54dd60cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of matched_df (All gps_df records with sectional data where available):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1936:===========================================>         (54 + 12) / 66]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+-------------------+---------------------+----------------------+------------+---------+--------------+\n",
      "|course_cd|race_date |race_number|saddle_cloth_number|time_stamp           |sec_time_stamp        |gate_numeric|gate_name|sectional_time|\n",
      "+---------+----------+-----------+-------------------+---------------------+----------------------+------------+---------+--------------+\n",
      "|ELP      |2023-07-30|1          |3                  |2023-07-30 16:48:47.5|null                  |null        |null     |null          |\n",
      "|ELP      |2023-07-30|1          |3                  |2023-07-30 16:48:48.5|null                  |null        |null     |null          |\n",
      "|LRL      |2023-11-24|9          |1                  |2023-11-24 21:25:51.2|null                  |null        |null     |null          |\n",
      "|LRL      |2023-11-24|9          |1                  |2023-11-24 21:25:52.2|null                  |null        |null     |null          |\n",
      "|TTP      |2022-12-09|3          |5                  |2022-12-10 00:03:04.2|null                  |null        |null     |null          |\n",
      "|TTP      |2022-12-09|3          |5                  |2022-12-10 00:03:05.2|null                  |null        |null     |null          |\n",
      "|TTP      |2022-12-09|3          |8                  |2022-12-10 00:03:04.2|2022-12-10 00:03:04.01|3.5         |3.5f     |5.71          |\n",
      "|TTP      |2022-12-09|3          |8                  |2022-12-10 00:03:05.2|null                  |null        |null     |null          |\n",
      "|ELP      |2023-07-30|1          |4                  |2023-07-30 16:48:47.5|null                  |null        |null     |null          |\n",
      "|ELP      |2023-07-30|1          |4                  |2023-07-30 16:48:48.5|null                  |null        |null     |null          |\n",
      "+---------+----------+-----------+-------------------+---------------------+----------------------+------------+---------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1936:=====================================================>(65 + 1) / 66]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 12: Verify the matched records\n",
    "print(\"Sample of matched_df (All gps_df records with sectional data where available):\")\n",
    "matched_df.select(\n",
    "    *race_id_cols,\n",
    "    \"time_stamp\",\n",
    "    \"sec_time_stamp\",\n",
    "    \"gate_numeric\",\n",
    "    \"gate_name\",\n",
    "    \"sectional_time\"\n",
    ").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ab6e7e03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1975:================================>                  (128 + 72) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in matched_df: 36633427\n",
      "Total number of rows in gps_df: 36633397\n",
      "Total number of rows in sectionals_df: 4742485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1975:==================================================> (193 + 7) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 13: Show the total number of rows in matched_df, gps_df, and sectionals_df\n",
    "matched_count = matched_df.count()\n",
    "print(f\"Total number of rows in matched_df: {matched_count}\")\n",
    "\n",
    "gps_count = gps_df.count()\n",
    "print(f\"Total number of rows in gps_df: {gps_count}\")\n",
    "\n",
    "sectionals_count = sectionals_df.count()\n",
    "print(f\"Total number of rows in sectionals_df: {sectionals_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "01761c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['course_cd',\n",
       " 'race_date',\n",
       " 'race_number',\n",
       " 'saddle_cloth_number',\n",
       " 'time_stamp',\n",
       " 'longitude',\n",
       " 'latitude',\n",
       " 'speed',\n",
       " 'progress',\n",
       " 'stride_frequency',\n",
       " 'post_time',\n",
       " 'location',\n",
       " 'time_stamp_ms',\n",
       " 'sec_time_stamp',\n",
       " 'sec_time_stamp_ms',\n",
       " 'gate_numeric',\n",
       " 'gate_name',\n",
       " 'sectional_time']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_df_unique.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "37c12b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2024:=================================>                 (130 + 70) / 200]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unmatched sectional records: 1045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2036:==================================================> (192 + 4) / 196]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 14: Show the number of unmatched sectional records\n",
    "\n",
    "# Extract matched sectional records\n",
    "matched_sectionals = matched_df.select(\n",
    "    *race_id_cols,\n",
    "    \"sec_time_stamp\"\n",
    ").distinct()\n",
    "\n",
    "# Perform a left anti join to find sectionals not present in matched_sectionals\n",
    "unmatched_sectionals_df = sectionals_df.join(\n",
    "    matched_sectionals,\n",
    "    on=race_id_cols + [\"sec_time_stamp\"],\n",
    "    how=\"leftanti\"\n",
    ")\n",
    "\n",
    "# Count the number of unmatched sectional records\n",
    "unmatched_sectionals_count = unmatched_sectionals_df.count()\n",
    "print(f\"Number of unmatched sectional records: {unmatched_sectionals_count}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c4fdc4",
   "metadata": {},
   "source": [
    "### Deduplication Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d78cf81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                28]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sectional records with duplicate GPS matches after deduplication: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2100:====================================================> (64 + 2) / 66]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+-------------------+---------------------+----------------------+------------+---------+--------------+\n",
      "|course_cd|race_date |race_number|saddle_cloth_number|time_stamp           |sec_time_stamp        |gate_numeric|gate_name|sectional_time|\n",
      "+---------+----------+-----------+-------------------+---------------------+----------------------+------------+---------+--------------+\n",
      "|AQU      |2023-01-07|6          |5                  |2023-01-07 19:21:22.2|2023-01-07 19:21:22.4 |0.5         |0.5f     |6.2           |\n",
      "|AQU      |2023-01-07|6          |5                  |2023-01-07 19:21:28.2|2023-01-07 19:21:28.66|1.0         |1f       |6.26          |\n",
      "|AQU      |2023-01-07|6          |5                  |2023-01-07 19:21:35.2|2023-01-07 19:21:35.01|1.5         |1.5f     |6.35          |\n",
      "|AQU      |2023-01-07|6          |5                  |2023-01-07 19:21:41.2|2023-01-07 19:21:41.36|2.0         |2f       |6.35          |\n",
      "|AQU      |2023-01-07|6          |5                  |2023-01-07 19:21:47.2|2023-01-07 19:21:47.49|2.5         |2.5f     |6.13          |\n",
      "|AQU      |2023-01-07|6          |5                  |2023-01-07 19:21:54.2|2023-01-07 19:21:53.72|3.0         |3f       |6.23          |\n",
      "|AQU      |2023-01-07|6          |5                  |2023-01-07 19:21:59.2|2023-01-07 19:21:59.43|3.5         |3.5f     |5.71          |\n",
      "|AQU      |2023-01-07|6          |5                  |2023-01-07 19:22:05.2|2023-01-07 19:22:05.14|4.0         |4f       |5.71          |\n",
      "|AQU      |2023-01-07|6          |5                  |2023-01-07 19:22:10.2|2023-01-07 19:22:10.65|4.5         |4.5f     |5.51          |\n",
      "|AQU      |2023-01-07|6          |5                  |2023-01-07 19:22:16.2|2023-01-07 19:22:16.19|5.0         |5f       |5.54          |\n",
      "+---------+----------+-----------+-------------------+---------------------+----------------------+------------+---------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, abs, col, asc, count\n",
    "\n",
    "# Define race identifier columns\n",
    "race_id_cols = [\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"]\n",
    "\n",
    "# Re-perform the join with the current tolerance (e.g., ±500 ms)\n",
    "matched_df = gps_with_ms.join(\n",
    "    sectionals_with_ms,\n",
    "    on=join_condition,  # Ensure this includes the ±500 ms tolerance\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    gps_with_ms[\"*\"],\n",
    "    sectionals_with_ms[\"sec_time_stamp\"],\n",
    "    sectionals_with_ms[\"sec_time_stamp_ms\"],  # Ensure this is included\n",
    "    sectionals_with_ms[\"gate_numeric\"],\n",
    "    sectionals_with_ms[\"gate_name\"],\n",
    "    sectionals_with_ms[\"sectional_time\"]\n",
    ")\n",
    "\n",
    "# Filter out records with null time_stamp_ms or sec_time_stamp_ms\n",
    "matched_df_filtered = matched_df.filter(\n",
    "    col(\"time_stamp_ms\").isNotNull() & col(\"sec_time_stamp_ms\").isNotNull()\n",
    ")\n",
    "\n",
    "# Define the window specification with secondary sort to handle ties\n",
    "window_spec_dup = Window.partitionBy(*race_id_cols, \"sec_time_stamp\") \\\n",
    "                        .orderBy(\n",
    "                            abs(col(\"time_stamp_ms\") - col(\"sec_time_stamp_ms\")),\n",
    "                            asc(\"time_stamp_ms\")  # Secondary sort to ensure deterministic selection\n",
    "                        )\n",
    "\n",
    "# Assign row numbers to each GPS match within the sectional group\n",
    "matched_df_with_row_num = matched_df_filtered.withColumn(\"row_num\", row_number().over(window_spec_dup))\n",
    "\n",
    "# Retain only the first (closest) GPS match per sectional record\n",
    "matched_df_unique = matched_df_with_row_num.filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "# Verify deduplication\n",
    "duplicate_check = matched_df_unique.groupBy(*race_id_cols, \"sec_time_stamp\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .filter(col(\"count\") > 1)\n",
    "\n",
    "num_duplicates_after = duplicate_check.count()\n",
    "print(f\"Number of sectional records with duplicate GPS matches after deduplication: {num_duplicates_after}\")\n",
    "\n",
    "\n",
    "# Display a sample to ensure correctness\n",
    "matched_df_unique.select(*race_id_cols, \"time_stamp\", \"sec_time_stamp\", \"gate_numeric\", \"gate_name\", \"sectional_time\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d22c25",
   "metadata": {},
   "source": [
    "### Recheck for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "22b9b91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2302:=============================================>        (40 + 8) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sectional records with duplicate GPS matches: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, col, collect_list\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Define race identifier columns\n",
    "race_id_cols = [\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"]\n",
    "\n",
    "\n",
    "# Group by sectional identifiers and collect GPS time_stamps\n",
    "duplicate_matches = matched_df_unique.groupBy(*race_id_cols, \"sec_time_stamp\") \\\n",
    "    .agg(\n",
    "        collect_list(\"time_stamp\").alias(\"gps_time_stamps\"),\n",
    "        count(\"*\").alias(\"gps_match_count\")\n",
    "    ) \\\n",
    "    .filter(col(\"gps_match_count\") > 1)\n",
    "\n",
    "# Count the number of sectional records with duplicates\n",
    "num_duplicate_sectionals = duplicate_matches.count()\n",
    "print(f\"Number of sectional records with duplicate GPS matches: {num_duplicate_sectionals}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472361ba",
   "metadata": {},
   "source": [
    "# Merge Route and Results with GPS and Sectionals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914fade7",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare the Routes Data\n",
    "\n",
    "The Routes table includes route layouts and the coordinates of the running and winning lines. \n",
    "\n",
    "We’ll need to:\n",
    "\n",
    "1.\tLoad the Routes data into a Spark DataFrame.\n",
    "\n",
    "2.\tJoin it with the GPS data using the race identifiers (course_cd, race_date, race_number).\n",
    "\n",
    "3.\tCalculate deviations from the “ideal” route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ffc70405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Routes data\n",
    "# PRIMARY KEY (course_cd, line_type, line_name)\n",
    "routes_query = \"\"\"\n",
    "    SELECT course_cd, track_name, line_type, line_name, coordinates, \n",
    "           created_at\n",
    "    FROM routes\n",
    "\"\"\" \n",
    "    \n",
    "routes_df = spark.read.jdbc(url=jdbc_url, table=f\"({routes_query}) AS subquery\", properties=jdbc_properties)\n",
    "\n",
    "# Save to Parquet for reuse\n",
    "routes_df.write.mode(\"overwrite\").parquet(os.path.join(parquet_dir, \"routes.parquet\"))\n",
    "\n",
    "# Reload if needed\n",
    "routes_df = spark.read.parquet(os.path.join(parquet_dir, \"routes.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f8d322bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------+------------+----------------------+\n",
      "|course_cd|track_name                 |line_type   |line_name             |\n",
      "+---------+---------------------------+------------+----------------------+\n",
      "|KEE      |KEENELAND                  |WINNING_LINE|WINNING_LINE          |\n",
      "|KEE      |KEENELAND                  |RUNNING_LINE|RUNNING_LINE 5 30FT   |\n",
      "|TGG      |GOLDEN GATE FIELDS         |WINNING_LINE|WINNING_LINE          |\n",
      "|TGG      |GOLDEN GATE FIELDS         |RUNNING_LINE|RUNNING_LINE_3        |\n",
      "|TOP      |OAKLAWN PARK               |WINNING_LINE|WINNING_LINE ALTERNATE|\n",
      "|TAM      |TAMPA BAY DOWNS            |WINNING_LINE|WINNING_LINE          |\n",
      "|TLS      |LONE STAR PARK             |WINNING_LINE|WINNING_LINE          |\n",
      "|TCD      |CHURCHILL DOWNS            |WINNING_LINE|WINNING_LINE          |\n",
      "|TCD      |CHURCHILL DOWNS            |RUNNING_LINE|RUNNING_LINE LANE 4   |\n",
      "|MVR      |MAHONING VALLEY RACE COURSE|WINNING_LINE|WINNING_LINE          |\n",
      "|MVR      |MAHONING VALLEY RACE COURSE|RUNNING_LINE|RUNNING_LINE          |\n",
      "|LAD      |LOUISIANA DOWNS            |WINNING_LINE|WINNING_LINE          |\n",
      "|LAD      |LOUISIANA DOWNS            |RUNNING_LINE|RUNNING_LINE LANE 3   |\n",
      "|TGP      |GULFSTREAM PARK            |WINNING_LINE|WINNING_LINE          |\n",
      "|TGP      |GULFSTREAM PARK            |RUNNING_LINE|RUNNING_LINE 65FT     |\n",
      "|HOU      |SAM HOUSTON RACE PARK      |WINNING_LINE|WINNING_LINE          |\n",
      "|HOU      |SAM HOUSTON RACE PARK      |RUNNING_LINE|RUNNING_LINE LANE 3   |\n",
      "|AQU      |AQUEDUCT                   |WINNING_LINE|WINNING_LINE          |\n",
      "|MTH      |MONMOUTH PARK              |WINNING_LINE|WINNING_LINE          |\n",
      "|IND      |HORSESHOE INDIANAPOLIS     |WINNING_LINE|WINNING_LINE          |\n",
      "|IND      |HORSESHOE INDIANAPOLIS     |RUNNING_LINE|RUNNING_LINE          |\n",
      "|LRL      |LAUREL PARK                |WINNING_LINE|WINNING_LINE ALTERNATE|\n",
      "|LRL      |LAUREL PARK                |RUNNING_LINE|RUNNING_LINE          |\n",
      "|ASD      |ASSINIBOIA DOWNS           |WINNING_LINE|WINNING_LINE          |\n",
      "|DMR      |DEL MAR                    |WINNING_LINE|WINNING_LINE          |\n",
      "|TWO      |WOODBINE                   |WINNING_LINE|WINNING_LINE          |\n",
      "|TWO      |WOODBINE                   |RUNNING_LINE|RUNNING_LINE LANE 4   |\n",
      "|CBY      |CANTERBURY PARK            |WINNING_LINE|WINNING_LINE          |\n",
      "|TTP      |TURFWAY PARK               |WINNING_LINE|WINNING_LINE          |\n",
      "|CNL      |COLONIAL DOWNS             |WINNING_LINE|WINNING_LINE          |\n",
      "|TSA      |SANTA ANITA PARK           |WINNING_LINE|WINNING_LINE          |\n",
      "|CNL      |COLONIAL DOWNS             |RUNNING_LINE|RUNNING_LINE_LANE_10  |\n",
      "|PIM      |PIMLICO                    |WINNING_LINE|WINNING_LINE          |\n",
      "|PIM      |PIMLICO                    |RUNNING_LINE|RUNNING_LINE +20FT    |\n",
      "|TKD      |KENTUCKY DOWNS             |WINNING_LINE|WINNING_LINE          |\n",
      "|PEN      |PENN NATIONAL              |WINNING_LINE|WINNING_LINE          |\n",
      "|TKD      |KENTUCKY DOWNS             |RUNNING_LINE|RUNNING_LINE 3        |\n",
      "|PEN      |PENN NATIONAL              |RUNNING_LINE|RUNNING_LINE          |\n",
      "+---------+---------------------------+------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "routes_df.select(\"course_cd\", \"track_name\", \"line_type\", \"line_name\").show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7a8fb94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_df = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/parsed_routes_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7546922f",
   "metadata": {},
   "source": [
    "# Route Metrics\n",
    "\n",
    "## 1. Route Efficiency\n",
    "\n",
    "Route Efficiency measures the degree to which a horse follows the optimal racing path defined by the track’s coordinates. It provides a ratio of the actual distance a horse runs versus the optimal distance prescribed by the track layout.\n",
    "\n",
    "> •\tFormula:\n",
    "\n",
    "> $$\\text{Route Efficiency} = \\frac{\\text{Distance Ran}}{\\text{Optimal Route Length}}$$\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "> •\tRoute Efficiency = 1: Perfect adherence to the optimal path.\n",
    "\n",
    "> •\tRoute Efficiency < 1: Runs shorter than the optimal path (possibly cutting corners).\n",
    "\n",
    "> •\tRoute Efficiency > 1: Runs longer than the optimal path (possibly veering off track).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "10b97619",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[184], line 61\u001b[0m\n\u001b[1;32m     54\u001b[0m routes_with_distance \u001b[38;5;241m=\u001b[39m routes_consecutive\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegment_distance\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m     haversine_udf(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat1\u001b[39m\u001b[38;5;124m\"\u001b[39m), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon1\u001b[39m\u001b[38;5;124m\"\u001b[39m), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat2\u001b[39m\u001b[38;5;124m\"\u001b[39m), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon2\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Sum up all segment distances to get the total optimal route length per course_cd\u001b[39;00m\n\u001b[1;32m     60\u001b[0m optimal_route_length \u001b[38;5;241m=\u001b[39m routes_with_distance\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcourse_cd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msegment_distance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimal_route_length_meters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m optimal_route_length\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf, explode, sum as spark_sum\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "# Define the Haversine function\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    if None in (lat1, lon1, lat2, lon2):\n",
    "        return 0.0\n",
    "    # Convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(math.radians, [lon1, lat1, lon2, lat2])\n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    r = 6371000  # Radius of Earth in meters\n",
    "    return c * r\n",
    "\n",
    "# Register the Haversine UDF\n",
    "haversine_udf = udf(haversine, DoubleType())\n",
    "\n",
    "# Explode the parsed_coordinates to compute distances between consecutive points\n",
    "routes_exploded = routes_df.withColumn(\"coord\", explode(\"parsed_coordinates\")) \\\n",
    "    .withColumn(\"coord_index\", col(\"coord\").getItem(\"longitude\"))  # Dummy column to preserve order\n",
    "\n",
    "# Since Spark doesn't guarantee order when using explode, we need to order the coordinates properly\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"course_cd\").orderBy(\"created_at\")\n",
    "\n",
    "routes_ordered = routes_df.withColumn(\"coord\", explode(\"parsed_coordinates\")) \\\n",
    "    .withColumn(\"coord_order\", row_number().over(window_spec)) \\\n",
    "    .select(\"course_cd\", \"coord_order\", \"coord.latitude\", \"coord.longitude\")\n",
    "\n",
    "# Self-join to get consecutive coordinates\n",
    "routes_shifted = routes_ordered.withColumnRenamed(\"latitude\", \"lat1\") \\\n",
    "    .withColumnRenamed(\"longitude\", \"lon1\")\n",
    "\n",
    "routes_consecutive = routes_ordered.alias(\"current\") \\\n",
    "    .join(routes_shifted.alias(\"previous\"),\n",
    "          (col(\"current.course_cd\") == col(\"previous.course_cd\")) &\n",
    "          (col(\"current.coord_order\") == col(\"previous.coord_order\") + 1),\n",
    "          \"left\") \\\n",
    "    .select(\n",
    "        col(\"current.course_cd\"),\n",
    "        col(\"previous.lat1\"),\n",
    "        col(\"previous.lon1\"),\n",
    "        col(\"current.latitude\").alias(\"lat2\"),\n",
    "        col(\"current.longitude\").alias(\"lon2\")\n",
    "    )\n",
    "\n",
    "# Calculate distance between consecutive points\n",
    "routes_with_distance = routes_consecutive.withColumn(\n",
    "    \"segment_distance\",\n",
    "    haversine_udf(col(\"lat1\"), col(\"lon1\"), col(\"lat2\"), col(\"lon2\"))\n",
    ")\n",
    "\n",
    "# Sum up all segment distances to get the total optimal route length per course_cd\n",
    "optimal_route_length = routes_with_distance.groupBy(\"course_cd\").agg(\n",
    "    spark_sum(\"segment_distance\").alias(\"optimal_route_length_meters\")\n",
    ")\n",
    "\n",
    "optimal_route_length.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f0e4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1059759d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f21c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d701c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0868e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bec1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb95de6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9d00d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbafbbbe",
   "metadata": {},
   "source": [
    "# Cached matchted_df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "76cab1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[course_cd: string, race_date: date, race_number: int, saddle_cloth_number: string, time_stamp: timestamp, longitude: double, latitude: double, speed: double, progress: double, stride_frequency: double, post_time: timestamp, location: string, time_stamp_ms: bigint, sec_time_stamp: timestamp, sec_time_stamp_ms: bigint, gate_numeric: double, gate_name: string, sectional_time: double]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_df_unique.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d385e158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "matched_df_unique.write.mode(\"overwrite\").parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/matched_df_unique.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc0d301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca25f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea57dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9051d974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa894c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84796b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbc77d5c",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "## Sample data initially\n",
    "\n",
    "Taking a random sample will not work for time series as was attempted, but taking a smaller sample by filtering on date should work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a4907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for a specific date range or course\n",
    "#df_filtered = merged_df[merged_df['race_date'] >= '2024-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab39f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3497607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_filtered.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64664ca4",
   "metadata": {},
   "source": [
    "## Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c3b96e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2342:=========>                                         (39 + 128) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+-------------------+----------+---------+--------+-----+--------+----------------+---------+--------+-------------+--------------+-----------------+------------+---------+--------------+\n",
      "|course_cd|race_date|race_number|saddle_cloth_number|time_stamp|longitude|latitude|speed|progress|stride_frequency|post_time|location|time_stamp_ms|sec_time_stamp|sec_time_stamp_ms|gate_numeric|gate_name|sectional_time|\n",
      "+---------+---------+-----------+-------------------+----------+---------+--------+-----+--------+----------------+---------+--------+-------------+--------------+-----------------+------------+---------+--------------+\n",
      "|0        |0        |0          |0                  |0         |0        |0       |0    |0       |397773          |0        |0       |0            |0             |0                |205559      |0        |0             |\n",
      "+---------+---------+-----------+-------------------+----------+---------+--------+-----+--------+----------------+---------+--------+-------------+--------------+-----------------+------------+---------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2342:============================================>      (173 + 27) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# List of all columns in the DataFrame\n",
    "columns = matched_df_unique.columns\n",
    "\n",
    "# Create a list of expressions to count nulls in each column\n",
    "null_counts = [count(when(col(c).isNull(), c)).alias(c) for c in columns]\n",
    "\n",
    "# Apply the expressions to the DataFrame\n",
    "matched_df_unique.select(null_counts).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Heatmap of missing values (on a small sample)\n",
    "#sns.heatmap(df.isnull(), cbar=False)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef399eb",
   "metadata": {},
   "source": [
    "## Imputation for Stride Frequency and number_of_strides\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5765470",
   "metadata": {},
   "source": [
    "#### Group-Based Imputation: Impute based on groups, such as per horse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d22ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stride_frequency'] = df.groupby('saddle_cloth_number')['stride_frequency'].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f917dd",
   "metadata": {},
   "source": [
    "#### gate_numeric remains the same within a group until changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea16726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['gate_numeric'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8464e",
   "metadata": {},
   "source": [
    "#### Interprolation distance_back changes over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bafc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['distance_back'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465eb678",
   "metadata": {},
   "source": [
    "#### Group Based Imputation for number of strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['number_of_strides'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc1d8b",
   "metadata": {},
   "source": [
    "## Choose Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ffa9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d11bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f5fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    'speed',\n",
    "    'progress',\n",
    "    'stride_frequency',\n",
    "    'number_of_strides',\n",
    "    'post_pos',\n",
    "    'gate_numeric',\n",
    "    'length_to_finish',\n",
    "    'sectional_time',\n",
    "    'running_time',\n",
    "    'distance_back',\n",
    "    'distance_ran'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb71eed",
   "metadata": {},
   "source": [
    "## Feature Engineering -- calculate additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33596bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['course_cd', 'race_date', 'race_number', 'saddle_cloth_number', 'time_stamp'], inplace=True)\n",
    "df['acceleration'] = df.groupby(\n",
    "    ['course_cd', 'race_date', 'race_number', 'saddle_cloth_number']\n",
    ")['speed'].diff() / df.groupby(\n",
    "    ['course_cd', 'race_date', 'race_number', 'saddle_cloth_number']\n",
    ")['time_stamp'].diff().dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02a724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['acceleration'] = df['acceleration'].replace([np.inf, -np.inf], np.nan)\n",
    "df['acceleration'] = df['acceleration'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d74bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns.append('acceleration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b93a8a",
   "metadata": {},
   "source": [
    "## Scale Features\n",
    "\n",
    "Scaling helps in training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Scaling should be done after sequences are created to avoid data leakage.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752c49a",
   "metadata": {},
   "source": [
    "## Create Sequences for LSTM\n",
    "\n",
    "a. Group Data\n",
    "\n",
    "Group the data to create sequences for each horse in each race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0fd29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_columns = ['course_cd', 'race_date', 'race_number', 'saddle_cloth_number']\n",
    "groups = df_sampled.groupby(group_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381cd694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f8bfdbd",
   "metadata": {},
   "source": [
    "##  Create Sequences and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c3e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for name, group in groups:\n",
    "    # Ensure group is sorted by time\n",
    "    group = group.sort_values('time_stamp')\n",
    "\n",
    "    # Extract features\n",
    "    features = group[feature_columns].values\n",
    "\n",
    "    # Append the sequence\n",
    "    sequences.append(features)\n",
    "\n",
    "    # Extract label (official finishing position)\n",
    "    label = group['official_fin'].iloc[0]  # Assuming it's the same for all entries in the group\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18e7000",
   "metadata": {},
   "source": [
    "## Determine max_seq_length and num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9175f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Alternatively, set a fixed max_seq_length to limit memory usage.\n",
    "max_seq_length = max(len(seq) for seq in sequences)\n",
    "num_features = len(feature_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_seq_length)\n",
    "print(num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa5185",
   "metadata": {},
   "source": [
    "## Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d49e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sequences, maxlen=max_seq_length, padding='post', dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa80b133",
   "metadata": {},
   "source": [
    "## Convert Labels\n",
    "\n",
    "Adjust labels to start from 0 if they start from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2dd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels).astype(int) - 1\n",
    "num_classes = labels.max() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8066c970",
   "metadata": {},
   "source": [
    "## Scale Features\n",
    "\n",
    "Now, scale the features. Be cautious to fit the scaler only on the training data to prevent data leakage.\n",
    "\n",
    "Flatten sequences for scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1131fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = padded_sequences.shape[0]\n",
    "X_flat = padded_sequences.reshape(-1, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13668f1",
   "metadata": {},
   "source": [
    "## Fit scaler on the flattened data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_flat = scaler.fit_transform(X_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d826ea",
   "metadata": {},
   "source": [
    "## Reshape back to original shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6450ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = X_scaled_flat.reshape(num_samples, max_seq_length, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9aa653",
   "metadata": {},
   "source": [
    "# Split Data into Training, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987873f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume sequences and labels have been created and padded_sequences is available\n",
    "\n",
    "# Convert labels\n",
    "labels = np.array(labels).astype(int) - 1\n",
    "num_classes = labels.max() + 1\n",
    "\n",
    "# Split data\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    padded_sequences, labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Check shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Flatten training data and fit scaler\n",
    "num_samples_train = X_train.shape[0]\n",
    "X_train_flat = X_train.reshape(-1, num_features)\n",
    "X_train_scaled_flat = scaler.fit_transform(X_train_flat)\n",
    "X_train_scaled = X_train_scaled_flat.reshape(num_samples_train, max_seq_length, num_features)\n",
    "\n",
    "# Scale validation data\n",
    "num_samples_val = X_val.shape[0]\n",
    "X_val_flat = X_val.reshape(-1, num_features)\n",
    "X_val_scaled_flat = scaler.transform(X_val_flat)\n",
    "X_val_scaled = X_val_scaled_flat.reshape(num_samples_val, max_seq_length, num_features)\n",
    "\n",
    "# Scale test data\n",
    "num_samples_test = X_test.shape[0]\n",
    "X_test_flat = X_test.reshape(-1, num_features)\n",
    "X_test_scaled_flat = scaler.transform(X_test_flat)\n",
    "X_test_scaled = X_test_scaled_flat.reshape(num_samples_test, max_seq_length, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92557e9",
   "metadata": {},
   "source": [
    "Ensure that X_train, X_val, X_test, y_train, y_val, and y_test are correctly shaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a4499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520c7e1",
   "metadata": {},
   "source": [
    "# Prepare Data for Model Training\n",
    "\n",
    "## Training the LSTM Model\n",
    "\n",
    "### Build the Model\n",
    "\n",
    "This model combines dropout, regularization, and normalization for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ef1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model_lstm = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(max_seq_length, num_features)),\n",
    "    tf.keras.layers.Masking(mask_value=0.0),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "        256, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(1e-4))),\n",
    "    tf.keras.layers.LayerNormalization(),    \n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "        128, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(1e-4))),\n",
    "    tf.keras.layers.LayerNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "        64, kernel_regularizer=tf.keras.regularizers.l2(1e-4))),\n",
    "    tf.keras.layers.LayerNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "#model_lstm = tf.keras.Sequential([\n",
    "#    tf.keras.Input(shape=(max_seq_length, num_features)),\n",
    "#    tf.keras.layers.Masking(mask_value=0.0),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "#    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "#])\n",
    "\n",
    "#model_lstm = tf.keras.Sequential([\n",
    "#    tf.keras.Input(shape=(max_seq_length, num_features)),\n",
    "#    tf.keras.layers.Masking(mask_value=0.0),\n",
    "#    tf.keras.layers.LSTM(128),\n",
    "#    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "#])\n",
    "\n",
    "#model_lstm = tf.keras.Sequential([\n",
    "#    tf.keras.Input(shape=(max_seq_length, num_features)),\n",
    "#    tf.keras.layers.Masking(mask_value=0.0),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "#    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "#    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "#])\n",
    "\n",
    "#988/988 ━━━━━━━━━━━━━━━━━━━━ 7s 7ms/step - accuracy: 0.3606 - loss: 1.6184\n",
    "#Test Loss: 1.6182985305786133, Test Accuracy: 0.36063656210899353"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d55364",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0502b82b",
   "metadata": {},
   "source": [
    "RMSprop is often a good choice for RNNs.\n",
    "\n",
    ">\t•\tThe learning rate of 0.001 is a typical starting point.\n",
    "\n",
    ">   •\tRecommendation:\n",
    "\n",
    ">   •\tYou can experiment with different learning rates (e.g., 0.0005, 0.0001) if needed.\n",
    "\n",
    ">   •\tAlternatively, you can also try the Adam optimizer and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimenting with different learning rates (e.g., 0.0005, 0.0001) to see if it affects convergence.\n",
    "\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "\n",
    "model_lstm.compile(\n",
    "    optimizer=optimizer,   # 'adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'] #,tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133892ef",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b320083",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "> Learning Rate Scheduler and Early Stopping\n",
    "\n",
    "> * Learning Rate Scheduler\n",
    "\n",
    ">  * Earlystopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914da2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb51d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_lstm.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,  \n",
    "    batch_size=128,  # 64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[\n",
    "        lr_scheduler, \n",
    "        early_stopping,\n",
    "        model_checkpoint\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa1f52",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fce6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_lstm.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae00699",
   "metadata": {},
   "source": [
    "## Plot Training and Validation Loss and Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ea95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb32d9b",
   "metadata": {},
   "source": [
    "### Check for Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e62123",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf8e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(unique, counts)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dec097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd02bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48bdbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e32c3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3efda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd412208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08664aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33143647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ef749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c30501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df[['speed', 'progress', 'stride_frequency', 'longitude', 'latitude', 'post_pos', 'official_fin']].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a5d93-705d-42e3-956a-9cb9dbc4178a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Define your variables\n",
    "max_seq_length = 120  # Replace with your maximum sequence length\n",
    "num_features = 5      # Replace with the actual number of features in your data\n",
    "num_classes = 12      # Replace with the actual number of classes\n",
    "\n",
    "# Build your model\n",
    "model_lstm = tf.keras.Sequential()\n",
    "model_lstm.add(tf.keras.layers.Masking(mask_value=0., input_shape=(max_seq_length, num_features)))\n",
    "model_lstm.add(tf.keras.layers.LSTM(128))\n",
    "model_lstm.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dddd20-cbb3-4444-98d7-10f3931ef518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into dataframe:\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02ef34-d54b-4619-bf4b-c2128e9bc171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "history_lstm = model_lstm.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d5e01-6b93-41af-af61-7ee2fa5827c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Combining the Models\n",
    "\n",
    "To create an ensemble, you can combine the predictions of these models in several ways:\n",
    "\t1.\tAveraging Probabilities:\n",
    "\t•\tObtain probability distributions over finishing positions from each model.\n",
    "\t•\tAverage the probabilities across models to get the final prediction.\n",
    "\t2.\tWeighted Averaging:\n",
    "\t•\tAssign weights to each model based on validation performance.\n",
    "\t•\tCompute a weighted average of the probabilities.\n",
    "\t3.\tStacking (Meta-Learner):\n",
    "\t•\tUse the predictions from the individual models as input features to a meta-model (e.g., a logistic regression or another neural network).\n",
    "\t•\tThe meta-model learns how to best combine the individual predictions.\n",
    "\t4.\tVoting (for Classification):\n",
    "\t•\tIf treating the problem as classification into discrete positions, use majority voting among the models.\n",
    "\t•\tNot as suitable if you need probability distributions.\n",
    "\n",
    "Implementation Steps\n",
    "\n",
    "1. Data Preparation\n",
    "\n",
    "\t•\tSequences:\n",
    "\t•\tUse the raw GPS data (gpspoint) to create sequences for each horse in each race.\n",
    "\t•\tEnsure that sequences are properly sorted by time_stamp.\n",
    "\t•\tFeatures:\n",
    "\t•\tInclude raw features such as speed, progress, stride_frequency.\n",
    "\t•\tAvoid hand-engineering features like acceleration to adhere to your objective.\n",
    "\t•\tLabels:\n",
    "\t•\tUse official_fin from results_entries as the target variable.\n",
    "\t•\tSince you want probabilities for each finishing position, consider encoding official_fin as categorical labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdede23-c3ae-4f35-9620-a281a6c599c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
