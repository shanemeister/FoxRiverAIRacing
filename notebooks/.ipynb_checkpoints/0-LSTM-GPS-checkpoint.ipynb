{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db68106",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "Plan is to model a horseâ€™s per-race sequence of positions/velocities/accelerations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d976bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment\n",
    "\n",
    "# Setup Environment\n",
    "import time\n",
    "from optuna.importance import MeanDecreaseImpurityImportanceEvaluator\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import optuna.visualization as viz\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import (col, count, row_number, abs, unix_timestamp, mean, \n",
    "                                   when, lit, min as F_min, max as F_max , upper, trim,\n",
    "                                   row_number, mean as F_mean, countDistinct, last, first, when)\n",
    "from src.data_preprocessing.data_prep1.data_utils import initialize_environment\n",
    "from src.data_preprocessing.data_prep1.data_loader import load_data_from_postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff7422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "spark, jdbc_url, jdbc_properties, parquet_dir, log_file = initialize_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c779b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gps_sql_queries():\n",
    "    queries = {\n",
    "        \"gps_horse\": \"\"\"\n",
    "            SELECT g.course_cd, g.race_date,g.race_number,\n",
    "            REGEXP_REPLACE(TRIM(UPPER(saddle_cloth_number)), '\\s+$', '') AS saddle_cloth_number, time_stamp, \n",
    "            longitude, latitude, speed, progress, stride_frequency, g.post_time, location,\n",
    "            re.axciskey, h.horse_id, re.official_fin, h.horse_name\n",
    "            FROM gpspoint g\n",
    "            JOIN results_entries re on g.course_cd = re.course_cd\n",
    "                AND g.race_date = re.race_date\n",
    "                AND g.race_number = re.race_number\n",
    "                AND g.saddle_cloth_number = re.program_num\n",
    "            JOIN horse h on re.axciskey = h.axciskey        \n",
    "            \"\"\",\n",
    "        \"sectionals\": \"\"\"\n",
    "            SELECT REGEXP_REPLACE(TRIM(UPPER(course_cd)), '\\s+$', '') AS course_cd, race_date, \n",
    "            race_number, REGEXP_REPLACE(TRIM(UPPER(saddle_cloth_number)), '\\s+$', '') AS saddle_cloth_number, \n",
    "            gate_name, gate_numeric,\n",
    "                length_to_finish, sectional_time, running_time, distance_back, distance_ran,\n",
    "                number_of_strides, post_time\n",
    "            FROM sectionals\n",
    "            \"\"\"\n",
    "    }\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d1447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- course_cd: string (nullable = true)\n",
      " |-- race_date: date (nullable = true)\n",
      " |-- race_number: integer (nullable = true)\n",
      " |-- saddle_cloth_number: string (nullable = true)\n",
      " |-- time_stamp: timestamp (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- speed: double (nullable = true)\n",
      " |-- progress: double (nullable = true)\n",
      " |-- stride_frequency: double (nullable = true)\n",
      " |-- post_time: timestamp (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- axciskey: string (nullable = true)\n",
      " |-- horse_id: integer (nullable = true)\n",
      " |-- official_fin: integer (nullable = true)\n",
      " |-- horse_name: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- course_cd: string (nullable = true)\n",
      " |-- race_date: date (nullable = true)\n",
      " |-- race_number: integer (nullable = true)\n",
      " |-- saddle_cloth_number: string (nullable = true)\n",
      " |-- gate_name: string (nullable = true)\n",
      " |-- gate_numeric: double (nullable = true)\n",
      " |-- length_to_finish: double (nullable = true)\n",
      " |-- sectional_time: double (nullable = true)\n",
      " |-- running_time: double (nullable = true)\n",
      " |-- distance_back: double (nullable = true)\n",
      " |-- distance_ran: double (nullable = true)\n",
      " |-- number_of_strides: double (nullable = true)\n",
      " |-- post_time: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# queries = gps_sql_queries()\n",
    "# dfs = load_data_from_postgresql(spark, jdbc_url, jdbc_properties, queries, parquet_dir)\n",
    "#         # Suppose we have a dictionary of queries\n",
    "# for name, df in dfs.items():\n",
    "#     logging.info(f\"DataFrame '{name}' loaded. Schema:\")\n",
    "#     df.printSchema()\n",
    "#     if name == \"gps_horse\":\n",
    "#         gps_horse_df = df\n",
    "#     elif name == \"sectionals\":\n",
    "#         sectionals_df = df    \n",
    "#     else:\n",
    "#         logging.error(f\"Unknown DataFrame name: {name}\")\n",
    "#         continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d54ab74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "gps_horse_df.write.mode(\"overwrite\").parquet(f\"{parquet_dir}/gps_horse_df\")\n",
    "sectionals_df.write.mode(\"overwrite\").parquet(f\"{parquet_dir}/sectionals_df\")\n",
    "logging.info(f\"Data written to Parquet in {time.time() - start_time:.2f} seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_horse_df = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/gps_horse_df\")\n",
    "sectionals_df = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/sectionals_df\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
