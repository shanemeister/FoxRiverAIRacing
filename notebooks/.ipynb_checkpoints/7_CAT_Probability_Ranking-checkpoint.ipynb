{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a43fc8b",
   "metadata": {},
   "source": [
    "# CATBoost Model Preparation\n",
    "\n",
    "A lot of work has gone into compiling the current dataset. I have merged the gps_df, sectionals_df and results_df. I have limited the amount of Equibase data I am using just to keep the focus on the TPD GPS data, and to do some feature engineering.  However, there are some good metrics from the Equibase data that are just basic measures that could be obtained from any racebook sheet. \n",
    "\n",
    "## Get Started\n",
    "\n",
    "1. Going to load the parquet DataFrame from disk and do some imputation, one-hot encoding, string indexing, and scaling. The run it through XBBoost to see how it's looking. At this point I will do the integration of route data, and add the GPS aggregations. I just want to see what I can minimally do and how its working before I go down the wrong path. If the XGBoost doesn't do any better than the LSTM, at least I won't have wasted any more time on it. \n",
    "\n",
    "### Model Additional Requirements\n",
    "\n",
    "#### Logistic Regression:\n",
    "> Ensure features are scaled (e.g., StandardScaler) and that categorical variables are one-hot encoded.\n",
    "\n",
    "#### Random Forest\t\n",
    "> Scaling is unnecessary, and categorical variables should be one-hot encoded.\n",
    "\n",
    "#### XGBoost/LightGBM\t\n",
    "> Scaling is unnecessary, and categorical variables should be one-hot encoded.\n",
    "\n",
    "#### Support Vector Machines (SVM)\t\n",
    ">Requires scaling and one-hot encoding.\n",
    "\n",
    "#### k-Nearest Neighbors\t\n",
    ">Requires scaling and one-hot encoding.\n",
    "\n",
    "#### Multi-Layer Perceptron (MLP)\n",
    "> Requires scaling and one-hot encoding.\n",
    "\n",
    "#### CatBoost\t\n",
    "> No need for one-hot encoding; you can specify categorical columns directly using CatBoostâ€™s cat_features parameter.\n",
    "\n",
    "\n",
    "### Load master_results_df.parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ed99b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install catboost -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1463424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pyspark.sql.functions as F\n",
    "import xgboost as xgb\n",
    "from sklearn import set_config\n",
    "from pyspark.sql.functions import (col, count, row_number, abs, unix_timestamp, mean, \n",
    "                                   when, lit, min as F_min, max as F_max , \n",
    "                                   row_number, mean as F_mean, countDistinct, last, first, when)\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession\n",
    "from src.data_preprocessing.data_prep1.sql_queries import sql_queries\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame, Window\n",
    "from src.data_preprocessing.data_prep1.data_utils import (save_parquet, gather_statistics, \n",
    "                initialize_environment, load_config, initialize_logging, initialize_spark, \n",
    "                identify_and_impute_outliers, \n",
    "                identify_and_remove_outliers, identify_missing_and_outliers)\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Set global references to None\n",
    "spark = None\n",
    "master_results_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73671a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 10:40:26,605 - INFO - Environment setup initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark, jdbc_url, jdbc_properties, queries, parquet_dir, log_file = initialize_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c57a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(os.path.join(parquet_dir, \"results_only_clean.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ccd5955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "777100"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce2ed82",
   "metadata": {},
   "source": [
    "# Switching to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e3b985b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert Spark DataFrame -> Pandas DataFrame\n",
    "\n",
    "rf_df = df.toPandas()\n",
    "df = None\n",
    "# Quick info about the DataFrame\n",
    "#print(df.info())\n",
    "#print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92eb1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose the finish position is in a column named 'official_fin'\n",
    "# Create a binary label: 1 = first place, 0 = others\n",
    "#df[\"label\"] = (df[\"official_fin\"] == 1).astype(int)\n",
    "\n",
    "# Check distribution of the label\n",
    "#print(df[\"label\"].value_counts())\n",
    "\n",
    "# Define a function to map official_fin to label\n",
    "def map_official_fin_to_label(official_fin):\n",
    "    if official_fin == 1:\n",
    "        return 0  # Win\n",
    "    elif official_fin == 2:\n",
    "        return 1  # Place\n",
    "    elif official_fin == 3:\n",
    "        return 2  # Show\n",
    "    elif official_fin == 4:\n",
    "        return 3  # Fourth\n",
    "    elif official_fin == 5:\n",
    "        return 4  # Fifth\n",
    "    elif official_fin == 6:\n",
    "        return 5  # Sixth\n",
    "    elif official_fin == 7:\n",
    "        return 6  # Seventh\n",
    "    else:\n",
    "        return 7  # Outside top-7\n",
    "\n",
    "# Apply the function to create the label column\n",
    "rf_df['label'] = rf_df['official_fin'].apply(map_official_fin_to_label)\n",
    "\n",
    "# Check the DataFrame\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7eb90e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a) Identify columns with high missingness\n",
    "missing_summary = rf_df.isna().sum().sort_values(ascending=False)\n",
    "#print(\"Missing Value Summary:\\n\", missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f3dade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Descriptive Stats:\\n\", df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0d9caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick correlation matrix\n",
    "# corr_matrix = df.corr(numeric_only=True)\n",
    "# print(\"Correlation Matrix:\\n\", corr_matrix[\"label\"].sort_values(ascending=False))\n",
    "\n",
    "# # Possibly visualize\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(corr_matrix, cmap=\"coolwarm\")\n",
    "# plt.title(\"Correlation Heatmap\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84383e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Histograms for numeric columns\n",
    "# df.hist(bins=30, figsize=(15,10))\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d071d9a",
   "metadata": {},
   "source": [
    "# OHE\n",
    "\n",
    "> Required for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a39d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b) Convert categorical columns to numeric dummies\n",
    "# categorical_cols = [\"course_cd\", \n",
    "#                     \"equip\", \n",
    "#                     \"surface\", \n",
    "#                     \"trk_cond\", \n",
    "#                     \"weather\", \n",
    "#                     \"med\", \n",
    "#                     \"stk_clm_md\", \n",
    "#                     \"turf_mud_mark\", \n",
    "#                     \"race_type\"]\n",
    "\n",
    "# # Perform one-hot encoding on the categorical columns\n",
    "# df_encoded = pd.get_dummies(rf_df, columns=categorical_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f701e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\"course_cd\", \n",
    "                    \"equip\", \n",
    "                    \"surface\", \n",
    "                    \"trk_cond\", \n",
    "                    \"weather\", \n",
    "                    \"med\", \n",
    "                    \"stk_clm_md\", \n",
    "                    \"turf_mud_mark\", \n",
    "                    \"race_type\"]\n",
    "\n",
    "metadata = rf_df[[\"race_date\", \"race_number\", \"horse_id\"]]  # Replace with your metadata columns\n",
    "\n",
    "feature_cols = [\"morn_odds\", \"net_sentiment\", \"power\", \"avg_spd_sd\", \"hi_spd_sd\",\n",
    "                \"avgspd\", \"ave_cl_sd\", \"cond_win\", \"cond_place\", \"all_win\", \"all_place\",\n",
    "                \"cond_earnings\", \"all_earnings\", \"weight\", \"cond_show\", \"all_show\", \n",
    "                \"cond_starts\", \"all_starts\", \"class_rating\", \"age_at_race_day\", \"distance\",\n",
    "                \"horse_id\", \"claimprice\", \"wps_pool\", \"cond_fourth\", \"all_fourth\", \n",
    "                \"purse\", \"pstyerl\", \"race_number\", \"start_position\"]\n",
    "\n",
    "# Separate the label from the features\n",
    "y = rf_df[\"label\"].values\n",
    "\n",
    "# Combine the encoded features with the numeric features\n",
    "X = rf_df[feature_cols + categorical_cols].copy()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test, metadata_train, metadata_test = train_test_split(\n",
    "    X, y, metadata, test_size=0.20, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d9f7b2",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "\n",
    ">Unnecessary for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd8a47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Scale numeric features\n",
    "# # Not necessary for Random Forest\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Check the scaled features\n",
    "# print(X_train_scaled)\n",
    "# print(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842de6d",
   "metadata": {},
   "source": [
    "# CATBoost Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b51ef4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    105776\n",
      "1    105638\n",
      "2    105525\n",
      "3    105107\n",
      "4    102116\n",
      "7     95879\n",
      "5     89994\n",
      "6     67065\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the distribution of the labels\n",
    "label_distribution = rf_df['label'].value_counts()\n",
    "print(label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a326fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# # Assuming y_train contains the labels\n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "# class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Use 'balanced' which automatically computes the class weights based on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "922d4e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 415\n"
     ]
    }
   ],
   "source": [
    "# Indices of categorical columns\n",
    "cat_features = [X.columns.get_loc(c) for c in categorical_cols]\n",
    "\n",
    "# Initialize CatBoostClassifier\n",
    "cat_model = CatBoostClassifier(\n",
    "    task_type='GPU',      # GPU training\n",
    "    iterations=1000,\n",
    "    cat_features=cat_features,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "cat_model.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "\n",
    "# Check the model's performance\n",
    "print(\"Best iteration:\", cat_model.best_iteration_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7aeff08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[10961  2861  1983  1561  1211   539    30  2009]\n",
      " [ 8091  3074  2388  2220  1870   823    44  2618]\n",
      " [ 6021  2789  2546  2728  2551  1269    60  3141]\n",
      " [ 4525  2322  2365  2906  3318  1736    75  3774]\n",
      " [ 3358  1823  2028  2823  3646  2171   107  4467]\n",
      " [ 2370  1242  1483  2016  3296  2360   155  5077]\n",
      " [ 1437   758   866  1112  1883  1889   115  5353]\n",
      " [ 1384   682   769   838  1344  1498   119 12542]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.52      0.37     21155\n",
      "           1       0.20      0.15      0.17     21128\n",
      "           2       0.18      0.12      0.14     21105\n",
      "           3       0.18      0.14      0.16     21021\n",
      "           4       0.19      0.18      0.18     20423\n",
      "           5       0.19      0.13      0.16     17999\n",
      "           6       0.16      0.01      0.02     13413\n",
      "           7       0.32      0.65      0.43     19176\n",
      "\n",
      "    accuracy                           0.25    155420\n",
      "   macro avg       0.21      0.24      0.20    155420\n",
      "weighted avg       0.22      0.25      0.21    155420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "y_pred = cat_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b66f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf_model.feature_importances_\n",
    "for col, imp in zip(feature_cols, importances):\n",
    "    print(f\"{col}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939f420",
   "metadata": {},
   "source": [
    "# Predicting Probabilities for Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Predict Probabilities\n",
    "predicted_probs = rf_model.predict_proba(X_test_scaled)[:, 1]  # Probability for winning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8963293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Combine Metadata with Predictions\n",
    "ranked_df = metadata_test.copy()\n",
    "ranked_df[\"predicted_probability\"] = predicted_probs\n",
    "\n",
    "# Step 3: Rank Horses\n",
    "ranked_df[\"rank\"] = (\n",
    "    ranked_df.groupby([\"race_date\", \"race_number\"])[\"predicted_probability\"]\n",
    "    .rank(method=\"first\", ascending=False)\n",
    ")\n",
    "\n",
    "# Step 4: Sort Ranked DataFrame\n",
    "ranked_df = ranked_df.sort_values(by=[\"race_date\", \"race_number\", \"rank\"])\n",
    "\n",
    "# Step 5: Save or Display Results\n",
    "print(ranked_df.head(20))  # Display top 20 ranked horses\n",
    "ranked_df.to_csv(\"ranked_horses.csv\", index=False)  # Save to CSV if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b85ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X_test corresponds to the feature matrix for your test set\n",
    "# And you have additional columns `race_date`, `race_number`, `horse_id` in the original data.\n",
    "\n",
    "# Step 1: Predict Probabilities\n",
    "proba = rf_model.predict_proba(X_test_scaled)  # Shape: (n_samples, 2)\n",
    "predicted_probs = proba[:, 1]  # Probability for class 1 (winning)\n",
    "\n",
    "# Step 2: Create a DataFrame for `X_test` with metadata\n",
    "# Replace with actual metadata (e.g., from original test data before splitting)\n",
    "metadata = pd.DataFrame({\n",
    "    \"race_date\": race_date,  # Replace with actual race dates\n",
    "    \"race_number\": race_number,  # Replace with actual race numbers\n",
    "    \"horse_id\": horse_id,  # Replace with actual horse IDs\n",
    "})\n",
    "\n",
    "# Combine predicted probabilities with metadata\n",
    "ranked_df = metadata.copy()\n",
    "ranked_df[\"predicted_probability\"] = predicted_probs\n",
    "\n",
    "# Step 3: Rank Horses\n",
    "# Group by race_date and race_number, then rank by predicted_probability\n",
    "ranked_df[\"rank\"] = (\n",
    "    ranked_df.groupby([\"race_date\", \"race_number\"])[\"predicted_probability\"]\n",
    "    .rank(method=\"first\", ascending=False)\n",
    ")\n",
    "\n",
    "# Step 4: Sort Ranked DataFrame\n",
    "ranked_df = ranked_df.sort_values(by=[\"race_date\", \"race_number\", \"rank\"])\n",
    "\n",
    "# Step 5: Save or Display Results\n",
    "print(ranked_df.head(20))  # Display top 20 ranked horses\n",
    "ranked_df.to_csv(\"ranked_horses.csv\", index=False)  # Save to CSV if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca5f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_df = metadata_test.copy()  # Metadata includes race_date, race_number, horse_id\n",
    "ranked_df[\"predicted_probability\"] = predicted_probs\n",
    "ranked_df[\"actual_label\"] = y_test  # Optional: for evaluation purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b920b06",
   "metadata": {},
   "source": [
    "## Predict via Ranking using Test Race from Dataset\n",
    "\n",
    "You can test the model on a race in your dataset by excluding the target variable (official_fin) and making predictions as if it were a new race. Hereâ€™s how you can proceed:\n",
    "\n",
    "Step 1: Select a Race for Testing\n",
    "\n",
    "Extract a specific race from your dataset based on race_date and race_number.\n",
    "\n",
    "race_to_test = df[\n",
    "    (df[\"race_date\"] == \"2023-05-15\") & (df[\"race_number\"] == 5)\n",
    "].drop(columns=[\"official_fin\"])  # Drop the target variable\n",
    "\n",
    "Step 2: Prepare the Features\n",
    "\n",
    "Ensure that the extracted race has the same feature processing (scaling, encoding) as was done during training.\n",
    "\n",
    "# Extract features\n",
    "X_race = race_to_test[feature_cols]  # Ensure `feature_cols` matches the training feature set\n",
    "\n",
    "# Scale features\n",
    "X_race_scaled = scaler.transform(X_race)  # Use the scaler fitted during training\n",
    "\n",
    "Step 3: Predict Probabilities\n",
    "\n",
    "Use the model to predict probabilities for this specific race.\n",
    "\n",
    "# Predict probabilities\n",
    "race_probs = rf_model.predict_proba(X_race_scaled)\n",
    "\n",
    "# Attach probabilities back to the metadata\n",
    "race_to_test[\"predicted_probability\"] = race_probs[:, 1]  # Assuming class 1 is 'winning'\n",
    "\n",
    "# Rank horses by predicted probability\n",
    "race_to_test[\"rank\"] = race_to_test[\"predicted_probability\"].rank(ascending=False)\n",
    "race_to_test = race_to_test.sort_values(by=\"rank\")\n",
    "\n",
    "Step 4: Compare to Actual Results\n",
    "\n",
    "If you still have the actual official_fin values in a backup, compare the modelâ€™s ranking against the real results.\n",
    "\n",
    "# Add actual finish positions for comparison (if available)\n",
    "actual_results = df[\n",
    "    (df[\"race_date\"] == \"2023-05-15\") & (df[\"race_number\"] == 5)\n",
    "][[\"horse_id\", \"official_fin\"]]\n",
    "\n",
    "race_to_test = race_to_test.merge(actual_results, on=\"horse_id\", how=\"left\")\n",
    "\n",
    "# Display results\n",
    "print(race_to_test[[\"horse_id\", \"predicted_probability\", \"rank\", \"official_fin\"]])\n",
    "\n",
    "Testing Without horse_id, race_date, or race_number\n",
    "\n",
    "If you remove horse_id, race_date, or race_number, the model should still be able to make predictions if those columns are not part of the feature set. However, you wonâ€™t be able to group or rank the predictions by race because race_date and race_number are critical for distinguishing horses in the same race.\n",
    "\n",
    "To simulate a future race:\n",
    "\t1.\tSelect a race that the model hasnâ€™t seen during training (e.g., from a holdout set).\n",
    "\t2.\tRemove any identifying metadata (e.g., horse_id, race_date, race_number).\n",
    "\t3.\tProcess the features the same way as during training.\n",
    "\t4.\tUse the model to predict probabilities for the horses in that race.\n",
    "\t5.\tRank the predictions by probability.\n",
    "\n",
    "Testing with a Future Race\n",
    "\n",
    "For a future race:\n",
    "\t1.\tCollect horse features for that race (e.g., from Equibase).\n",
    "\t2.\tProcess the data the same way as the training data.\n",
    "\t3.\tUse the model to make predictions.\n",
    "\t4.\tRank horses by predicted probabilities.\n",
    "\t5.\tWait for the race results and compare the modelâ€™s rankings to the actual outcomes.\n",
    "\n",
    "This approach ensures the model is evaluated on unseen data, simulating a real-world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea8560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9d5cf9c",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69c255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f05de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", 0.5],\n",
    "    \"class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",    # or \"f1\", \"balanced_accuracy\", etc.\n",
    "    cv=3,                  # 3-fold cross-validation\n",
    "    n_jobs=-1             # use all CPU cores\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Params:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Then evaluate on the test set:\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_best = best_rf.predict(X_test)\n",
    "print(\"Final Test Accuracy:\", (y_pred_best == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a0649f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486d1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb16452a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
