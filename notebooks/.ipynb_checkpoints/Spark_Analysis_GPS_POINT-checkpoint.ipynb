{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d31374ea-f808-4394-a60c-c3f0a08e2627",
   "metadata": {},
   "source": [
    "# Spark Anaysis of Total Performance Data: GPS and Sectional Data\n",
    "\n",
    "Analyzing the TPD GPS data alongside Equibase (EQB) data can provide significant predictive insights into horse performance. By focusing on the granular details of a horse’s movement during a race, you can derive valuable metrics that complement traditional EQB ratings and help identify under- or over-rated horses. \n",
    "\n",
    "## Roadmap for maximizing the predictive capabilities of the TPD GPS data:\n",
    "\n",
    "### Key Ideas and Strategies\n",
    "\n",
    "1. Derive Advanced Pace Metrics\n",
    "\n",
    "Understanding how a horse’s speed changes over the course of a race can reveal its racing style and potential strengths or weaknesses:\n",
    "\n",
    "\t•\tEarly Pace: Average speed and acceleration during the first segment (e.g., first 20% of the race).\n",
    "\t•\tMid-Race Pace: Average speed and deceleration during the middle segments.\n",
    "\t•\tLate Pace: Average speed and deceleration in the final segment.\n",
    "\t•\tSustained Speed: Identify segments where the horse maintains a steady speed.\n",
    "\t•\tPeak Speed Timing: The point in the race where the horse reaches its peak speed.\n",
    "\n",
    "2. Fatigue Factor\n",
    "\n",
    "Calculate how much the horse slows down as the race progresses:\n",
    "\n",
    "\t•\tUse metrics like:\n",
    "\t•\tPercentage drop from peak speed to finish speed.\n",
    "\t•\tMaximum acceleration vs. deceleration ratios.\n",
    "\t•\tChange in stride frequency as the race progresses.\n",
    "\n",
    "3. Sectional Efficiency\n",
    "\n",
    "Quantify how efficiently the horse runs its sections:\n",
    "\n",
    "    •\tCompare actual times vs. expected times for each section based on the route characteristics.\n",
    "\t•\tEfficiency Ratio:\n",
    "    •\tA high ratio might indicate a horse ran extra distance due to poor cornering or positioning.\n",
    "\n",
    "4. Overlay TPD with EQB Ratings\n",
    "\n",
    "Use EQB’s traditional metrics (e.g., speed ratings, form) to cross-reference with TPD data:\n",
    "\n",
    "\t•\tIdentify horses that consistently outperform EQB predictions.\n",
    "\t•\tInvestigate horses with high EQB ratings but poor TPD-based performance (e.g., poor fatigue factors or inefficient sectional running).\n",
    "\n",
    "5. Route Characteristics\n",
    "\n",
    "If the routes table contains track-specific details (e.g., turn sharpness, surface type, gradient):\n",
    "\n",
    "\t•\tIncorporate these into the analysis.\n",
    "\t•\tEvaluate how specific horses handle different track conditions (e.g., wide turns, long stretches).\n",
    "\t•\tIdentify patterns like “performs better on flatter tracks” or “struggles on uphill finishes.”\n",
    "\n",
    "6. Horse vs. Peer Comparisons\n",
    "\n",
    "Evaluate how each horse performs relative to its competition in the same race:\n",
    "\n",
    "\t•\tCompare sectional times and speeds with other horses in the race.\n",
    "\t•\tRank horses based on performance within each race segment.\n",
    "\n",
    "7. Acceleration Profiles\n",
    "\n",
    "\t•\tPlot acceleration over time to identify patterns (e.g., burst speed vs. steady acceleration).\n",
    "    •\tHighlight horses with exceptional closing speed (valuable in longer races) or fast starts (important in short sprints).\n",
    "\n",
    "9. Cluster Analysis of Racing Styles\n",
    "\n",
    "Use clustering techniques to group horses by similar racing profiles:\n",
    "\n",
    "\t•\tInputs: Early pace, mid-pace, late pace, fatigue factor, sectional efficiency.\n",
    "\t•\tOutput: Clusters representing different racing styles (e.g., “early speed burners,” “closers,” “steady sustainers”).\n",
    "\n",
    "9. Historical Analysis\n",
    "\n",
    "Identify trends over a horse’s career:\n",
    "\n",
    "\t•\tDoes the horse improve or decline over time?\n",
    "\t•\tAre there patterns in performance tied to specific jockeys, trainers, or race conditions?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b2d7b-00ae-4a2c-9081-e27a7e1e1573",
   "metadata": {},
   "source": [
    "# Using Spark for Efficient Processing\n",
    "\n",
    "Steps to Implement\n",
    "\n",
    "\t1.\tLoad the Data\n",
    "\t•\tLoad gpspoint, gps_aggregated_results, and routes into Spark DataFrames.\n",
    "\t2.\tSegment the Race\n",
    "\t•\tDivide each race into sections (e.g., by distance markers or time intervals).\n",
    "\t•\tUse PARTITION BY in Spark to process horses within each race separately.\n",
    "\t3.\tDerive Metrics\n",
    "\t•\tSpeed Metrics: Use window functions to calculate average, min, max speeds.\n",
    "\t•\tAcceleration/Deceleration: Compute using differences in speed and timestamps.\n",
    "\t•\tEfficiency: Calculate distance ran vs. track distance.\n",
    "\t4.\tIntegrate with EQB Data\n",
    "\t•\tJoin with EQB tables on course_cd, race_date, and race_number for comparison.\n",
    "\t5.\tSave Aggregated Data\n",
    "\t•\tSave results into gps_aggregated_results and tpd_features.\n",
    "\n",
    "Example Spark Code\n",
    "\n",
    "Here’s a high-level implementation for deriving pace metrics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c520e11-da4a-43c1-8db8-33f2c1ac3771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import configparser\n",
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "def setup_logging(script_dir, log_dir=None):\n",
    "    \"\"\"Sets up logging configuration to write logs to a file and the console.\"\"\"\n",
    "    try:\n",
    "        # Default log directory\n",
    "        if not log_dir:\n",
    "            log_dir = '/home/exx/myCode/horse-racing/FoxRiverAIRacing/logs'\n",
    "        \n",
    "        # Ensure the log directory exists\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        log_file = os.path.join(log_dir, 'SparkPy_load.log')\n",
    "\n",
    "        # Create a logger and clear existing handlers\n",
    "        logger = logging.getLogger()\n",
    "        if logger.hasHandlers():\n",
    "            logger.handlers.clear()\n",
    "\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "        # Create file handler\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "\n",
    "        # Create console handler\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "\n",
    "        # Define a common format\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(levelname)s - %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        file_handler.setFormatter(formatter)\n",
    "        console_handler.setFormatter(formatter)\n",
    "\n",
    "        # Add handlers to the logger\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "        logger.info(\"Logging has been set up successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to set up logging: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "def read_config(config_file_path=\"config.ini\"):\n",
    "    \"\"\"Read database configuration from config.ini.\"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file_path)\n",
    "    if 'database' not in config:\n",
    "        raise KeyError(\"Database configuration missing in config.ini\")\n",
    "    return config['database']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2c25a67a-9e31-4552-89be-efc2522a79d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkPy24!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DB_PASSWORD\"] = \"SparkPy24!\"\n",
    "print(os.getenv(\"DB_PASSWORD\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a55500b7-732c-441a-aec4-c21b190ba7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting Spark Processing for GPS Data\n",
      "INFO:root:Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, lag, avg, max, min, stddev, expr, unix_timestamp\n",
    "import logging\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Suppress INFO and WARN logs\n",
    "spark = SparkSession.builder.appName(\"GPS Sectionals Analysis with Enhanced Metrics\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Reduce Py4J logging\n",
    "logger = logging.getLogger(\"py4j\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "log_file = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/logs/SparkPy_load.log\"\n",
    "logging.basicConfig(level=logging.INFO, handlers=[\n",
    "    logging.FileHandler(log_file),\n",
    "    logging.StreamHandler()\n",
    "])\n",
    "logging.info(\"Starting Spark Processing for GPS Data\")\n",
    "\n",
    "db_config = {\n",
    "    \"host\": \"192.168.4.25\",\n",
    "    \"port\": \"5433\",\n",
    "    \"dbname\": \"foxriverai\",\n",
    "    \"user\": \"rshane\",\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\")  # Fetch from environment variable\n",
    "}\n",
    "\n",
    "if not db_config[\"password\"]:\n",
    "    raise Exception(\"Database password is missing. Set it in the DB_PASSWORD environment variable.\")\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{db_config['host']}:{db_config['port']}/{db_config['dbname']}\"\n",
    "jdbc_properties = {\n",
    "    \"user\": db_config[\"user\"],\n",
    "    \"password\": db_config[\"password\"],\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "jdbc_driver_path = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/jdbc/postgresql-42.7.4.jar\"\n",
    "\n",
    "logging.info(\"Spark session created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7b8f4de7-4b25-4f89-bf57-eddc44c68731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- progress: double (nullable = true)\n",
      " |-- speed: double (nullable = true)\n",
      " |-- stride_frequency: double (nullable = true)\n",
      " |-- g_course_cd: string (nullable = true)\n",
      " |-- g_race_date: date (nullable = true)\n",
      " |-- g_race_number: integer (nullable = true)\n",
      " |-- g_saddle_cloth_number: string (nullable = true)\n",
      " |-- time_stamp: timestamp (nullable = true)\n",
      " |-- gate_name: string (nullable = true)\n",
      " |-- length_to_finish: double (nullable = true)\n",
      " |-- sectional_time: double (nullable = true)\n",
      " |-- running_time: double (nullable = true)\n",
      " |-- distance_back: double (nullable = true)\n",
      " |-- distance_ran: double (nullable = true)\n",
      " |-- number_of_strides: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gps_with_metrics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67ed15e4-cdb0-49fa-ba8e-c15a7a04d57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing course: CNL\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `speed` cannot be resolved. Did you mean one of the following? [`avg_speed`, `max_speed`, `min_speed`, `course_cd`, `gate_name`].;\n'Aggregate [course_cd#2332743, race_date#2332744, race_number#2332745, saddle_cloth_number#2332746, gate_name#2332617, avg_speed#2332763, max_speed#2332765, min_speed#2332767, avg_stride_freq#2332769, max_stride_freq#2332771, speed_stddev#2332772, data_points#2332774L, avg_speed_change#2332804, max_speed_increase#2332818, max_speed_decrease#2332833, fatigue_factor#2332849, (max('speed) - min('speed)) AS speed_range#2332868]\n+- Project [course_cd#2332743, race_date#2332744, race_number#2332745, saddle_cloth_number#2332746, gate_name#2332617, avg_speed#2332763, max_speed#2332765, min_speed#2332767, avg_stride_freq#2332769, max_stride_freq#2332771, speed_stddev#2332772, data_points#2332774L, avg_speed_change#2332804, max_speed_increase#2332818, max_speed_decrease#2332833, null AS fatigue_factor#2332849]\n   +- Project [course_cd#2332743, race_date#2332744, race_number#2332745, saddle_cloth_number#2332746, gate_name#2332617, avg_speed#2332763, max_speed#2332765, min_speed#2332767, avg_stride_freq#2332769, max_stride_freq#2332771, speed_stddev#2332772, data_points#2332774L, avg_speed_change#2332804, max_speed_increase#2332818, null AS max_speed_decrease#2332833]\n      +- Project [course_cd#2332743, race_date#2332744, race_number#2332745, saddle_cloth_number#2332746, gate_name#2332617, avg_speed#2332763, max_speed#2332765, min_speed#2332767, avg_stride_freq#2332769, max_stride_freq#2332771, speed_stddev#2332772, data_points#2332774L, avg_speed_change#2332804, null AS max_speed_increase#2332818]\n         +- Project [course_cd#2332743, race_date#2332744, race_number#2332745, saddle_cloth_number#2332746, gate_name#2332617, avg_speed#2332763, max_speed#2332765, min_speed#2332767, avg_stride_freq#2332769, max_stride_freq#2332771, speed_stddev#2332772, data_points#2332774L, null AS avg_speed_change#2332804]\n            +- Aggregate [g_course_cd#2332678, g_race_date#2332679, g_race_number#2332680, g_saddle_cloth_number#2332681, gate_name#2332617], [g_course_cd#2332678 AS course_cd#2332743, g_race_date#2332679 AS race_date#2332744, g_race_number#2332680 AS race_number#2332745, g_saddle_cloth_number#2332681 AS saddle_cloth_number#2332746, gate_name#2332617, avg(speed#2332656) AS avg_speed#2332763, max(speed#2332656) AS max_speed#2332765, min(speed#2332656) AS min_speed#2332767, avg(stride_frequency#2332658) AS avg_stride_freq#2332769, max(stride_frequency#2332658) AS max_stride_freq#2332771, stddev(speed#2332656) AS speed_stddev#2332772, count(1) AS data_points#2332774L]\n               +- Project [progress#2332657, speed#2332656, stride_frequency#2332658, g_course_cd#2332678, g_race_date#2332679, g_race_number#2332680, g_saddle_cloth_number#2332681, time_stamp#2332652, gate_name#2332617, length_to_finish#2332618, sectional_time#2332619, running_time#2332620, distance_back#2332621, distance_ran#2332622, number_of_strides#2332623]\n                  +- Join Inner, ((((g_course_cd#2332678 = s_course_cd#2332637) AND (g_race_date#2332679 = s_race_date#2332638)) AND (g_race_number#2332680 = s_race_number#2332639)) AND (g_saddle_cloth_number#2332681 = s_saddle_cloth_number#2332640))\n                     :- Project [progress#2332657, speed#2332656, stride_frequency#2332658, course_cd#2332665 AS g_course_cd#2332678, race_date#2332661 AS g_race_date#2332679, race_number#2332662 AS g_race_number#2332680, saddle_cloth_number#2332664 AS g_saddle_cloth_number#2332681, time_stamp#2332652]\n                     :  +- Project [time_stamp#2332652, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, saddle_cloth_number#2332653, 3, true, false, true) AS saddle_cloth_number#2332664, longitude#2332654, latitude#2332655, speed#2332656, progress#2332657, stride_frequency#2332658, location#2332659, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, course_cd#2332660, 3, true, false, true) AS course_cd#2332665, race_date#2332661, race_number#2332662, last_updated#2332663]\n                     :     +- Relation [time_stamp#2332652,saddle_cloth_number#2332653,longitude#2332654,latitude#2332655,speed#2332656,progress#2332657,stride_frequency#2332658,location#2332659,course_cd#2332660,race_date#2332661,race_number#2332662,last_updated#2332663] JDBCRelation(gpspoint) [numPartitions=1]\n                     +- Project [course_cd#2332625 AS s_course_cd#2332637, race_date#2332615 AS s_race_date#2332638, race_number#2332616 AS s_race_number#2332639, saddle_cloth_number#2332624 AS s_saddle_cloth_number#2332640, gate_name#2332617, length_to_finish#2332618, sectional_time#2332619, running_time#2332620, distance_back#2332621, distance_ran#2332622, number_of_strides#2332623]\n                        +- Project [staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, saddle_cloth_number#2332613, 3, true, false, true) AS saddle_cloth_number#2332624, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, course_cd#2332614, 3, true, false, true) AS course_cd#2332625, race_date#2332615, race_number#2332616, gate_name#2332617, length_to_finish#2332618, sectional_time#2332619, running_time#2332620, distance_back#2332621, distance_ran#2332622, number_of_strides#2332623]\n                           +- Relation [saddle_cloth_number#2332613,course_cd#2332614,race_date#2332615,race_number#2332616,gate_name#2332617,length_to_finish#2332618,sectional_time#2332619,running_time#2332620,distance_back#2332621,distance_ran#2332622,number_of_strides#2332623] JDBCRelation(sectionals) [numPartitions=1]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 101\u001b[0m\n\u001b[1;32m     80\u001b[0m aggregated_metrics \u001b[38;5;241m=\u001b[39m joined_df\u001b[38;5;241m.\u001b[39mgroupBy(\n\u001b[1;32m     81\u001b[0m     col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg_course_cd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcourse_cd\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     82\u001b[0m     col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg_race_date\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrace_date\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m     count(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_points\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Optional: Count of data points used in calculation\u001b[39;00m\n\u001b[1;32m     94\u001b[0m )\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Add placeholders for other columns in the DDL if needed\u001b[39;00m\n\u001b[1;32m     97\u001b[0m aggregated_metrics \u001b[38;5;241m=\u001b[39m \u001b[43maggregated_metrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavg_speed_change\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_speed_increase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_speed_decrease\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfatigue_factor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m--> 101\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspeed_range\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_max\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspeed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF_min\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspeed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m    102\u001b[0m                                        \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstride_efficiency\u001b[39m\u001b[38;5;124m\"\u001b[39m, avg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstride_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m avg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeed\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Write to the target table\u001b[39;00m\n\u001b[1;32m    105\u001b[0m aggregated_metrics\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mjdbc(\n\u001b[1;32m    106\u001b[0m     url\u001b[38;5;241m=\u001b[39mjdbc_url,\n\u001b[1;32m    107\u001b[0m     table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgps_aggregated_results_with_ga\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    108\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Use append to add data for each course\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     properties\u001b[38;5;241m=\u001b[39mjdbc_properties\n\u001b[1;32m    110\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_env/envs/tf_gpu/lib/python3.11/site-packages/pyspark/sql/dataframe.py:5176\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5173\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5174\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   5175\u001b[0m     )\n\u001b[0;32m-> 5176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_env/envs/tf_gpu/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/mamba_env/envs/tf_gpu/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `speed` cannot be resolved. Did you mean one of the following? [`avg_speed`, `max_speed`, `min_speed`, `course_cd`, `gate_name`].;\n'Aggregate [course_cd#2332743, race_date#2332744, race_number#2332745, saddle_cloth_number#2332746, gate_name#2332617, avg_speed#2332763, max_speed#2332765, min_speed#2332767, avg_stride_freq#2332769, max_stride_freq#2332771, speed_stddev#2332772, data_points#2332774L, avg_speed_change#2332804, max_speed_increase#2332818, max_speed_decrease#2332833, fatigue_factor#2332849, (max('speed) - min('speed)) AS speed_range#2332868]\n+- Project [course_cd#2332743, race_date#2332744, race_number#2332745, saddle_cloth_number#2332746, gate_name#2332617, avg_speed#2332763, max_speed#2332765, min_speed#2332767, avg_stride_freq#2332769, max_stride_freq#2332771, speed_stddev#2332772, data_points#2332774L, avg_speed_change#2332804, max_speed_increase#2332818, max_speed_decrease#2332833, null AS fatigue_factor#2332849]\n   +- Project [course_cd#2332743, race_date#2332744, race_number#2332745, saddle_cloth_number#2332746, gate_name#2332617, avg_speed#2332763, max_speed#2332765, min_speed#2332767, avg_stride_freq#2332769, max_stride_freq#2332771, speed_stddev#2332772, data_points#2332774L, avg_speed_change#2332804, max_speed_increase#2332818, null AS max_speed_decrease#2332833]\n      +- Project [course_cd#2332743, race_date#2332744, race_number#2332745, saddle_cloth_number#2332746, gate_name#2332617, avg_speed#2332763, max_speed#2332765, min_speed#2332767, avg_stride_freq#2332769, max_stride_freq#2332771, speed_stddev#2332772, data_points#2332774L, avg_speed_change#2332804, null AS max_speed_increase#2332818]\n         +- Project [course_cd#2332743, race_date#2332744, race_number#2332745, saddle_cloth_number#2332746, gate_name#2332617, avg_speed#2332763, max_speed#2332765, min_speed#2332767, avg_stride_freq#2332769, max_stride_freq#2332771, speed_stddev#2332772, data_points#2332774L, null AS avg_speed_change#2332804]\n            +- Aggregate [g_course_cd#2332678, g_race_date#2332679, g_race_number#2332680, g_saddle_cloth_number#2332681, gate_name#2332617], [g_course_cd#2332678 AS course_cd#2332743, g_race_date#2332679 AS race_date#2332744, g_race_number#2332680 AS race_number#2332745, g_saddle_cloth_number#2332681 AS saddle_cloth_number#2332746, gate_name#2332617, avg(speed#2332656) AS avg_speed#2332763, max(speed#2332656) AS max_speed#2332765, min(speed#2332656) AS min_speed#2332767, avg(stride_frequency#2332658) AS avg_stride_freq#2332769, max(stride_frequency#2332658) AS max_stride_freq#2332771, stddev(speed#2332656) AS speed_stddev#2332772, count(1) AS data_points#2332774L]\n               +- Project [progress#2332657, speed#2332656, stride_frequency#2332658, g_course_cd#2332678, g_race_date#2332679, g_race_number#2332680, g_saddle_cloth_number#2332681, time_stamp#2332652, gate_name#2332617, length_to_finish#2332618, sectional_time#2332619, running_time#2332620, distance_back#2332621, distance_ran#2332622, number_of_strides#2332623]\n                  +- Join Inner, ((((g_course_cd#2332678 = s_course_cd#2332637) AND (g_race_date#2332679 = s_race_date#2332638)) AND (g_race_number#2332680 = s_race_number#2332639)) AND (g_saddle_cloth_number#2332681 = s_saddle_cloth_number#2332640))\n                     :- Project [progress#2332657, speed#2332656, stride_frequency#2332658, course_cd#2332665 AS g_course_cd#2332678, race_date#2332661 AS g_race_date#2332679, race_number#2332662 AS g_race_number#2332680, saddle_cloth_number#2332664 AS g_saddle_cloth_number#2332681, time_stamp#2332652]\n                     :  +- Project [time_stamp#2332652, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, saddle_cloth_number#2332653, 3, true, false, true) AS saddle_cloth_number#2332664, longitude#2332654, latitude#2332655, speed#2332656, progress#2332657, stride_frequency#2332658, location#2332659, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, course_cd#2332660, 3, true, false, true) AS course_cd#2332665, race_date#2332661, race_number#2332662, last_updated#2332663]\n                     :     +- Relation [time_stamp#2332652,saddle_cloth_number#2332653,longitude#2332654,latitude#2332655,speed#2332656,progress#2332657,stride_frequency#2332658,location#2332659,course_cd#2332660,race_date#2332661,race_number#2332662,last_updated#2332663] JDBCRelation(gpspoint) [numPartitions=1]\n                     +- Project [course_cd#2332625 AS s_course_cd#2332637, race_date#2332615 AS s_race_date#2332638, race_number#2332616 AS s_race_number#2332639, saddle_cloth_number#2332624 AS s_saddle_cloth_number#2332640, gate_name#2332617, length_to_finish#2332618, sectional_time#2332619, running_time#2332620, distance_back#2332621, distance_ran#2332622, number_of_strides#2332623]\n                        +- Project [staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, saddle_cloth_number#2332613, 3, true, false, true) AS saddle_cloth_number#2332624, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, course_cd#2332614, 3, true, false, true) AS course_cd#2332625, race_date#2332615, race_number#2332616, gate_name#2332617, length_to_finish#2332618, sectional_time#2332619, running_time#2332620, distance_back#2332621, distance_ran#2332622, number_of_strides#2332623]\n                           +- Relation [saddle_cloth_number#2332613,course_cd#2332614,race_date#2332615,race_number#2332616,gate_name#2332617,length_to_finish#2332618,sectional_time#2332619,running_time#2332620,distance_back#2332621,distance_ran#2332622,number_of_strides#2332623] JDBCRelation(sectionals) [numPartitions=1]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, avg, max as F_max, min as F_min, count, stddev, row_number, lag, lead,\n",
    "    when, lit, first, last, abs, sum as F_sum, udf\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "import math\n",
    "\n",
    "# Initialize Spark session\n",
    "def initialize_spark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"GPS Sectionals Analysis - Enhanced Aggregation\") \\\n",
    "        .config(\"spark.jars\", jdbc_driver_path) \\\n",
    "        .config(\"spark.driver.memory\", \"64g\") \\\n",
    "        .config(\"spark.executor.memory\", \"32g\") \\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"8g\") \\\n",
    "        .config(\"spark.sql.debug.maxToStringFields\", \"1000\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "# Define the haversine function and UDF\n",
    "def define_haversine_udf():\n",
    "    def haversine(lat1, lon1, lat2, lon2):\n",
    "        # Check for None values\n",
    "        if None in (lat1, lon1, lat2, lon2):\n",
    "            return 0.0\n",
    "        # Convert decimal degrees to radians\n",
    "        lon1, lat1, lon2, lat2 = map(\n",
    "            lambda x: math.radians(float(x)), [lon1, lat1, lon2, lat2]\n",
    "        )\n",
    "        # Haversine formula\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = math.sin(dlat / 2) ** 2 + \\\n",
    "            math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
    "        c = 2 * math.asin(math.sqrt(a))\n",
    "        # Radius of earth in meters\n",
    "        r = 6371000\n",
    "        return c * r\n",
    "    return udf(haversine, DoubleType())\n",
    "\n",
    "# Load data from the database\n",
    "def load_data(spark, course):\n",
    "    # Load sectionals data\n",
    "    sectionals_df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"sectionals\",\n",
    "        properties=jdbc_properties\n",
    "    ).filter(col(\"course_cd\") == course).select(\n",
    "        col(\"course_cd\"),\n",
    "        col(\"race_date\"),\n",
    "        col(\"race_number\"),\n",
    "        col(\"saddle_cloth_number\"),\n",
    "        col(\"gate_name\"),\n",
    "        col(\"length_to_finish\"),\n",
    "        col(\"sectional_time\"),\n",
    "        col(\"running_time\"),\n",
    "        col(\"distance_back\"),\n",
    "        col(\"distance_ran\"),\n",
    "        col(\"number_of_strides\")\n",
    "    )\n",
    "\n",
    "    # Load gpspoint data\n",
    "    gps_df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"gpspoint\",\n",
    "        properties=jdbc_properties\n",
    "    ).filter(col(\"course_cd\") == course).select(\n",
    "        col(\"course_cd\"),\n",
    "        col(\"race_date\"),\n",
    "        col(\"race_number\"),\n",
    "        col(\"saddle_cloth_number\"),\n",
    "        \"time_stamp\",\n",
    "        \"longitude\",\n",
    "        \"latitude\",\n",
    "        \"progress\",\n",
    "        \"speed\",\n",
    "        \"stride_frequency\"\n",
    "    )\n",
    "\n",
    "    # Load races data to get nominal race distance\n",
    "    races_df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"races\",\n",
    "        properties=jdbc_properties\n",
    "    ).filter(col(\"course_cd\") == course).select(\n",
    "        \"course_cd\",\n",
    "        \"race_date\",\n",
    "        \"race_number\",\n",
    "        col(\"distance\").alias(\"nominal_distance\"),\n",
    "        col(\"dist_unit\").alias(\"nominal_dist_unit\")\n",
    "    )\n",
    "    return sectionals_df, gps_df, races_df\n",
    "\n",
    "# Rename columns for clarity\n",
    "def rename_columns(sectionals_df, gps_df, races_df):\n",
    "    sectionals_df = sectionals_df.select(\n",
    "        col(\"course_cd\").alias(\"s_course_cd\"),\n",
    "        col(\"race_date\").alias(\"s_race_date\"),\n",
    "        col(\"race_number\").alias(\"s_race_number\"),\n",
    "        col(\"saddle_cloth_number\").alias(\"s_saddle_cloth_number\"),\n",
    "        \"gate_name\",\n",
    "        \"length_to_finish\",\n",
    "        \"sectional_time\",\n",
    "        \"running_time\",\n",
    "        \"distance_back\",\n",
    "        \"distance_ran\",\n",
    "        \"number_of_strides\"\n",
    "    )\n",
    "\n",
    "    gps_df = gps_df.select(\n",
    "        col(\"course_cd\").alias(\"g_course_cd\"),\n",
    "        col(\"race_date\").alias(\"g_race_date\"),\n",
    "        col(\"race_number\").alias(\"g_race_number\"),\n",
    "        col(\"saddle_cloth_number\").alias(\"g_saddle_cloth_number\"),\n",
    "        \"time_stamp\",\n",
    "        \"longitude\",\n",
    "        \"latitude\",\n",
    "        \"progress\",\n",
    "        \"speed\",\n",
    "        \"stride_frequency\"\n",
    "    )\n",
    "\n",
    "    races_df = races_df.select(\n",
    "        col(\"course_cd\").alias(\"r_course_cd\"),\n",
    "        col(\"race_date\").alias(\"r_race_date\"),\n",
    "        col(\"race_number\").alias(\"r_race_number\"),\n",
    "        col(\"nominal_distance\"),\n",
    "        col(\"nominal_dist_unit\")\n",
    "    )\n",
    "    return sectionals_df, gps_df, races_df\n",
    "\n",
    "# Join sectionals and GPS data\n",
    "def join_sectionals_gps(sectionals_df, gps_df):\n",
    "    gps_with_gates = sectionals_df.alias(\"s\").join(\n",
    "        gps_df.alias(\"g\"),\n",
    "        (col(\"s.s_course_cd\") == col(\"g.g_course_cd\")) &\n",
    "        (col(\"s.s_race_date\") == col(\"g.g_race_date\")) &\n",
    "        (col(\"s.s_race_number\") == col(\"g.g_race_number\")) &\n",
    "        (col(\"s.s_saddle_cloth_number\") == col(\"g.g_saddle_cloth_number\")),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    return gps_with_gates\n",
    "\n",
    "# Find the closest GPS point to each gate\n",
    "def find_closest_gps_points(gps_with_gates):\n",
    "    gps_with_gates = gps_with_gates.withColumn(\n",
    "        \"progress_diff\",\n",
    "        abs(col(\"g.progress\") - col(\"s.length_to_finish\"))\n",
    "    )\n",
    "\n",
    "    window_spec_gate = Window.partitionBy(\n",
    "        \"s.s_course_cd\",\n",
    "        \"s.s_race_date\",\n",
    "        \"s.s_race_number\",\n",
    "        \"s.s_saddle_cloth_number\",\n",
    "        \"gate_name\"\n",
    "    ).orderBy(\"progress_diff\")\n",
    "\n",
    "    gps_at_gates = gps_with_gates.withColumn(\n",
    "        \"row_number\",\n",
    "        row_number().over(window_spec_gate)\n",
    "    ).filter(col(\"row_number\") == 1).drop(\"row_number\", \"progress_diff\")\n",
    "    return gps_at_gates\n",
    "\n",
    "# Calculate speed changes and acceleration\n",
    "def calculate_speed_acceleration(gps_at_gates):\n",
    "    gate_order_window = Window.partitionBy(\n",
    "        \"s.s_course_cd\",\n",
    "        \"s.s_race_date\",\n",
    "        \"s.s_race_number\",\n",
    "        \"s.s_saddle_cloth_number\"\n",
    "    ).orderBy(\"gate_name\")  # Modify if gate_name is not sortable\n",
    "\n",
    "    gps_at_gates = gps_at_gates.withColumn(\n",
    "        \"previous_speed\",\n",
    "        lag(\"g.speed\").over(gate_order_window)\n",
    "    ).withColumn(\n",
    "        \"speed_change\",\n",
    "        col(\"g.speed\") - col(\"previous_speed\")\n",
    "    ).withColumn(\n",
    "        \"acceleration\",\n",
    "        when(col(\"previous_speed\").isNotNull(),\n",
    "             col(\"speed_change\") / col(\"previous_speed\")\n",
    "             ).otherwise(lit(0))\n",
    "    )\n",
    "    return gps_at_gates\n",
    "\n",
    "# Identify fastest and slowest gates\n",
    "def identify_fastest_slowest_gates(gps_at_gates):\n",
    "    speed_window = Window.partitionBy(\n",
    "        \"s.s_course_cd\",\n",
    "        \"s.s_race_date\",\n",
    "        \"s.s_race_number\",\n",
    "        \"s.s_saddle_cloth_number\"\n",
    "    )\n",
    "\n",
    "    gps_at_gates = gps_at_gates.withColumn(\n",
    "        \"max_speed\",\n",
    "        F_max(\"g.speed\").over(speed_window)\n",
    "    ).withColumn(\n",
    "        \"min_speed\",\n",
    "        F_min(\"g.speed\").over(speed_window)\n",
    "    ).withColumn(\n",
    "        \"is_fastest_gate\",\n",
    "        when(col(\"g.speed\") == col(\"max_speed\"), lit(1)).otherwise(lit(0))\n",
    "    ).withColumn(\n",
    "        \"is_slowest_gate\",\n",
    "        when(col(\"g.speed\") == col(\"min_speed\"), lit(1)).otherwise(lit(0))\n",
    "    )\n",
    "    return gps_at_gates\n",
    "\n",
    "# Calculate fatigue factor\n",
    "def calculate_fatigue_factor(gps_at_gates):\n",
    "    # Extract finish speed\n",
    "    finish_speed = gps_at_gates.filter(col(\"gate_name\") == \"Finish\").select(\n",
    "        col(\"s.s_course_cd\").alias(\"course_cd\"),\n",
    "        col(\"s.s_race_date\").alias(\"race_date\"),\n",
    "        col(\"s.s_race_number\").alias(\"race_number\"),\n",
    "        col(\"s.s_saddle_cloth_number\").alias(\"saddle_cloth_number\"),\n",
    "        col(\"g.speed\").alias(\"finish_speed\")\n",
    "    )\n",
    "\n",
    "    # Join finish speed back to gps_at_gates\n",
    "    gps_at_gates = gps_at_gates.join(\n",
    "        finish_speed,\n",
    "        on=[\n",
    "            gps_at_gates[\"s.s_course_cd\"] == finish_speed[\"course_cd\"],\n",
    "            gps_at_gates[\"s.s_race_date\"] == finish_speed[\"race_date\"],\n",
    "            gps_at_gates[\"s.s_race_number\"] == finish_speed[\"race_number\"],\n",
    "            gps_at_gates[\"s.s_saddle_cloth_number\"] == finish_speed[\"saddle_cloth_number\"]\n",
    "        ],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Calculate fatigue factor\n",
    "    gps_at_gates = gps_at_gates.withColumn(\n",
    "        \"fatigue_factor\",\n",
    "        (col(\"max_speed\") - col(\"finish_speed\")) / col(\"max_speed\")\n",
    "    )\n",
    "    return gps_at_gates\n",
    "\n",
    "# Prepare aggregated metrics per gate\n",
    "def prepare_aggregated_metrics(gps_at_gates):\n",
    "    aggregated_metrics = gps_at_gates.select(\n",
    "        col(\"s.s_course_cd\").alias(\"course_cd\"),\n",
    "        col(\"s.s_race_date\").alias(\"race_date\"),\n",
    "        col(\"s.s_race_number\").alias(\"race_number\"),\n",
    "        col(\"s.s_saddle_cloth_number\").alias(\"saddle_cloth_number\"),\n",
    "        \"gate_name\",\n",
    "        col(\"g.speed\").alias(\"speed\"),\n",
    "        \"acceleration\",\n",
    "        \"fatigue_factor\",\n",
    "        \"is_fastest_gate\",\n",
    "        \"is_slowest_gate\"\n",
    "    )\n",
    "\n",
    "    per_gate_metrics = aggregated_metrics.groupBy(\n",
    "        \"course_cd\",\n",
    "        \"race_date\",\n",
    "        \"race_number\",\n",
    "        \"saddle_cloth_number\",\n",
    "        \"gate_name\"\n",
    "    ).agg(\n",
    "        avg(\"speed\").alias(\"avg_speed\"),\n",
    "        avg(\"acceleration\").alias(\"avg_acceleration\"),\n",
    "        F_max(\"speed\").alias(\"max_speed\"),\n",
    "        F_min(\"speed\").alias(\"min_speed\"),\n",
    "        F_max(\"fatigue_factor\").alias(\"fatigue_factor\"),\n",
    "        F_max(\"is_fastest_gate\").alias(\"is_fastest_gate\"),\n",
    "        F_max(\"is_slowest_gate\").alias(\"is_slowest_gate\")\n",
    "    )\n",
    "    return per_gate_metrics\n",
    "\n",
    "# Calculate actual distance run and ground loss\n",
    "def calculate_ground_loss(gps_df, races_df, haversine_udf):\n",
    "    # Define window specification\n",
    "    window_spec_time = Window.partitionBy(\n",
    "        \"g_course_cd\", \"g_race_date\", \"g_race_number\", \"g_saddle_cloth_number\"\n",
    "    ).orderBy(\"time_stamp\")\n",
    "\n",
    "    # Get previous latitude and longitude\n",
    "    gps_df = gps_df.withColumn(\"prev_latitude\", lag(\"latitude\").over(window_spec_time))\n",
    "    gps_df = gps_df.withColumn(\"prev_longitude\", lag(\"longitude\").over(window_spec_time))\n",
    "\n",
    "    # Calculate segment distance using haversine formula\n",
    "    gps_df = gps_df.withColumn(\n",
    "        \"segment_distance\",\n",
    "        haversine_udf(\n",
    "            col(\"prev_latitude\"),\n",
    "            col(\"prev_longitude\"),\n",
    "            col(\"latitude\"),\n",
    "            col(\"longitude\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Fill null values\n",
    "    gps_df = gps_df.fillna({\"segment_distance\": 0})\n",
    "\n",
    "    # Calculate cumulative distance\n",
    "    gps_df = gps_df.withColumn(\n",
    "        \"cumulative_distance\",\n",
    "        F_sum(\"segment_distance\").over(window_spec_time)\n",
    "    )\n",
    "\n",
    "    # Get total distance per horse\n",
    "    total_distance_df = gps_df.groupBy(\n",
    "        \"g_course_cd\", \"g_race_date\", \"g_race_number\", \"g_saddle_cloth_number\"\n",
    "    ).agg(\n",
    "        F_max(\"cumulative_distance\").alias(\"total_distance_run\")\n",
    "    )\n",
    "\n",
    "    # Convert nominal distance to meters\n",
    "    races_df = races_df.withColumn(\n",
    "        \"nominal_distance_meters\",\n",
    "        when(col(\"nominal_dist_unit\") == 'F', col(\"nominal_distance\") * 201.168)\n",
    "        .when(col(\"nominal_dist_unit\") == 'Y', col(\"nominal_distance\") * 0.9144)\n",
    "        .otherwise(lit(None))\n",
    "    )\n",
    "\n",
    "    # Join total_distance_df with races_df\n",
    "    distance_comparison_df = total_distance_df.join(\n",
    "        races_df,\n",
    "        (total_distance_df[\"g_course_cd\"] == races_df[\"r_course_cd\"]) &\n",
    "        (total_distance_df[\"g_race_date\"] == races_df[\"r_race_date\"]) &\n",
    "        (total_distance_df[\"g_race_number\"] == races_df[\"r_race_number\"]),\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Calculate ground loss\n",
    "    distance_comparison_df = distance_comparison_df.withColumn(\n",
    "        \"ground_loss\",\n",
    "        col(\"total_distance_run\") - col(\"nominal_distance_meters\")\n",
    "    )\n",
    "\n",
    "    # Prepare for joining\n",
    "    distance_comparison_df = distance_comparison_df.select(\n",
    "        col(\"g_course_cd\").alias(\"course_cd\"),\n",
    "        col(\"g_race_date\").alias(\"race_date\"),\n",
    "        col(\"g_race_number\").alias(\"race_number\"),\n",
    "        col(\"g_saddle_cloth_number\").alias(\"saddle_cloth_number\"),\n",
    "        \"ground_loss\"\n",
    "    )\n",
    "    return distance_comparison_df\n",
    "\n",
    "# Integrate ground loss into final metrics\n",
    "def integrate_ground_loss(per_gate_metrics, distance_comparison_df):\n",
    "    final_metrics = per_gate_metrics.join(\n",
    "        distance_comparison_df,\n",
    "        on=[\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    return final_metrics\n",
    "\n",
    "# Write final metrics to the database\n",
    "def write_to_database(final_metrics):\n",
    "    final_metrics.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"gps_aggregated_results_with_gates\",\n",
    "        mode=\"append\",\n",
    "        properties=jdbc_properties\n",
    "    )\n",
    "\n",
    "# Main processing function for each course\n",
    "def process_course(course, spark, haversine_udf):\n",
    "    print(f\"Processing course: {course}\")\n",
    "\n",
    "    # Load data\n",
    "    sectionals_df, gps_df, races_df = load_data(spark, course)\n",
    "\n",
    "    # Rename columns\n",
    "    sectionals_df, gps_df, races_df = rename_columns(sectionals_df, gps_df, races_df)\n",
    "\n",
    "    # Join sectionals and GPS data\n",
    "    gps_with_gates = join_sectionals_gps(sectionals_df, gps_df)\n",
    "\n",
    "    # Find closest GPS points to gates\n",
    "    gps_at_gates = find_closest_gps_points(gps_with_gates)\n",
    "\n",
    "    # Calculate speed changes and acceleration\n",
    "    gps_at_gates = calculate_speed_acceleration(gps_at_gates)\n",
    "\n",
    "    # Identify fastest and slowest gates\n",
    "    gps_at_gates = identify_fastest_slowest_gates(gps_at_gates)\n",
    "\n",
    "    # Calculate fatigue factor\n",
    "    gps_at_gates = calculate_fatigue_factor(gps_at_gates)\n",
    "\n",
    "    # Prepare aggregated metrics\n",
    "    per_gate_metrics = prepare_aggregated_metrics(gps_at_gates)\n",
    "\n",
    "    # Calculate ground loss\n",
    "    distance_comparison_df = calculate_ground_loss(gps_df, races_df, haversine_udf)\n",
    "\n",
    "    # Integrate ground loss into final metrics\n",
    "    final_metrics = integrate_ground_loss(per_gate_metrics, distance_comparison_df)\n",
    "\n",
    "    # Write final metrics to database\n",
    "    write_to_database(final_metrics)\n",
    "\n",
    "    print(f\"Completed processing for course: {course}\")\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    # Initialize Spark session\n",
    "    spark = initialize_spark()\n",
    "\n",
    "    # Define haversine UDF\n",
    "    haversine_udf = define_haversine_udf()\n",
    "\n",
    "    # List of courses\n",
    "    courses = ['CNL', 'SAR', 'PIM', 'TSA', 'BEL', 'MVR', 'TWO', 'CLS', 'KEE', 'TAM',\n",
    "               'TTP', 'TKD', 'ELP', 'PEN', 'HOU', 'DMR', 'TLS', 'AQU', 'MTH', 'TGP',\n",
    "               'TGG', 'CBY', 'LRL', 'TED', 'IND', 'CTD', 'ASD', 'TCD', 'LAD', 'MED',\n",
    "               'TOP', 'HOO']\n",
    "\n",
    "    # Process each course\n",
    "    for course in courses:\n",
    "        try:\n",
    "            process_course(course, spark, haversine_udf)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing course {course}: {e}\")\n",
    "\n",
    "    # Stop Spark session\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f75611-2333-4205-8bce-0d9280c55440",
   "metadata": {},
   "source": [
    "1. Modular Functions\n",
    "\n",
    ">    •\tinitialize_spark(): Sets up the Spark session with the necessary configurations.\n",
    "> \n",
    ">    •\tdefine_haversine_udf(): Defines the Haversine function to calculate distances between GPS points and registers it as a UDF.\n",
    "> \n",
    ">\t•\t**load_data()**: Loads the sectionals, gpspoint, and races data from the database for a given course.\n",
    "> \n",
    "> \t•\trename_columns(): Aliases columns in the DataFrames for clarity and to avoid naming conflicts.\n",
    "> \n",
    "> \t•\tjoin_sectionals_gps(): Joins the sectionals and gpspoint DataFrames on the race and horse identifiers.\n",
    "> \n",
    ">\t•\tfind_closest_gps_points(): Finds the closest GPS point to each gate for each horse.\n",
    "> \n",
    "> \t•\tcalculate_speed_acceleration(): Calculates speed changes and acceleration between gates.\n",
    "> \n",
    ">\t•\tidentify_fastest_slowest_gates(): Identifies the fastest and slowest gates for each horse.\n",
    "> \n",
    ">\t•\tcalculate_fatigue_factor(): Computes the fatigue factor for each horse based on their maximum speed and finish speed.\n",
    "> \n",
    ">\t•\tprepare_aggregated_metrics(): Aggregates the metrics per gate and per horse.\n",
    "> \n",
    ">\t•\tcalculate_ground_loss(): Calculates the actual distance run by each horse and computes the ground loss compared to the nominal race distance.\n",
    "> \n",
    ">\t•\tintegrate_ground_loss(): Integrates the ground loss metric into the final aggregated metrics.\n",
    ">\t•\twrite_to_database(): Writes the final metrics to the database.\n",
    "> \n",
    ">\t•\tprocess_course(): Orchestrates the processing steps for a single course.\n",
    "> \n",
    ">\t•\tmain(): The main function that initializes the Spark session, processes each course, and stops the Spark session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714c416-49e6-4ab7-899e-8a36276825dd",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "\n",
    "\t1.\tVisualization:\n",
    "\t•\tPlot speed and stride frequency trends across gates for specific horses to validate the analysis.\n",
    "\t2.\tTesting:\n",
    "\t•\tApply this to a few more courses to confirm that the calculations are meaningful and robust across different races.\n",
    "\t3.\tAnalysis:\n",
    "\t•\tCompare fatigue factors or speed trends between winners and non-winners to derive insights about race dynamics.\n",
    "\n",
    "These enhancements should provide valuable insights and improve the predictive capabilities of your analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffe146d-1e27-4d44-a282-06623496d1f0",
   "metadata": {},
   "source": [
    "\n",
    "# Additional steps to derive fatigue factors, sectional efficiency, etc.\n",
    "\n",
    "Innovative Applications\n",
    "\n",
    "\t1.\tPredictive Fatigue Model: Train a model using TPD data to predict fatigue thresholds for horses.\n",
    "\t2.\tRace Simulation: Use historical TPD data to simulate how horses might perform in upcoming races.\n",
    "\t3.\tDynamic Betting Insights: Provide real-time insights into how race conditions or competitor performance might influence outcomes.\n",
    "\n",
    "Spark’s distributed computing will allow you to process the large dataset efficiently and scale as needed. By creatively combining TPD and EQB data, you can uncover insights that traditional analysis might overlook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
