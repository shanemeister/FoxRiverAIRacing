{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a43fc8b",
   "metadata": {},
   "source": [
    "# XGBoost Data Preparation\n",
    "\n",
    "A lot of work has gone into compiling the current dataset. I have merged the gps_df, sectionals_df and results_df. I have limited the amount of Equibase data I am using just to keep the focus on the TPD GPS data, and to do some feature engineering.  However, there are some good metrics from the Equibase data that are just basic measures that could be obtained from any racebook sheet. \n",
    "\n",
    "## Get Started\n",
    "\n",
    "1. Going to load the parquet DataFrame from disk and do some imputation, one-hot encoding, string indexing, and scaling. The run it through XBBoost to see how it's looking. At this point I will do the integration of route data, and add the GPS aggregations. I just want to see what I can minimally do and how its working before I go down the wrong path. If the XGBoost doesn't do any better than the LSTM, at least I won't have wasted any more time on it. \n",
    "\n",
    "### Load master_results_df.parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1463424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from pyspark.sql.functions import (col, count, row_number, abs, unix_timestamp, mean, \n",
    "                                   when, lit, min as spark_min, max as spark_max , \n",
    "                                   row_number, mean, countDistinct, last, first, when)\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession\n",
    "from src.data_preprocessing.data_prep1.sql_queries import sql_queries\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame, Window\n",
    "from src.data_preprocessing.data_prep1.data_utils import (save_parquet, gather_statistics, \n",
    "                initialize_environment, load_config, initialize_logging, initialize_spark, \n",
    "                drop_duplicates_with_tolerance, identify_and_impute_outliers, \n",
    "                identify_and_remove_outliers, identify_missing_and_outliers)\n",
    "# Set global references to None\n",
    "spark = None\n",
    "master_results_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73671a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 22:44:04,072 - INFO - Environment setup initialized.\n",
      "2024-12-10 22:44:04,075 - INFO - Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark, jdbc_url, jdbc_properties, queries, parquet_dir, log_file = initialize_environment()\n",
    "master_results_df = spark.read.parquet(os.path.join(parquet_dir, \"master_results_df.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca7b00d",
   "metadata": {},
   "source": [
    "### Imputation for Time-Series like Data\n",
    "\n",
    " In time-series-like data (which GPS and sectionals data resemble), more sophisticated imputation methods are often desirable. While Spark doesn’t provide a built-in linear interpolation or regression-based imputation function out-of-the-box, you can approximate these methods using a combination of window functions, logical steps, or even Pandas UDFs if you need more complex logic.\n",
    "\n",
    "#### Approaches\n",
    "\n",
    "\tBelow is a more complete and refined version of the code tries earlier. It implements the forward/backward fill logic entirely in Spark using window functions, without having to resort to Pandas. The approach is:\n",
    "    \n",
    "\t1.\tSort by time for each race/horse partition.\n",
    "\n",
    "    2.\tCompute a forward fill by looking up the last non-null value encountered so far.\n",
    "\t\n",
    "    3.\tCompute a backward fill by ordering the DataFrame in reverse order and again using last(...) with ignorenulls=True.\n",
    "\t\n",
    "    4.\tJoin the forward and backward fills together or handle them in one go if you prefer to cache and re-order.\n",
    "\t\n",
    "    5.\tFinally, impute the missing stride_frequency values by taking the average of forward and backward fills.\n",
    "\n",
    "Note: In the example below, we use a temporary DataFrame for the backward fill results and then join them back to avoid complexity. Another approach is to re-apply the window with reverse ordering and store the result, but you’d need to ensure that the ordering and partitioning keys are identical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_id_cols = [\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c44de15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8871a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def forward_backward_fill_impute(df: DataFrame, group_cols, time_col: str, value_col: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Perform forward and backward fill imputation on `value_col` within each group defined by `group_cols`,\n",
    "    then impute missing values by taking the average of forward and backward fill values.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input DataFrame\n",
    "    group_cols (list of str): Columns that define the partition (e.g. [\"course_cd\", \"race_date\", \"race_number\", \"saddle_cloth_number\"])\n",
    "    time_col (str): The timestamp or time ordering column\n",
    "    value_col (str): The column to impute missing values for\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with an imputed column named `value_col+\"_imputed\"` \n",
    "    \"\"\"\n",
    "    # Create a window for forward fill\n",
    "    forward_window = Window.partitionBy(*group_cols).orderBy(time_col)\n",
    "\n",
    "    # Forward fill\n",
    "    df_fwd = df.withColumn(\"forward_fill_value\", last(value_col, ignorenulls=True).over(forward_window))\n",
    "    \n",
    "    # Create a window for backward fill (reverse order)\n",
    "    backward_window = Window.partitionBy(*group_cols).orderBy(col(time_col).desc())\n",
    "\n",
    "    # Backward fill\n",
    "    # We'll do this in a separate DataFrame and then join, to avoid complexity\n",
    "    df_bwd = df.withColumn(\"backward_fill_value\", last(value_col, ignorenulls=True).over(backward_window)) \\\n",
    "               .select(*group_cols, time_col, \"backward_fill_value\")\n",
    "\n",
    "    # Join forward and backward fills together\n",
    "    join_cond = [df_fwd[c] == df_bwd[c] for c in group_cols] + [df_fwd[time_col] == df_bwd[time_col]]\n",
    "    df_joined = df_fwd.join(df_bwd, join_cond, how=\"inner\")\n",
    "\n",
    "    # Impute missing stride_frequency by averaging forward and backward fills if both exist\n",
    "    # If one side is null and the other is not, we can just use the available one.\n",
    "    # The following logic: if original is null, use avg of forward/backward. If one is null, the avg will still work as intended.\n",
    "    df_imputed = df_joined.withColumn(\n",
    "        value_col + \"_imputed\",\n",
    "        when(col(value_col).isNull(),\n",
    "             (when(col(\"forward_fill_value\").isNull(), col(\"backward_fill_value\"))\n",
    "              .otherwise(when(col(\"backward_fill_value\").isNull(), col(\"forward_fill_value\"))\n",
    "                         .otherwise((col(\"forward_fill_value\") + col(\"backward_fill_value\")) / 2.0))))\n",
    "        ).otherwise(col(value_col))\n",
    "    )\n",
    "\n",
    "    # Drop intermediate columns\n",
    "    df_final = df_imputed.drop(\"forward_fill_value\", \"backward_fill_value\")\n",
    "\n",
    "    return df_final\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
