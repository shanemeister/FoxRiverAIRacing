{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a43fc8b",
   "metadata": {},
   "source": [
    "# CATBoost Model Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed99b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install catboost -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963dda4f-1bb1-4a95-8bd6-be25dbfa7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1463424",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup Environment\n",
    "import time\n",
    "from optuna.importance import MeanDecreaseImpurityImportanceEvaluator\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib # Used for encoding horse_id\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import optuna.visualization as viz\n",
    "from catboost import CatBoostRanker, CatBoostRegressor, CatBoostClassifier, Pool\n",
    "import catboost as cb\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import (col, count, row_number, abs, unix_timestamp, mean, \n",
    "                                   when, lit, min as F_min, max as F_max , upper, trim,\n",
    "                                   row_number, mean as F_mean, countDistinct, last, first, when)\n",
    "from src.data_preprocessing.data_prep1.data_utils import initialize_environment \n",
    "# Set global references to None\n",
    "spark = None\n",
    "master_results_df = None\n",
    "race_df = None\n",
    "df = None\n",
    "training_data = None\n",
    "train_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73671a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark, jdbc_url, jdbc_properties, parquet_dir, log_file = initialize_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset has already been cleaned up in the LGB notebook and saved as a starting point\n",
    "# It now just needs to be converted to Panadas and run in the GBDT variant model (LGB, XGB, CatBoost)\n",
    "#full_data = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/horse_embedding_data-2025-01-25-2309.parquet\")\n",
    "full_data = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/speed_figure.parquet\")\n",
    "\n",
    "# full_data = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/speed_figure.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ccc107",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce2ed82",
   "metadata": {},
   "source": [
    "# Switching to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3b985b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame -> Pandas DataFrame\n",
    "\n",
    "full_data = full_data.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba21807-df2c-498c-922d-557627c932e1",
   "metadata": {},
   "source": [
    "# Convert DataTime columns to Numerical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae263c-9abd-4a62-9e68-85fdfa0842fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming training_data is your DataFrame\n",
    "# Convert datetime columns to numerical\n",
    "datetime_columns = [\"first_race_date_5\", \"most_recent_race_5\", \"prev_race_date\", \"date_of_birth\"]\n",
    "for col in datetime_columns:\n",
    "    full_data[col] = pd.to_datetime(full_data[col])\n",
    "    full_data[f\"{col}_numeric\"] = (full_data[col] - pd.Timestamp(\"1970-01-01\")).dt.days\n",
    "\n",
    "\n",
    "# Drop the original datetime columns\n",
    "full_data.drop(columns=datetime_columns, inplace=True)\n",
    "\n",
    "# Display summary statistics for the numeric columns\n",
    "print(full_data[[f\"{col}_numeric\" for col in datetime_columns]].describe())\n",
    "\n",
    "# # Plot the distribution of the numeric columns\n",
    "# for col in datetime_columns:\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.hist(full_data[f\"{col}_numeric\"], bins=50, edgecolor='black')\n",
    "#     plt.title(f'Distribution of {col}_numeric')\n",
    "#     plt.xlabel(f'{col}_numeric')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f2bd84",
   "metadata": {},
   "source": [
    "# Split the data - Training/Valid & Meta-Data Training by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the date boundary for hold-out as 2024-07-01\n",
    "holdout_start = \"2024-07-01\"\n",
    "\n",
    "# Convert race_date if not done\n",
    "full_data[\"race_date\"] = pd.to_datetime(full_data[\"race_date\"])\n",
    "\n",
    "# meta_data/hold-out: from 2024-07-01 onward\n",
    "meta_data = full_data[full_data[\"race_date\"] >= holdout_start].copy()\n",
    "\n",
    "# combined_data: everything before 2024-07-01\n",
    "combined_data = full_data[full_data[\"race_date\"] < holdout_start].copy()\n",
    "\n",
    "print(\"meta_data shape:\", meta_data.shape)\n",
    "print(\"combined_data shape:\", combined_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dc503c",
   "metadata": {},
   "source": [
    "# Drop Unnecssary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528e0538",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"horse_name\", \"horse_id\", \"official_fin\", \"saddle_cloth_number\" , \"group_id\",\n",
    "                \"group_id\", \"dam_earnings\", \"sire_earnings\", \"date_of_birth\"]\n",
    "\n",
    "meta_data.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")\n",
    "combined_data.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec130f9-fbd2-45b4-9954-09d9c5de0f3a",
   "metadata": {},
   "source": [
    "# Assigned Numerical & Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4665bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    \"course_cd\", \"trk_cond\", \"sex\", \"equip\", \"surface\", \"med\", \n",
    "    \"race_type\", \"stk_clm_md\", \"turf_mud_mark\", \"layoff_cat\", \"previous_surface\"\n",
    "]\n",
    "\n",
    "label_col = \"perf_target\"  # The finishing position or performance measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2463260-e601-4420-b354-5ba98f4e6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "    col for col in combined_data.columns\n",
    "    if combined_data[col].dtype in [\"float64\", \"int64\"] and col not in [\"perf_target\", \"group_id\"]\n",
    "]\n",
    "\n",
    "# Add embedding columns explicitly\n",
    "embed_cols = [f\"embed_{i}\" for i in range(64)]\n",
    "numeric_cols.extend(embed_cols)\n",
    "\n",
    "# Final numeric columns\n",
    "print(\"Numerical Columns for Training:\", numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029de77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2932fe9",
   "metadata": {},
   "source": [
    "# No Encoding of cat_cols -- just check for NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d13e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for c in cat_cols:\n",
    "    combined_data[c] = combined_data[c].astype(str).fillna(\"Missing\")\n",
    "    meta_data[c] = meta_data[c].astype(str).fillna(\"Missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display all columns with their data types and null status\n",
    "#combined_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2adbbd-77a2-4051-9607-df75a5a4378a",
   "metadata": {},
   "source": [
    "# Group and sort data by race_id and group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc47a3bb-c4a5-49c7-88e8-3c66f4e80e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.rename(columns={\"race_id\": \"group_id\"}, inplace=True)\n",
    "meta_data.rename(columns={\"race_id\": \"group_id\"}, inplace=True)\n",
    "\n",
    "combined_data.sort_values(by=[\"group_id\"], ascending=True, inplace=True)\n",
    "meta_data.sort_values(by=[\"group_id\"], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f4b9cb",
   "metadata": {},
   "source": [
    "# perf_target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee81238-fdad-4845-a5bc-b5bb2c2bd186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming training_data is your DataFrame\n",
    "# Plot the distribution of the 'rank' column\n",
    "combined_data[\"perf_target\"].plot(kind='hist', bins=400, edgecolor='black')\n",
    "plt.title('Distribution of Rank')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df79a49-368c-483a-8b3d-82d0b5eb31a8",
   "metadata": {},
   "source": [
    "# Split Combined Data Into Train/Valid\n",
    "\n",
    "Split combined_data (which covers 2022-01-01 through 2024-06-30) into two parts:\n",
    "\n",
    "\t•\ttrain_data up to 2023-12-31\n",
    "\t•\tvalid_data from 2024-01-01 to 2024-06-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end_date = \"2023-12-31\"\n",
    "valid_data_end_date = \"2024-06-30\"\n",
    "\n",
    "# Alternatively, you may have separate logic if combined_data is already filtered \n",
    "# by date. We assume \"combined_data\" only goes up to 2024-06-30, so you might do\n",
    "train_data = combined_data[combined_data[\"race_date\"] <= train_end_date]\n",
    "valid_data = combined_data[combined_data[\"race_date\"] > train_end_date]\n",
    "\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Valid shape:\", valid_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38146b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data=train_data.drop(columns=[\"race_date\"], errors=\"ignore\")\n",
    "# valid_data=valid_data.drop(columns=[\"race_date\"], errors=\"ignore\")\n",
    "# meta_data=meta_data.drop(columns=[\"race_date\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35096c0",
   "metadata": {},
   "source": [
    "# Remove Unnecessary Columns, Create X_ / y_ Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ca9706",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = \"perf_target\"\n",
    "\n",
    "# Drop from features\n",
    "X_train = train_data.drop(columns=[label_col, \"group_id\"], errors=\"ignore\")\n",
    "y_train = train_data[label_col]\n",
    "train_group_id = train_data[\"group_id\"]\n",
    "\n",
    "X_valid = valid_data.drop(columns=[label_col, \"group_id\"], errors=\"ignore\")\n",
    "y_valid = valid_data[label_col]\n",
    "valid_group_id = valid_data[\"group_id\"]\n",
    "\n",
    "# For meta_data\n",
    "X_meta = meta_data.drop(columns=[label_col, \"group_id\"], errors=\"ignore\")\n",
    "y_meta = meta_data[label_col]\n",
    "meta_group_id = meta_data[\"group_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc0fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id_series = pd.Series(train_group_id)\n",
    "\n",
    "# Get the count of distinct values\n",
    "distinct_count = group_id_series.nunique()\n",
    "print(distinct_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ee781",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id_series = pd.Series(valid_group_id)\n",
    "\n",
    "# Get the count of distinct values\n",
    "distinct_count = group_id_series.nunique()\n",
    "print(distinct_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c0c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id_series = pd.Series(meta_group_id)\n",
    "\n",
    "# Get the count of distinct values\n",
    "distinct_count = group_id_series.nunique()\n",
    "print(distinct_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bddeb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = X_train.isna().sum()\n",
    "print(\"Number of NaNs or nulls in each column:\")\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a1e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of empty strings in each column\n",
    "empty_string_counts = (X_train == '').sum()\n",
    "print(\"Number of empty strings in each column:\")\n",
    "print(empty_string_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46cae64",
   "metadata": {},
   "source": [
    "# Create CatBoost Pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d41cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_idx = [\n",
    "    X_train.columns.get_loc(c) for c in cat_cols if c in X_train.columns\n",
    "]\n",
    "\n",
    "train_pool = Pool(\n",
    "    data=X_train,\n",
    "    label=y_train,\n",
    "    group_id=train_group_id,\n",
    "    cat_features=cat_features_idx\n",
    ")\n",
    "\n",
    "valid_pool = Pool(\n",
    "    data=X_valid,\n",
    "    label=y_valid,\n",
    "    group_id=valid_group_id,\n",
    "    cat_features=cat_features_idx\n",
    ")\n",
    "\n",
    "# For hold-out meta_data (final test or hold-out)\n",
    "meta_pool = Pool(\n",
    "    data=X_meta,\n",
    "    label=y_meta,  # if you have the labels\n",
    "    group_id=meta_group_id,\n",
    "    cat_features=cat_features_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94e3f78-d7c3-486c-ae2e-5b615f7265c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verify the sorting\n",
    "print(\"Unique race_id in X_train:\", pd.unique(train_group_id).size)\n",
    "print(\"Unique race_id in X_valid:\", pd.unique(valid_group_id).size)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"race_id_train length:\", len(train_group_id))\n",
    "print(\"y_train length:\", len(y_train))\n",
    "print(\"X_valid shape:\", X_valid.shape)\n",
    "print(\"y_valid length:\", len(y_valid))\n",
    "print(\"race_id_valid length:\", len(valid_group_id))\n",
    "\n",
    "# Check for duplicates and missing values\n",
    "print(\"Duplicates in X_train:\", X_train.duplicated().sum())\n",
    "print(\"Missing values in X_train:\", X_train.isnull().sum().sum())\n",
    "print(\"Missing values in X_valid:\", X_valid.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d159b2f1-ac43-42e2-9459-fa6631e53025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train is already defined\n",
    "# Check if X_train is a pandas DataFrame\n",
    "is_dataframe = isinstance(X_train, pd.DataFrame)\n",
    "print(f\"Is X_train a pandas DataFrame? {is_dataframe}\")\n",
    "\n",
    "# Check if all column names in X_train are strings\n",
    "are_columns_strings = all(isinstance(col, str) for col in X_train.columns)\n",
    "print(f\"Are all column names in X_train strings? {are_columns_strings}\")\n",
    "\n",
    "# Display the first few rows of X_train to verify\n",
    "# print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6910f1-0f89-42bc-a5ff-0bc4462ee5cb",
   "metadata": {},
   "source": [
    "# Ensure Categorical Features are Valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3cda34-8c80-41cb-8c8d-9d06878a3a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all categorical features are in the dataset\n",
    "invalid_features = [feat for feat in cat_cols if feat not in X_train.columns]\n",
    "if invalid_features:\n",
    "    raise ValueError(f\"Invalid features in cat_cols: {invalid_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b62870b-ee74-4267-84a1-9588d5a9b92a",
   "metadata": {},
   "source": [
    "# Check for Dups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acca5a1-d7f2-419f-bf18-2e84a7f28a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicates in X_train:\", X_train.duplicated().sum())\n",
    "print(\"Duplicates in X_valid:\", X_valid.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9917ba6-1970-4afd-957e-3857281f6ad9",
   "metadata": {},
   "source": [
    "# Check for Missing Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e3c593-dd0f-45e7-abfe-ba7651dd5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in X_train:\", X_train.isnull().sum().sum())\n",
    "print(\"Missing values in X_valid:\", X_valid.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f0e8b6-2a82-40ac-a785-63c6d361c4f9",
   "metadata": {},
   "source": [
    "# Grouping Information: Ensure the race_id and group_id columns are consistent and correctly split between training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e55d62-478f-4f65-8188-998d6f677fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique train_group_id in X_train:\",train_group_id.nunique())\n",
    "print(\"Unique valid_group_id in X_valid:\",valid_group_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68811f-bb49-40f1-9fae-195b005b9e53",
   "metadata": {},
   "source": [
    "# Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9117a32-b4e8-4e44-926e-1bbf11bc2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "y_train.hist()\n",
    "plt.title('Target Distribution in Training Set')\n",
    "plt.subplot(1, 2, 2)\n",
    "y_valid.hist()\n",
    "plt.title('Target Distribution in Validation Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842de6d",
   "metadata": {},
   "source": [
    "# CATBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a4d59-288c-41e3-9316-fae27d75efe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3f16780-7cd5-46b9-accf-2d6753fa38c9",
   "metadata": {},
   "source": [
    "# Run using Optuna or Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f2750a-f75f-4050-85ca-724a607c6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "from sklearn.metrics import ndcg_score\n",
    "import optuna\n",
    "from catboost import CatBoostRanker, Pool\n",
    "\n",
    "###############################################################################\n",
    "# 1) Utility: get_timestamp()\n",
    "###############################################################################\n",
    "def get_timestamp():\n",
    "    \"\"\"Return a timestamp string like '20230115_223045'.\"\"\"\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d7f86-09a5-4c39-af12-32011381b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 2) The main objective function for Optuna\n",
    "###############################################################################\n",
    "def objective(\n",
    "    trial,\n",
    "    catboost_loss_function,\n",
    "    eval_metric,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    race_id_train,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    race_id_valid,\n",
    "    cat_cols\n",
    "):\n",
    "    \"\"\"\n",
    "    An Optuna objective function that trains a CatBoostRanker on (X_train, y_train)\n",
    "    with group info (race_id_train), evaluates on (X_valid, y_valid) with group info\n",
    "    (race_id_valid). Returns the validation NDCG:top=4 (or whichever metric we want to track)\n",
    "    in order to maximize it.\n",
    "    \"\"\"\n",
    "    # Log trial info\n",
    "    logging.info(f\"Starting Optuna Trial {trial.number}: {catboost_loss_function}, Metric: {eval_metric}\")\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        \"loss_function\": catboost_loss_function,\n",
    "        \"eval_metric\": eval_metric,\n",
    "        \"task_type\": \"GPU\",\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 500, 2000),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
    "        \"random_seed\": 42,\n",
    "        \"verbose\": 100,# Enable verbose output to ensure metrics are tracked\n",
    "    }\n",
    "\n",
    "\n",
    "    # Build and train the model\n",
    "    model = CatBoostRanker(**params)\n",
    "    model.fit(\n",
    "        train_pool,\n",
    "        eval_set=valid_pool,\n",
    "        early_stopping_rounds=100        \n",
    "    )\n",
    "\n",
    "    # Debug: Inspect predictions and ground truth\n",
    "    val_preds = model.predict(valid_pool)\n",
    "    print(\"Sample predictions:\", val_preds[:10])\n",
    "    print(\"y_valid sample:\", y_valid.values[:10])\n",
    "\n",
    "    # Compute NDCG manually if get_best_score() is empty\n",
    "    score_dict = model.get_best_score()\n",
    "    print(\"Model Best Scores (Debug):\", score_dict)\n",
    "\n",
    "    valid_ndcg_k = 0.0\n",
    "    if \"validation\" in score_dict:\n",
    "        metric_key = f\"{eval_metric};type=Base\"\n",
    "        valid_ndcg_k = score_dict[\"validation\"].get(metric_key, 0.0)\n",
    "    else:\n",
    "        print(\"Manually computing NDCG@1...\")\n",
    "        true_labels = y_valid.values.reshape(1, -1)\n",
    "        predicted_scores = val_preds.reshape(1, -1)\n",
    "        valid_ndcg_k = ndcg_score(true_labels, predicted_scores, k=1)\n",
    "        print(f\"Manually Calculated NDCG@1: {valid_ndcg_k}\")\n",
    "\n",
    "    return valid_ndcg_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d3382-112e-4f18-9c64-f69d39a54a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 3) Helper to run Optuna\n",
    "###############################################################################\n",
    "def run_optuna(\n",
    "    catboost_loss_function,\n",
    "    eval_metric,\n",
    "    X_train, y_train, race_id_train,\n",
    "    X_valid, y_valid, race_id_valid,\n",
    "    cat_cols,\n",
    "    n_trials=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates an Optuna study with direction='maximize' for NDCG,\n",
    "    runs the objective, returns the study.\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "    def _objective(trial):\n",
    "        return objective(\n",
    "            trial,\n",
    "            catboost_loss_function,\n",
    "            eval_metric,\n",
    "            X_train, y_train, race_id_train,\n",
    "            X_valid, y_valid, race_id_valid,\n",
    "            cat_cols\n",
    "        )\n",
    "\n",
    "    study.optimize(_objective, n_trials=n_trials)\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def785d9-81d0-4cab-9a2e-fe1dc68f1def",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Train final model & save\n",
    "###############################################################################\n",
    "def train_and_save_model(\n",
    "    catboost_loss_function,\n",
    "    eval_metric,\n",
    "    best_params,\n",
    "    X_train, y_train, race_id_train,\n",
    "    cat_cols,\n",
    "    save_dir=\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/models/catboost\"\n",
    "):\n",
    "    \"\"\"Train a final CatBoostRanker using best_params, then save model to disk.\"\"\"\n",
    "\n",
    "    # Clean up unsupported parameters\n",
    "    recognized_params = dict(best_params)\n",
    "    recognized_params.pop(\"early_stopping_rounds\", None)\n",
    "\n",
    "    # Add core training parameters\n",
    "    recognized_params[\"loss_function\"] = catboost_loss_function\n",
    "    recognized_params[\"eval_metric\"] = eval_metric\n",
    "    recognized_params[\"random_seed\"] = 42\n",
    "    recognized_params[\"task_type\"] = \"GPU\"\n",
    "\n",
    "    # Train the model\n",
    "    final_model = CatBoostRanker(**recognized_params)\n",
    "    final_model.fit(train_pool, verbose=100)\n",
    "\n",
    "    # Save the model\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_filename = f\"catboost_{catboost_loss_function.replace(':', '_')}_{eval_metric.replace(':', '_')}_{timestamp}.cbm\"\n",
    "    model_path = os.path.join(save_dir, model_filename)\n",
    "    final_model.save_model(model_path)\n",
    "\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    return final_model, model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c9fd9f-22b6-485d-a1a1-c3f2abc2bb28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 5) Main script or usage\n",
    "###############################################################################\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example main function to train 8 models:\n",
    "      - YetiRank:top=1..4\n",
    "      - QueryRMSE with eval_metric = NDCG:top=1..4\n",
    "    \"\"\"\n",
    "\n",
    "    # The list of objectives and metrics\n",
    "    objectives_and_metrics = [\n",
    "        (\"YetiRank:top=1\", \"NDCG:top=1\"),\n",
    "        (\"YetiRank:top=2\", \"NDCG:top=2\"),\n",
    "        (\"YetiRank:top=3\", \"NDCG:top=3\"),\n",
    "        (\"YetiRank:top=4\", \"NDCG:top=4\"),\n",
    "        (\"QueryRMSE\", \"NDCG:top=1\"),\n",
    "        (\"QueryRMSE\", \"NDCG:top=2\"),\n",
    "        (\"QueryRMSE\", \"NDCG:top=3\"),\n",
    "        (\"QueryRMSE\", \"NDCG:top=4\"),\n",
    "    ]\n",
    "\n",
    "    # Save directory for models\n",
    "    save_dir = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/models/catboost\"\n",
    "    all_models = {}\n",
    "\n",
    "    for loss_func, eval_met in objectives_and_metrics:\n",
    "        print(f\"=== Starting Optuna for {loss_func} / {eval_met} ===\")\n",
    "        study = run_optuna(\n",
    "            catboost_loss_function=loss_func,\n",
    "            eval_metric=eval_met,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            race_id_train=train_group_id,\n",
    "            X_valid=X_valid,\n",
    "            y_valid=y_valid,\n",
    "            race_id_valid=valid_group_id,\n",
    "            cat_cols=cat_cols,\n",
    "            n_trials=20  # Adjust as needed\n",
    "        )\n",
    "\n",
    "        best_score = study.best_value\n",
    "        best_params = study.best_params\n",
    "        print(f\"Best score: {best_score}, Best params: {best_params}\")\n",
    "\n",
    "        # Train the final model with the best params\n",
    "        final_model, model_path = train_and_save_model(\n",
    "            catboost_loss_function=loss_func,\n",
    "            eval_metric=eval_met,\n",
    "            best_params=best_params,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            race_id_train=train_group_id,\n",
    "            cat_cols=cat_cols,\n",
    "            save_dir=save_dir\n",
    "        )\n",
    "        print(f\"Saved model to {model_path}\")\n",
    "\n",
    "        # Store model info\n",
    "        all_models[f\"{loss_func}_{eval_met}\"] = (final_model, best_params, best_score, model_path)\n",
    "\n",
    "    print(\"=== Done training all models ===\")\n",
    "\n",
    "# If you want to run the main function:\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eafa00e-6b04-44b8-ba17-f74857d7e671",
   "metadata": {},
   "source": [
    "# Combine Train and Valid data to create Test data for Model Evaulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7765a5-4148-4482-9457-6009a3132193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X_train and X_valid to create X_test\n",
    "X_test = pd.concat([X_train, X_valid], axis=0, ignore_index=True)\n",
    "\n",
    "# Combine y_train and y_valid to create y_test\n",
    "y_test = pd.concat([y_train, y_valid], axis=0, ignore_index=True)\n",
    "\n",
    "# Combine train_group_id and valid_group_id to create group_id_test\n",
    "group_id_test = pd.concat([train_group_id, valid_group_id], axis=0, ignore_index=True)\n",
    "\n",
    "# Create a DataFrame to sort by group_id_test\n",
    "test_df = pd.DataFrame({\n",
    "    'group_id_test': group_id_test,\n",
    "    'y_test': y_test\n",
    "})\n",
    "test_df = pd.concat([test_df, X_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Sort the DataFrame by group_id_test\n",
    "test_df.sort_values(by='group_id_test', ascending=True, inplace=True)\n",
    "\n",
    "# Extract the sorted values\n",
    "group_id_test = test_df['group_id_test'].values\n",
    "y_test = test_df['y_test'].values\n",
    "X_test = test_df.drop(columns=['group_id_test', 'y_test'])\n",
    "\n",
    "\n",
    "# Print the shapes of the sorted DataFrames/Series\n",
    "print(f\"full_data shape: {full_data.shape}\")\n",
    "print(f\"meta_data shape: {meta_data.shape}\")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {X_train.shape}\")\n",
    "\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"group_id_test shape: {group_id_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607842dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get distinct values using a set\n",
    "distinct_group_ids = set(group_id_test)\n",
    "\n",
    "# Get the count of distinct values\n",
    "distinct_count = len(distinct_group_ids)\n",
    "\n",
    "print(f\"Distinct count: {distinct_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3242d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train:\", X_train.shape[0])\n",
    "print(\"X_valid:\", X_valid.shape[0])\n",
    "print(\"Sum =>\", X_train.shape[0] + X_valid.shape[0])\n",
    "print(\"Delta =>\", X_test.shape[0] - (X_train.shape[0] + X_valid.shape[0]) )\n",
    "\n",
    "print(\"group_id_train:\", train_group_id.shape[0])\n",
    "print(\"group_id_valid:\", valid_group_id.shape[0])\n",
    "print(\"group_id total =>\", train_group_id.shape[0] + valid_group_id.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After concat => X_test rows:\", X_test.shape[0])\n",
    "print(\"group_id_test rows:\", group_id_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c7581a-448a-4f89-bbf8-052de5186da4",
   "metadata": {},
   "source": [
    "# Evaluate a Single Model's Top-4 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899dd45-6087-4d15-a074-17b8d3fe6ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRanker, Pool\n",
    "\n",
    "# def evaluate_model_top4(\n",
    "#     model_path,\n",
    "#     X_test,\n",
    "#     y_test,\n",
    "#     group_id_test,\n",
    "#     cat_cols,\n",
    "#     label_name=\"perf_target\"\n",
    "# ):\n",
    "def evaluate_model_top4(\n",
    "    model_path,\n",
    "    X_meta,\n",
    "    y_meta,\n",
    "    meta_group_id,\n",
    "    cat_cols\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a CatBoostRanker model, predicts on X_meta,\n",
    "    measures top-4 correctness (membership & exact order)\n",
    "    for bigger=better true_label and smaller=better pred_rank.\n",
    "    \"\"\"\n",
    "    from catboost import CatBoostRanker, Pool\n",
    "    import pandas as pd\n",
    "    \n",
    "    # 1) Load the CatBoostRanker model\n",
    "    model = CatBoostRanker()\n",
    "    model.load_model(model_path, format=\"cbm\")\n",
    "\n",
    "    # 2) Build test Pool\n",
    "    test_pool = Pool(\n",
    "        data=X_meta,\n",
    "        label=None,  # inference only\n",
    "        group_id=meta_group_id,\n",
    "        cat_features=cat_cols\n",
    "    )\n",
    "\n",
    "    # 3) Predict\n",
    "    preds = model.predict(test_pool)\n",
    "\n",
    "    # 4) Combine predictions with ground truth\n",
    "    df_analysis = pd.DataFrame({\n",
    "        \"race_id\": meta_group_id,\n",
    "        \"pred_score\": preds,\n",
    "        \"true_label\": y_meta\n",
    "    })\n",
    "\n",
    "    # Sort each race by pred_score descending => rank=1 is best\n",
    "    df_analysis = df_analysis.sort_values([\"race_id\", \"pred_score\"], ascending=[True, False])\n",
    "    df_analysis[\"pred_rank\"] = (\n",
    "        df_analysis.groupby(\"race_id\")[\"pred_score\"]\n",
    "                   .rank(method=\"first\", ascending=False)\n",
    "    )\n",
    "\n",
    "    # aggregator functions\n",
    "    def top4_correctness(group):\n",
    "        # bigger is better => actual top-4 = nlargest(4, \"true_label\")\n",
    "        actual_top4_idx = group.nlargest(4, \"true_label\").index\n",
    "        # smaller rank is better => predicted top-4 = nsmallest(4, \"pred_rank\")\n",
    "        pred_top4_idx   = group.nsmallest(4, \"pred_rank\").index\n",
    "        return pd.Series({\n",
    "            \"correct_in_top4\": len(set(actual_top4_idx) & set(pred_top4_idx))\n",
    "        })\n",
    "\n",
    "    def exact_top4_positions(group):\n",
    "        # bigger is better => descending for actual\n",
    "        actual_top4_order = group.sort_values(\"true_label\", ascending=False).head(4).index\n",
    "        # smaller rank is better => ascending for predicted\n",
    "        pred_top4_order   = group.sort_values(\"pred_rank\", ascending=True).head(4).index\n",
    "        exact_positions = sum(a == b for a, b in zip(actual_top4_order, pred_top4_order))\n",
    "        return pd.Series({\"exact_positions_0to4\": exact_positions})\n",
    "\n",
    "    # 5) groupby aggregator\n",
    "    results_top4 = (\n",
    "        df_analysis.groupby(\"race_id\", group_keys=False)[[\"true_label\", \"pred_rank\"]]\n",
    "                   .apply(top4_correctness)\n",
    "    )\n",
    "    top4_counts = results_top4[\"correct_in_top4\"].value_counts(dropna=False).sort_index()\n",
    "\n",
    "    results_order = (\n",
    "        df_analysis.groupby(\"race_id\", group_keys=False)[[\"true_label\", \"pred_rank\"]]\n",
    "                   .apply(exact_top4_positions)\n",
    "    )\n",
    "    order_counts = results_order[\"exact_positions_0to4\"].value_counts(dropna=False).sort_index()\n",
    "\n",
    "    metrics = {\n",
    "        \"top4_counts\": top4_counts.to_dict(),\n",
    "        \"exact_top4_counts\": order_counts.to_dict()\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f9a70b-b9eb-4355-9f62-fd3dc28e8aff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def get_model_filename_pattern(objective, metric):\n",
    "    \"\"\"\n",
    "    Generate the model filename pattern based on the given objective and metric.\n",
    "    \"\"\"\n",
    "    return f\"catboost_{objective.replace(':', '_')}_{metric.replace(':', '_')}\"\n",
    "\n",
    "def find_model_file(model_dir, pattern):\n",
    "    \"\"\"\n",
    "    Find the relevant .cbm file in the specified directory based on the generated pattern.\n",
    "    \"\"\"\n",
    "    search_pattern = os.path.join(model_dir, f\"{pattern}_*.cbm\")\n",
    "    model_files = glob.glob(search_pattern)\n",
    "    if model_files:\n",
    "        # Assuming the latest file is the one you want\n",
    "        model_files.sort()\n",
    "        return model_files[-1]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No model file found for pattern: {pattern}\")\n",
    "\n",
    "# Example usage\n",
    "objectives_and_metrics = [\n",
    "    (\"YetiRank:top=1\", \"NDCG:top=1\"),\n",
    "    (\"YetiRank:top=2\", \"NDCG:top=2\"),\n",
    "    (\"YetiRank:top=3\", \"NDCG:top=3\"),\n",
    "    (\"YetiRank:top=4\", \"NDCG:top=4\"),\n",
    "    (\"QueryRMSE\",      \"NDCG:top=1\"),\n",
    "    (\"QueryRMSE\",      \"NDCG:top=2\"),\n",
    "    (\"QueryRMSE\",      \"NDCG:top=3\"),\n",
    "    (\"QueryRMSE\",      \"NDCG:top=4\"),\n",
    "]\n",
    "model_dir = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/models/catboost\"\n",
    "\n",
    "for objective, metric in objectives_and_metrics:\n",
    "    pattern = get_model_filename_pattern(objective, metric)\n",
    "    print(f\"Pattern: {pattern}\")\n",
    "    model_path = find_model_file(model_dir, pattern)\n",
    "    # print(f\"Using model file: {model_path}\")\n",
    "\n",
    "#     # Suppose you stored them in a dictionary or have them. For an example, if:\n",
    "#     metrics = evaluate_model_top4(\n",
    "#         model_path=model_path,\n",
    "#         X_test=X_test,\n",
    "#         y_test=y_test,\n",
    "#         group_id_test=group_id_test,\n",
    "#         cat_cols=cat_cols\n",
    "#     )# Run Model Evaluation\n",
    "\n",
    "        # Suppose you stored them in a dictionary or have them. For an example, if:\n",
    "    metrics = evaluate_model_top4(\n",
    "        model_path=model_path,\n",
    "        X_meta=X_meta,\n",
    "        y_meta=y_meta,\n",
    "        meta_group_id=meta_group_id,\n",
    "        cat_cols=cat_cols\n",
    "    )# Run Model Evaluation\n",
    "\n",
    "    print(\"=== Weighted Ensemble Results ===\")\n",
    "    print(\"top4_counts:\", metrics[\"top4_counts\"])\n",
    "    print(\"exact_top4_counts:\", metrics[\"exact_top4_counts\"])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c3d2f-6f0e-4a21-851e-722e7d7b3920",
   "metadata": {},
   "source": [
    "# Building the Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86e645f",
   "metadata": {},
   "source": [
    "# Weighted Average Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042fb9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRanker, Pool\n",
    "\n",
    "# 8 models: combos of YetiRank/QueryRMSE with NDCG:top=1..4\n",
    "objectives_and_metrics = [\n",
    "    (\"YetiRank:top=1\", \"NDCG:top=1\"),\n",
    "    (\"YetiRank:top=2\", \"NDCG:top=2\"),\n",
    "    (\"YetiRank:top=3\", \"NDCG:top=3\"),\n",
    "    (\"YetiRank:top=4\", \"NDCG:top=4\"),\n",
    "    (\"QueryRMSE\",      \"NDCG:top=1\"),\n",
    "    (\"QueryRMSE\",      \"NDCG:top=2\"),\n",
    "    (\"QueryRMSE\",      \"NDCG:top=3\"),\n",
    "    (\"QueryRMSE\",      \"NDCG:top=4\"),\n",
    "]\n",
    "\n",
    "def get_model_filename_pattern(objective, metric):\n",
    "    \"\"\"Generate the model filename pattern based on objective and metric.\"\"\"\n",
    "    return f\"catboost_{objective.replace(':', '_')}_{metric.replace(':', '_')}\"\n",
    "\n",
    "def find_model_file(model_dir, pattern):\n",
    "    \"\"\"\n",
    "    Find the relevant .cbm file in the specified directory based on the generated pattern.\n",
    "    Assumes each pattern matches exactly one saved model.\n",
    "    \"\"\"\n",
    "    search_pattern = os.path.join(model_dir, f\"{pattern}_*.cbm\")\n",
    "    model_files = glob.glob(search_pattern)\n",
    "    if model_files:\n",
    "        model_files.sort()\n",
    "        return model_files[-1]  # Return the latest (or only) matching file\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No model file found for pattern: {pattern}\")\n",
    "\n",
    "def load_all_models(model_dir, objectives_and_metrics):\n",
    "    \"\"\"\n",
    "    Loads each of the 8 CatBoostRanker models from disk, returning a list of (objective, metric, model).\n",
    "    \"\"\"\n",
    "    loaded_models = []\n",
    "    for objective, metric in objectives_and_metrics:\n",
    "        pattern = get_model_filename_pattern(objective, metric)\n",
    "        model_path = find_model_file(model_dir, pattern)\n",
    "        cboost = CatBoostRanker()\n",
    "        cboost.load_model(model_path, format=\"cbm\")\n",
    "        loaded_models.append((objective, metric, cboost))\n",
    "    return loaded_models\n",
    "\n",
    "def ensemble_predict(models, X_meta, meta_group_id, cat_cols, weights=None):\n",
    "    \"\"\"\n",
    "    Compute an ensemble prediction (weighted average) across the given models.\n",
    "      - models: list of (objective, metric, catboost_model)\n",
    "      - X_meta: DataFrame or array of shape (n_samples, n_features)\n",
    "      - meta_group_id: array-like, shape (n_samples,) (race_id)\n",
    "      - cat_cols: list of categorical columns if using a DataFrame\n",
    "      - weights: array-like of length = number of models, summing to 1.0\n",
    "                 If None, uses uniform weights.\n",
    "\n",
    "    Returns: ensemble_scores (np.array), shape (n_samples,)\n",
    "    \"\"\"\n",
    "    n_models = len(models)\n",
    "    if weights is None:\n",
    "        # Uniform weighting\n",
    "        weights = np.ones(n_models) / n_models\n",
    "    else:\n",
    "        weights = np.array(weights, dtype=float)\n",
    "        if not np.isclose(weights.sum(), 1.0):\n",
    "            raise ValueError(\"Weights must sum to 1.0\")\n",
    "\n",
    "    # Build a CatBoost Pool for inference\n",
    "    meta_pool = Pool(data=X_meta, group_id=meta_group_id, cat_features=cat_cols)\n",
    "\n",
    "    # Collect predictions from all models\n",
    "    preds_list = []\n",
    "    for (obj, met, model) in models:\n",
    "        preds = model.predict(meta_pool)\n",
    "        preds_list.append(preds)\n",
    "    preds_array = np.column_stack(preds_list)  # shape: (n_samples, n_models)\n",
    "\n",
    "    # Weighted average across columns\n",
    "    ensemble_scores = preds_array @ weights\n",
    "    return ensemble_scores\n",
    "\n",
    "def evaluate_ensemble_top4(ensemble_scores, meta_group_id, y_meta):\n",
    "    \"\"\"\n",
    "    Given the ensemble_scores for each row, meta_group_id, and y_meta,\n",
    "    compute top-4 correctness + exact top-4 positions.\n",
    "    - bigger = better for y_meta (like perf_target=21-official_fin)\n",
    "    - smaller rank is better for ensemble_scores\n",
    "    \"\"\"\n",
    "    # 1) Build DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"group_id\": meta_group_id,\n",
    "        \"ensemble_score\": ensemble_scores,  # lower rank => better if you do ascending=False\n",
    "        \"true_label\": y_meta               # bigger => better\n",
    "    })\n",
    "\n",
    "    # 2) Sort by ensemble_score descending within each group\n",
    "    #    so that highest ensemble_score is first => or if \"score\" is a rank, do ascending\n",
    "    #    Here we assume a higher \"ensemble_score\" is better => so descending sort:\n",
    "    df.sort_values([\"group_id\", \"ensemble_score\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "    # 3) Assign a \"pred_rank\" => rank=1 is best\n",
    "    df[\"pred_rank\"] = df.groupby(\"group_id\")[\"ensemble_score\"].rank(\n",
    "        method=\"first\",\n",
    "        ascending=False  # best => rank=1\n",
    "    )\n",
    "\n",
    "    # 4) Aggregator logic\n",
    "    #    If bigger is better for true_label, we use nlargest(4, \"true_label\").\n",
    "    #    If smaller rank is better, we use nsmallest(4, \"pred_rank\").\n",
    "    def top4_correctness(group):\n",
    "        if group.empty:\n",
    "            return 0\n",
    "        # actual top-4 => largest true_label\n",
    "        actual_top4_idx = group.nlargest(4, \"true_label\").index\n",
    "        # predicted top-4 => smallest pred_rank\n",
    "        pred_top4_idx   = group.nsmallest(4, \"pred_rank\").index\n",
    "        return len(set(actual_top4_idx) & set(pred_top4_idx))\n",
    "\n",
    "    def exact_top4_positions(group):\n",
    "        if group.empty:\n",
    "            return 0\n",
    "        # actual top-4 in descending order => big -> small\n",
    "        actual_top4_order = group.sort_values(\"true_label\", ascending=False).head(4).index\n",
    "        # predicted top-4 in ascending order => rank=1 best\n",
    "        pred_top4_order   = group.sort_values(\"pred_rank\", ascending=True).head(4).index\n",
    "        exact_positions = sum(1 for a, b in zip(actual_top4_order, pred_top4_order) if a == b)\n",
    "        return exact_positions\n",
    "\n",
    "    # 5) Group by race_id\n",
    "    results = df.groupby(\"group_id\", group_keys=False).apply(\n",
    "        lambda g: pd.Series({\n",
    "            \"top4_correct\": top4_correctness(g),\n",
    "            \"exact_top4_positions\": exact_top4_positions(g)\n",
    "        })\n",
    "    )\n",
    "\n",
    "    top4_counts = results[\"top4_correct\"].value_counts().sort_index().to_dict()\n",
    "    exact_top4_counts = results[\"exact_top4_positions\"].value_counts().sort_index().to_dict()\n",
    "    return top4_counts, exact_top4_counts\n",
    "\n",
    "def main_ensemble_meta(model_dir, cat_cols, X_meta, y_meta, meta_group_id):\n",
    "    \"\"\"\n",
    "    1) Load the 8 CatBoost models\n",
    "    2) Predict on X_meta => Weighted Average Ensemble\n",
    "    3) Evaluate top-4 correctness\n",
    "    \"\"\"\n",
    "    # 1) Load models\n",
    "    models_8 = load_all_models(model_dir, objectives_and_metrics)\n",
    "\n",
    "    # 2) Weighted average ensemble\n",
    "    ensemble_scores = ensemble_predict(\n",
    "        models=models_8,\n",
    "        X_meta=X_meta,\n",
    "        meta_group_id=meta_group_id,\n",
    "        cat_cols=cat_cols,\n",
    "        weights=None  # uniform weighting\n",
    "    )\n",
    "\n",
    "    # 3) Evaluate\n",
    "    top4_counts, exact_top4_counts = evaluate_ensemble_top4(\n",
    "        ensemble_scores,\n",
    "        meta_group_id,\n",
    "        y_meta\n",
    "    )\n",
    "\n",
    "    print(\"=== Weighted Ensemble Results ===\")\n",
    "    print(\"top4_counts:\", top4_counts)\n",
    "    print(\"exact_top4_counts:\", exact_top4_counts)\n",
    "\n",
    "    # Approach 1: sum of top4_counts values\n",
    "    total_races = sum(top4_counts.values())\n",
    "    print(\"Total races used (from top4_counts):\", total_races)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    model_dir = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/models/catboost\"\n",
    "    cat_cols = [\n",
    "        \"course_cd\", \"trk_cond\", \"sex\", \"equip\", \"surface\", \"med\",\n",
    "        \"race_type\", \"stk_clm_md\", \"turf_mud_mark\", \"layoff_cat\", \"previous_surface\"\n",
    "    ]\n",
    "    # Suppose X_meta, y_meta, meta_group_id are loaded or defined above,\n",
    "    # and bigger is better for y_meta => e.g. perf_target = 21 - official_fin\n",
    "    main_ensemble_meta(model_dir, cat_cols, X_meta, y_meta, meta_group_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a42d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c223c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd1fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_weight_search(\n",
    "    models,\n",
    "    X_meta,\n",
    "    y_meta,\n",
    "    meta_group_id,\n",
    "    cat_cols,\n",
    "    n_trials=1000,\n",
    "    scoring=\"top4_perfect\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Randomly searches for a weight vector w (length=len(models)) that sums to 1\n",
    "    to maximize a chosen 'scoring' metric on (X_meta, y_meta).\n",
    "\n",
    "    Params:\n",
    "        models: list of (objective, metric, catboost_model) from load_all_models\n",
    "        X_meta: DataFrame or array of shape (n_samples, n_features)\n",
    "        y_meta: array-like, shape (n_samples, )\n",
    "        meta_group_id: array-like, shape (n_samples, )\n",
    "        cat_cols: list of cat columns if X_meta is DataFrame\n",
    "        n_trials: int, how many random weight vectors to try\n",
    "        scoring: str => \"top4_perfect\" or \"exact4_perfect\"\n",
    "                 or any custom approach\n",
    "\n",
    "    Returns:\n",
    "        best_weights: np.array, shape=(n_models,)\n",
    "        best_score: float, the best metric found\n",
    "    \"\"\"\n",
    "    from copy import deepcopy\n",
    "\n",
    "    best_score = -1\n",
    "    best_weights = None\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        # 1) Generate random vector, length=n_models\n",
    "        w = np.random.rand(len(models))\n",
    "        w /= w.sum()  # normalize to sum=1\n",
    "\n",
    "        # 2) Ensemble predict\n",
    "        ensemble_scores = ensemble_predict(\n",
    "            models=models,\n",
    "            X_meta=X_meta,\n",
    "            meta_group_id=meta_group_id,\n",
    "            cat_cols=cat_cols,\n",
    "            weights=w\n",
    "        )\n",
    "        # 3) Evaluate\n",
    "        top4_counts, exact_top4_counts = evaluate_ensemble_top4(\n",
    "            ensemble_scores,\n",
    "            meta_group_id,\n",
    "            y_meta\n",
    "        )\n",
    "        # Let's define a 'score' function:\n",
    "        #   \"top4_perfect\" => top4_counts.get(4, 0)\n",
    "        #   \"exact4_perfect\" => exact_top4_counts.get(4, 0)\n",
    "        #   or \"any other\" => some combination\n",
    "        if scoring == \"top4_perfect\":\n",
    "            score = top4_counts.get(4, 0)\n",
    "        elif scoring == \"exact4_perfect\":\n",
    "            score = exact_top4_counts.get(4, 0)\n",
    "        else:\n",
    "            # Fallback: top4_perfect\n",
    "            score = top4_counts.get(4, 0)\n",
    "\n",
    "        # 4) Track best\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_weights = deepcopy(w)\n",
    "\n",
    "    return best_weights, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc878907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_weight_search_example(\n",
    "    model_dir,\n",
    "    cat_cols,\n",
    "    X_meta,\n",
    "    y_meta,\n",
    "    meta_group_id,\n",
    "    n_trials=500\n",
    "):\n",
    "    # 1) Load 8 first-level models\n",
    "    models_8 = load_all_models(model_dir, objectives_and_metrics)\n",
    "\n",
    "    # 2) Run random search\n",
    "    best_w, best_score = random_weight_search(\n",
    "        models=models_8,\n",
    "        X_meta=X_meta,\n",
    "        y_meta=y_meta,\n",
    "        meta_group_id=meta_group_id,\n",
    "        cat_cols=cat_cols,\n",
    "        n_trials=n_trials,\n",
    "        scoring=\"top4_perfect\"  # or \"exact4_perfect\"\n",
    "    )\n",
    "\n",
    "    print(f\"Best weighting after {n_trials} trials =>\", best_w)\n",
    "    print(\"Best top4_perfect (or whatever scoring) =>\", best_score)\n",
    "    # best_score is the count of races with top4_counts=4 for that weighting\n",
    "    # e.g. if your total races is 6868, you can do ratio=best_score/6868 => ~ success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5627774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_dir = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/models/catboost\"\n",
    "    cat_cols = cat_cols\n",
    "    # X_meta, y_meta, meta_group_id as before\n",
    "    main_weight_search_example(model_dir, cat_cols, X_meta, y_meta, meta_group_id, n_trials=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f424b7b",
   "metadata": {},
   "source": [
    "The above weighted trial did worse at 5728 compared to the YetiRank top-3 at 5813.\n",
    "\n",
    "Will try a stacking approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8466ae77",
   "metadata": {},
   "source": [
    "## Pattern: catboost_YetiRank_top=3_NDCG_top=3\n",
    "\n",
    "=== Weighted Ensemble Results ===\n",
    "\n",
    "> top4_counts: {1: 1, 2: 14, 3: 1040, 4: 5813}\n",
    "\n",
    "> exact_top4_counts: {0: 156, 1: 445, 2: 1706, 3: 652, 4: 3909}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c1c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd44695f",
   "metadata": {},
   "source": [
    "# True 2-Level Meta-Model: A “stacking” approach\n",
    "\n",
    "Below is a step-by-step guide (with example code) for building a true 2-level meta-model (often called **stacking**) using eight first-level CatBoostRanker models and your meta_data. The key difference from a simple weighted average is that you will train a new CatBoost model on the **out-of-sample predictions** of the first-level models (**plus any additional features you choose**) to produce a final ranking score.\n",
    "\n",
    "> Important Note: For a fully robust stacking setup, you typically want three sets:\n",
    "\n",
    "1.\tA train set to train the first-level models,\n",
    "\n",
    "2.\tA meta (or “blending”) set to get out-of-sample predictions from first-level models and train the second-level model,\n",
    "\n",
    "3.\tA final hold-out set for a truly unbiased final evaluation.\n",
    "\n",
    "\tIn this example, we’ll demonstrate stacking using meta_data as both the source of first-level predictions and the set on which we train the second-level model. Ideally, you’d still keep a final hold-out to confirm generalization.\n",
    "    \n",
    "    \n",
    "## High-Level Steps\n",
    "\n",
    "1.\tTrain 8 First-Level Models (already done on your main train set).\n",
    "\n",
    "2.\tInfer the 8 Models on meta_data to get out-of-sample predictions—one column per model.\n",
    "\n",
    "3.\tForm the Second-Level Training Features (X_level2):\n",
    "\t•\tThe 8 predicted score columns, plus any optional original features (like horse embeddings).\n",
    "\n",
    "4.\tTrain a Second-Level CatBoostRanker on (X_level2, y_meta, meta_group_id).\n",
    "\n",
    "5.\tEvaluate on meta_data (or, better yet, a new hold-out data if you have it).\n",
    "\n",
    "Below is code and commentary:\n",
    "\n",
    "\n",
    "### 1. Prerequisites\n",
    "\n",
    "> Assume you already:\n",
    "\n",
    "•\tHave 8 first-level models saved in model_dir with patterns like catboost_YetiRank_top=1_NDCG_top=1_*.cbm.\n",
    "\n",
    "•\tHave a DataFrame meta_data with:\n",
    "\n",
    "    •\tgroup_id (meta_group_id),\n",
    "\t•\ty_meta as finishing positions or perf_target,\n",
    "\t•\tplus optional additional features (like horse embeddings).\n",
    "\n",
    "You also have your lists: cat_cols for the original cat features if you want them again, or some numeric columns. For the second-level model, you may or may not treat columns as categorical. Often, the 8 first-level predictions are numeric.\n",
    "\n",
    "\n",
    "### 2. Load the First-Level Models\n",
    "\n",
    "We can reuse the functions from your existing code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c201431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRanker, Pool\n",
    "\n",
    "objectives_and_metrics = [\n",
    "    (\"YetiRank:top=1\", \"NDCG:top=1\"),\n",
    "    (\"YetiRank:top=2\", \"NDCG:top=2\"),\n",
    "    (\"YetiRank:top=3\", \"NDCG:top=3\"),\n",
    "    (\"YetiRank:top=4\", \"NDCG:top=4\"),\n",
    "    (\"QueryRMSE\",      \"NDCG:top=1\"),\n",
    "    (\"QueryRMSE\",      \"NDCG:top=2\"),\n",
    "    (\"QueryRMSE\",      \"NDCG:top=3\"),\n",
    "    (\"QueryRMSE\",      \"NDCG:top=4\"),\n",
    "]\n",
    "\n",
    "def get_model_filename_pattern(objective, metric):\n",
    "    return f\"catboost_{objective.replace(':', '_')}_{metric.replace(':', '_')}\"\n",
    "\n",
    "def find_model_file(model_dir, pattern):\n",
    "    search_pattern = os.path.join(model_dir, f\"{pattern}_*.cbm\")\n",
    "    model_files = glob.glob(search_pattern)\n",
    "    if model_files:\n",
    "        model_files.sort()\n",
    "        return model_files[-1]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No model file found for pattern: {pattern}\")\n",
    "\n",
    "def load_all_models(model_dir, objectives_and_metrics):\n",
    "    loaded_models = []\n",
    "    for objective, metric in objectives_and_metrics:\n",
    "        pattern = get_model_filename_pattern(objective, metric)\n",
    "        model_path = find_model_file(model_dir, pattern)\n",
    "        cboost = CatBoostRanker()\n",
    "        cboost.load_model(model_path, format=\"cbm\")\n",
    "        loaded_models.append((objective, metric, cboost))\n",
    "    return loaded_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f815ff",
   "metadata": {},
   "source": [
    "###  3. Generate Out-of-Sample Predictions (8 columns) for meta_data\n",
    "\n",
    "3.1 Create a Pool or pass raw DataFrame\n",
    "\n",
    "We’ll build a Pool for meta_data using the original features. Or if your first-level models only used certain columns, make sure X_meta has the same columns. Then we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d700ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_level1_predictions(models, X_meta, meta_group_id, cat_cols):\n",
    "    \"\"\"\n",
    "    For each row in meta_data, produce 8 predictions (one from each model).\n",
    "    Returns a DataFrame with columns [pred_1, pred_2, ..., pred_8].\n",
    "    \"\"\"\n",
    "    meta_pool = Pool(data=X_meta, group_id=meta_group_id, cat_features=cat_cols)\n",
    "\n",
    "    pred_columns = {}\n",
    "    for i, (obj, met, model) in enumerate(models):\n",
    "        preds = model.predict(meta_pool)\n",
    "        pred_columns[f\"pred_{i+1}\"] = preds\n",
    "\n",
    "    # Build a DataFrame\n",
    "    df_preds = pd.DataFrame(pred_columns)\n",
    "    return df_predsdf_preds ends up shape (len(X_meta), 8), one column per model’s predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bc5ad1",
   "metadata": {},
   "source": [
    "3.2 Combine with Additional Features (Optional)\n",
    "\n",
    "If you want to feed embeddings or other horse-level stats into the second-level model, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76afdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_level2_features(df_preds, X_meta):\n",
    "    \"\"\"\n",
    "    Combine the first-level predictions with any additional columns from X_meta\n",
    "    (like embeddings, etc.). Return X_level2 DataFrame.\n",
    "    \"\"\"\n",
    "    # Suppose you only want embeddings or a subset from X_meta\n",
    "    # e.g. embed_0..embed_15, custom_speed_figure, etc.\n",
    "\n",
    "    embeddings = X_meta.filter(regex=\"^embed_\").copy()   # e.g. columns that start with \"embed_\"\n",
    "    other_features = X_meta[[\"custom_speed_figure\"]].copy()\n",
    "    \n",
    "    X_level2 = pd.concat([df_preds.reset_index(drop=True),\n",
    "                          embeddings.reset_index(drop=True),\n",
    "                          other_features.reset_index(drop=True)], axis=1)\n",
    "    return X_level2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a11d8d",
   "metadata": {},
   "source": [
    "**If you prefer to feed only the 8 predictions to the second-level model, skip the extra merges.**\n",
    "\n",
    "\n",
    "### 4. Train the Second-Level Model on (X_level2, y_meta, meta_group_id)\n",
    "\n",
    "Here we treat the second-level approach as a ranking problem again, using CatBoostRanker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e50ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_second_level_catboost(X_level2, y_meta, meta_group_id, cat_features=None):\n",
    "    \"\"\"\n",
    "    Trains a second-level CatBoostRanker on the out-of-sample predictions + optional features\n",
    "    from X_meta, using y_meta as the target finishing position.\n",
    "    \"\"\"\n",
    "    if cat_features is None:\n",
    "        cat_features = []\n",
    "\n",
    "    # Suppose cat_features is a list of column indices or names from X_level2 if you have some cats\n",
    "    # Convert them to indices if needed:\n",
    "    if len(cat_features) > 0 and isinstance(cat_features[0], str):\n",
    "        cat_features_idx = [X_level2.columns.get_loc(c) for c in cat_features if c in X_level2.columns]\n",
    "    else:\n",
    "        cat_features_idx = cat_features\n",
    "\n",
    "    train_pool = Pool(\n",
    "        data=X_level2,\n",
    "        label=y_meta,\n",
    "        group_id=meta_group_id,\n",
    "        cat_features=cat_features_idx\n",
    "    )\n",
    "\n",
    "    meta_model = CatBoostRanker(\n",
    "        loss_function=\"YetiRank\",  # or QueryRMSE\n",
    "        iterations=500,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_seed=42\n",
    "    )\n",
    "\n",
    "    meta_model.fit(train_pool, verbose=100, early_stopping_rounds=50)\n",
    "    return meta_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4de41e",
   "metadata": {},
   "source": [
    "**Note:** If you want a final out-of-sample set to measure performance, you’d do that after training the second-level model. For demonstration, we’ll just train on meta_data here.\n",
    "\n",
    "### 5. Evaluate the Second-Level Model\n",
    "\n",
    "If you want to measure top-4 correctness on the same meta_data you just used for training the second-level model, it’s not a strictly unbiased measure. But here’s how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9864fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_second_level_model(meta_model, X_level2, y_meta, meta_group_id):\n",
    "    # Build a Pool\n",
    "    test_pool = Pool(data=X_level2, label=None, group_id=meta_group_id)\n",
    "    preds = meta_model.predict(test_pool)\n",
    "\n",
    "    # Then rank them\n",
    "    df = pd.DataFrame({\n",
    "        \"group_id\": meta_group_id,\n",
    "        \"pred_score\": preds,\n",
    "        \"true_label\": y_meta\n",
    "    })\n",
    "\n",
    "    df.sort_values([\"group_id\", \"pred_score\"], ascending=[True, False], inplace=True)\n",
    "    df[\"pred_rank\"] = df.groupby(\"group_id\")[\"pred_score\"].rank(method=\"first\", ascending=False)\n",
    "\n",
    "    # top-4 correctness, assuming smaller=better for true_label\n",
    "    def top4_correctness(group):\n",
    "        actual_top4_idx = group.nsmallest(4, \"true_label\").index\n",
    "        pred_top4_idx   = group.nsmallest(4, \"pred_rank\").index\n",
    "        return len(set(actual_top4_idx) & set(pred_top4_idx))\n",
    "\n",
    "    results = df.groupby(\"group_id\").apply(top4_correctness)\n",
    "    return results.value_counts().sort_index().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093480f6",
   "metadata": {},
   "source": [
    "### 7. Where to Evaluate Unbiased\n",
    "\n",
    "In an ideal scenario, you have a final hold-out separate from meta_data. You’d:\n",
    "\n",
    "1.  Train the second-level model on meta_data.\n",
    "    \n",
    "2.  For the final set, run the same process of generating 8 first-level predictions, building X_level2 for the hold-out, then predict with the second-level model, and measure top-4 correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c070b10c",
   "metadata": {},
   "source": [
    "## Final Summary of Steps\n",
    "\t1.\tLoad 8 First-Level Models (already trained on your main train set).\n",
    "\t2.\tPredict on meta_data to get 8 columns of out-of-sample predictions.\n",
    "\t3.\tForm X_level2 = \\text{[8 pred columns]} + \\text{(optional embeddings or other features)}.\n",
    "\t4.\tTrain a second-level CatBoostRanker on (X_level2, y_meta, meta_group_id).\n",
    "\t5.\tPredict with the second-level model for evaluation.\n",
    "\n",
    "That is a true 2-level stacking approach, letting the new second-level model learn how to best combine or correct the first-level predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a4af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
