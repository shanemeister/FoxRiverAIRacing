{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a43fc8b",
   "metadata": {},
   "source": [
    "# Results Dataset Preparation\n",
    "\n",
    "A lot of work has gone into compiling the current dataset. I have merged the gps_df, sectionals_df and results_df. I have limited the amount of Equibase data I am using just to keep the focus on the TPD GPS data, and to do some feature engineering.  However, there are some good metrics from the Equibase data that are just basic measures that could be obtained from any racebook sheet. \n",
    "\n",
    "## Get Started\n",
    "\n",
    "1. Going to load the parquet DataFrame from disk and do some imputation, one-hot encoding, string indexing, and scaling. The run it through XBBoost to see how it's looking. At this point I will do the integration of route data, and add the GPS aggregations. I just want to see what I can minimally do and how its working before I go down the wrong path. If the XGBoost doesn't do any better than the LSTM, at least I won't have wasted any more time on it. \n",
    "\n",
    "### Load master_results_df.parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dab04f-b81d-479d-9cb5-aaaf2760410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1463424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn import set_config\n",
    "from src.data_preprocessing.data_prep1.data_loader import load_data_from_postgresql\n",
    "from src.data_preprocessing.data_prep1.sql_queries import sql_queries\n",
    "import pyspark.sql.functions as F\n",
    "import xgboost as xgb\n",
    "from sklearn import set_config\n",
    "from pyspark.sql.functions import (col, count, row_number, abs, unix_timestamp, mean, \n",
    "                                   when, lit, min as F_min, max as F_max , upper, trim,\n",
    "                                   row_number, mean as F_mean, countDistinct, last, first, when)\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession\n",
    "from src.data_preprocessing.data_prep1.sql_queries import sql_queries\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame, Window\n",
    "from src.data_preprocessing.data_prep1.data_utils import (save_parquet, gather_statistics, \n",
    "                initialize_environment, load_config, initialize_spark, \n",
    "                identify_and_impute_outliers, \n",
    "                identify_and_remove_outliers, identify_missing_and_outliers)\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Set global references to None\n",
    "spark = None\n",
    "master_results_df = None\n",
    "race_df = None\n",
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73671a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark, jdbc_url, jdbc_properties, parquet_dir, log_file = initialize_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c57a596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "    # Suppose we have a dictionary of queries\n",
    "    queries = sql_queries()  # you define these queries\n",
    "\n",
    "    # Load them\n",
    "    dfs = load_data_from_postgresql(\n",
    "        spark, jdbc_url, jdbc_properties,\n",
    "        queries, parquet_dir\n",
    "    )\n",
    "\n",
    "    race_df = None\n",
    "    workouts_df = None\n",
    "    for name, df in dfs.items():\n",
    "        logging.info(f\"DataFrame '{name}' loaded. Schema:\")\n",
    "        if name == \"results\":\n",
    "            race_df = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ccd5955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# 1) Basic logging\n",
    "# ------------------------------------------------\n",
    "def setup_logging(script_dir, log_file):\n",
    "    \"\"\"Sets up logging configuration to write logs to a file.\"\"\"\n",
    "    try:\n",
    "        # Truncate the log file first\n",
    "        with open(log_file, 'w'):\n",
    "            pass\n",
    "\n",
    "        logger = logging.getLogger()\n",
    "        if logger.hasHandlers():\n",
    "            logger.handlers.clear()\n",
    "\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(levelname)s - %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "        logging.info(\"Logging initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to set up logging: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "log_file = \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/logs/Data_Model_training.log\"\n",
    "setup_logging(parquet_dir, log_file)\n",
    "\n",
    "logging.info(\"Data prep logging ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2a40d62-4334-442c-ae7b-34e8cc60363a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- course_cd: string (nullable = true)\n",
      " |-- race_date: date (nullable = true)\n",
      " |-- race_number: integer (nullable = true)\n",
      " |-- saddle_cloth_number: string (nullable = true)\n",
      " |-- horse_id: integer (nullable = true)\n",
      " |-- horse_name: string (nullable = true)\n",
      " |-- official_fin: integer (nullable = true)\n",
      " |-- purse: integer (nullable = true)\n",
      " |-- wps_pool: decimal(10,2) (nullable = true)\n",
      " |-- weight: decimal(10,2) (nullable = true)\n",
      " |-- date_of_birth: date (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- equip: string (nullable = true)\n",
      " |-- claimprice: double (nullable = true)\n",
      " |-- surface: string (nullable = true)\n",
      " |-- trk_cond: string (nullable = true)\n",
      " |-- weather: string (nullable = true)\n",
      " |-- distance: decimal(10,2) (nullable = true)\n",
      " |-- dist_unit: string (nullable = true)\n",
      " |-- power: decimal(10,2) (nullable = true)\n",
      " |-- med: string (nullable = true)\n",
      " |-- morn_odds: decimal(10,2) (nullable = true)\n",
      " |-- avgspd: double (nullable = true)\n",
      " |-- race_type: string (nullable = true)\n",
      " |-- class_rating: integer (nullable = true)\n",
      " |-- todays_cls: decimal(10,2) (nullable = true)\n",
      " |-- net_sentiment: integer (nullable = true)\n",
      " |-- stk_clm_md: string (nullable = true)\n",
      " |-- turf_mud_mark: string (nullable = true)\n",
      " |-- avg_spd_sd: double (nullable = true)\n",
      " |-- ave_cl_sd: double (nullable = true)\n",
      " |-- hi_spd_sd: double (nullable = true)\n",
      " |-- pstyerl: double (nullable = true)\n",
      " |-- all_starts: integer (nullable = true)\n",
      " |-- all_win: integer (nullable = true)\n",
      " |-- all_place: integer (nullable = true)\n",
      " |-- all_show: integer (nullable = true)\n",
      " |-- all_fourth: integer (nullable = true)\n",
      " |-- all_earnings: decimal(12,2) (nullable = true)\n",
      " |-- cond_starts: integer (nullable = true)\n",
      " |-- cond_win: integer (nullable = true)\n",
      " |-- cond_place: integer (nullable = true)\n",
      " |-- cond_show: integer (nullable = true)\n",
      " |-- cond_fourth: integer (nullable = true)\n",
      " |-- cond_earnings: decimal(12,2) (nullable = true)\n",
      " |-- avg_fin_3: double (nullable = true)\n",
      " |-- avg_beaten_3: double (nullable = true)\n",
      " |-- avg_speed_3: double (nullable = true)\n",
      " |-- avg_fin_5: double (nullable = true)\n",
      " |-- avg_beaten_5: double (nullable = true)\n",
      " |-- avg_speed_5: double (nullable = true)\n",
      " |-- speed_improvement: double (nullable = true)\n",
      " |-- days_off: integer (nullable = true)\n",
      " |-- layoff_cat: string (nullable = true)\n",
      " |-- avgtime_gate1: double (nullable = true)\n",
      " |-- avgtime_gate2: double (nullable = true)\n",
      " |-- avgtime_gate3: double (nullable = true)\n",
      " |-- avgtime_gate4: double (nullable = true)\n",
      " |-- sa_dist_bk_gate4: double (nullable = true)\n",
      " |-- total_distance_ran: double (nullable = true)\n",
      " |-- running_time: double (nullable = true)\n",
      " |-- speed_q1: double (nullable = true)\n",
      " |-- speed_q2: double (nullable = true)\n",
      " |-- speed_q3: double (nullable = true)\n",
      " |-- speed_q4: double (nullable = true)\n",
      " |-- total_dist_covered: double (nullable = true)\n",
      " |-- avg_acceleration: double (nullable = true)\n",
      " |-- net_progress_gain: double (nullable = true)\n",
      " |-- gps_avg_stride_length: double (nullable = true)\n",
      " |-- jock_win_percent: decimal(5,2) (nullable = true)\n",
      " |-- jock_itm_percent: decimal(5,2) (nullable = true)\n",
      " |-- trainer_win_percent: decimal(5,2) (nullable = true)\n",
      " |-- trainer_itm_percent: decimal(5,2) (nullable = true)\n",
      " |-- jt_win_percent: decimal(5,2) (nullable = true)\n",
      " |-- jt_itm_percent: decimal(5,2) (nullable = true)\n",
      " |-- jock_win_track: decimal(5,2) (nullable = true)\n",
      " |-- jock_itm_track: decimal(5,2) (nullable = true)\n",
      " |-- trainer_win_track: decimal(5,2) (nullable = true)\n",
      " |-- trainer_itm_track: decimal(5,2) (nullable = true)\n",
      " |-- jt_win_track: decimal(5,2) (nullable = true)\n",
      " |-- jt_itm_track: decimal(5,2) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "race_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80fa01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Dups:\n",
    "primary_keys = [\"course_cd\", \"race_date\", \"race_number\", \"horse_id\"]\n",
    "duplicates = (\n",
    "    race_df.groupBy(*primary_keys)\n",
    "      .agg(F.count(\"*\").alias(\"cnt\"))\n",
    "      .filter(F.col(\"cnt\") > 1)\n",
    ")\n",
    "\n",
    "dup_count = duplicates.count()\n",
    "if dup_count > 0:\n",
    "    print(f\"Found {dup_count} duplicate primary key combinations.\")\n",
    "    duplicates.show()\n",
    "    raise ValueError(f\"Duplicates found: {dup_count}. Deduplication required.\")\n",
    "\n",
    "print(dup_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b03096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Convert Decimal Columns to Double\n",
    "decimal_cols = [\"wps_pool\", \"weight\", \"power\", \"distance\", \"morn_odds\", \n",
    "                \"todays_cls\", \"all_earnings\", \"cond_earnings\", \"wps_pool\",\n",
    "               \"jock_win_percent\", \"jock_itm_percent\", \"trainer_itm_percent\", \n",
    "                \"trainer_win_percent\", \"jt_win_percent\", \"jt_itm_percent\",\n",
    "                \"jock_win_track\", \"jock_itm_track\", \"trainer_win_track\", \"trainer_itm_track\",\n",
    "                \"jt_win_track\", \"jt_itm_track\"]\n",
    "for col_name in decimal_cols:\n",
    "    race_df = race_df.withColumn(col_name, F.col(col_name).cast(\"double\"))\n",
    "print(\"2. Decimal columns converted to double.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_date_of_birth_with_median(race_df):\n",
    "    \"\"\"\n",
    "    Impute date_of_birth with the median value (or a default if no data exists).\n",
    "    \"\"\"\n",
    "    race_df = race_df.withColumn(\"date_of_birth_ts\", F.col(\"date_of_birth\").cast(\"timestamp\").cast(\"long\"))\n",
    "    median_window = Window.orderBy(\"date_of_birth_ts\")\n",
    "\n",
    "    median_ts = race_df.filter(F.col(\"date_of_birth_ts\").isNotNull()).approxQuantile(\"date_of_birth_ts\", [0.5], 0)[0]\n",
    "    if median_ts is None:\n",
    "        median_date = F.lit(\"2000-01-01\").cast(\"date\")\n",
    "    else:\n",
    "        median_date = F.from_unixtime(F.lit(median_ts)).cast(\"date\")\n",
    "\n",
    "    race_df = race_df.withColumn(\n",
    "        \"date_of_birth\",\n",
    "        F.when(F.col(\"date_of_birth\").isNull(), median_date).otherwise(F.col(\"date_of_birth\"))\n",
    "    ).drop(\"date_of_birth_ts\")\n",
    "    print(\"3a. Missing date_of_birth values imputed with median date.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931beafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b. Create age_at_race_day\n",
    "race_df = race_df.withColumn(\n",
    "    \"age_at_race_day\",\n",
    "    F.datediff(F.col(\"race_date\"), F.col(\"date_of_birth\")) / 365.25\n",
    ")\n",
    "print(\"3b. Created age_at_race_day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd6f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3c. Impute categorical and numeric columns -- ensure no whitespace in categorical columns\n",
    "categorical_defaults = {\"weather\": \"UNKNOWN\", \"turf_mud_mark\": \"MISSING\", \"trk_cond\": \"UNKNOWN\"}\n",
    "# Fill missing values for categorical defaults\n",
    "race_df = race_df.fillna(categorical_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31a2bf-c804-4968-9bc3-1619dcd3d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "race_df = race_df.withColumn(\n",
    "    \"med\",\n",
    "    when(col(\"med\") == \"\", \"NONE\").otherwise(col(\"med\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d46b899-e8fc-47b4-a9b2-ba072e40d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_df = race_df.withColumn(\n",
    "    \"turf_mud_mark\",\n",
    "    when(col(\"turf_mud_mark\") == \"\", \"MISSING\").otherwise(col(\"turf_mud_mark\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993868c1-b2d2-457d-a844-eef7a5e8f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_df = race_df.drop(\"date_of_birth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec295f4d-283c-4666-b512-bc32bc17b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_update = [\n",
    "    'jock_itm_percent', 'jock_itm_track', 'jock_win_percent', 'jock_win_track',\n",
    "    'jt_itm_percent', 'jt_itm_track', 'jt_win_percent', 'jt_win_track',\n",
    "    'trainer_itm_percent', 'trainer_itm_track', 'trainer_win_percent', 'trainer_win_track'\n",
    "]\n",
    "\n",
    "# Set the specified columns to 0\n",
    "for column in columns_to_update:\n",
    "    race_df = race_df.withColumn(column, lit(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d12cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove whitespace in column names\n",
    "race_df = race_df.select([F.col(c).alias(c.strip()) for c in race_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53adc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the 'wps_pool' column, excluding nulls\n",
    "mean_value = race_df.select(F.mean(F.col(\"wps_pool\")).alias(\"mean_wps_pool\")).collect()[0][\"mean_wps_pool\"]\n",
    "race_df = race_df.withColumn(\n",
    "    \"wps_pool\",\n",
    "    when(col(\"wps_pool\").isNull(), mean_value).otherwise(col(\"wps_pool\"))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9074f5-bda0-417c-a051-e981f3fc6668",
   "metadata": {},
   "source": [
    "## Missing Horse Form Values\n",
    "\n",
    "Speed improvement, days_off, avg_speed, avg_speed, avg_beaten_3, avg_beaten_5, avg_fin_3, avg_fin_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4fba3d-0057-48be-ae9f-d58fea7b506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming race_df_asc is the DataFrame you are working with\n",
    "race_df = race_df.withColumn(\n",
    "    \"is_first_race\",\n",
    "    when(\n",
    "        col(\"speed_improvement\").isNull() &\n",
    "        col(\"days_off\").isNull() &\n",
    "        col(\"avg_speed_3\").isNull() &\n",
    "        col(\"avg_speed_5\").isNull() &\n",
    "        col(\"avg_beaten_3\").isNull() &\n",
    "        col(\"avg_beaten_5\").isNull() &\n",
    "        col(\"avg_fin_3\").isNull() &\n",
    "        col(\"avg_fin_5\").isNull(),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Populate the columns with -1 where is_first_race is 1\n",
    "columns_to_update = [\n",
    "    \"speed_improvement\", \"days_off\", \"avg_speed_3\", \"avg_speed_5\",\n",
    "    \"avg_beaten_3\", \"avg_beaten_5\", \"avg_fin_3\", \"avg_fin_5\"\n",
    "]\n",
    "\n",
    "for column in columns_to_update:\n",
    "    race_df = race_df.withColumn(\n",
    "        column,\n",
    "        when(col(\"is_first_race\") == 1, lit(-1)).otherwise(col(column))\n",
    "    )\n",
    "\n",
    "# Show the updated DataFrame\n",
    "#race_df_asc.select(\"speed_improvement\", \"days_off\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1353ae-b21e-40f9-b9dd-7ea63c142fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows that have mising values in gate 4 and distance back because it impacts the target label -- \n",
    "# only 22, and 29 rows erach.\n",
    "\n",
    "race_df = race_df.filter(\n",
    "    col(\"avgtime_gate4\").isNotNull() & col(\"sa_dist_bk_gate4\").isNotNull()\n",
    ")\n",
    "# Set remaining values to 0 -- total of 6 features/rows\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "missing_values_to_fill = {\n",
    "    'avg_acceleration': 0,\n",
    "    'speed_q2': 0,\n",
    "    'speed_q3': 0,\n",
    "    'speed_q4': 0\n",
    "}\n",
    "\n",
    "# Fill missing values with 0 for the specified columns\n",
    "race_df = race_df.fillna(missing_values_to_fill)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb9795-7b20-4110-8015-32c707a40fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_parquet(spark, race_df, \"race_df_p1\", parquet_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba7a916-31c6-4b0a-9ec6-d26f4a472fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa864e-e923-45e8-9a47-db082b6f4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_df = spark.read.parquet(os.path.join(parquet_dir, \"race_df_p1.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d88f9-cd9b-4b04-8d9d-1f398aa5a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e57eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25150830-6639-489f-a36e-7c9a7eea21a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "raw",
   "id": "afe76f79-2f1f-4759-a881-9de7f6018025",
   "metadata": {},
   "source": [
    "## Use race_df_p1 for XGBoost, LGBoost, CatBoost and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de0bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
