{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f96c21c8",
   "metadata": {},
   "source": [
    "# Proposed Ensemble Models\n",
    "\n",
    "Given the constraints and objectives, I will consider the following models for the ensemble:\n",
    "\t\n",
    "    1.\tModel 1: LSTM Network on Raw GPS Data\n",
    "    \n",
    ">•\tInput Data: Sequences of raw GPS data (speed, progress, stride_frequency, etc.).\n",
    "\n",
    ">•\tArchitecture: An LSTM network designed to capture temporal dependencies and patterns in the sequential data.\n",
    "\n",
    ">•\tAdvantage: LSTMs are well-suited for time-series data and can learn complex temporal dynamics without the need for hand-engineered features like acceleration.\n",
    "\n",
    "    2.\tModel 2: 1D Convolutional Neural Network (1D-CNN)\n",
    "\t\n",
    ">•\tInput Data: The same raw GPS sequences as in Model 1.\n",
    "\n",
    ">•\tArchitecture: A 1D-CNN that applies convolutional filters across the time dimension to detect local patterns.\n",
    "\n",
    ">•\tAdvantage: CNNs can capture spatial hierarchies and are effective in recognizing patterns in sequences, potentially identifying features like sudden changes in speed or stride frequency.\n",
    "\n",
    "    3.\tModel 3: Transformer-based Model\n",
    "\t\n",
    ">•\tInput Data: Raw GPS sequences and possibly sectionals data.\n",
    "\n",
    ">•\tArchitecture: A Transformer model that uses self-attention mechanisms to weigh the importance of different parts of the sequence.\n",
    "\n",
    ">•\tAdvantage: Transformers can model long-range dependencies and focus on the most relevant parts of the sequence for prediction.\n",
    "\n",
    "## Additional Models (Optional):\n",
    "\n",
    "    4.\tModel 4: Gated Recurrent Unit (GRU) Network\n",
    "\n",
    ">•\tSimilar to LSTMs but with a simpler architecture, GRUs can be more efficient and may perform better on certain datasets.\n",
    "\n",
    ">•\tModel 5: Temporal Convolutional Network (TCN)\n",
    "\n",
    ">•\tTCNs are designed for sequential data and can capture long-term dependencies using causal convolutions and residual connections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f703df",
   "metadata": {},
   "source": [
    "## Load Parquet Train, Test, and Validaion (VAL) Data:\n",
    "\n",
    "/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/train_sequences.parquet\n",
    "\n",
    "/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/test_sequences.parquet\n",
    "\n",
    "/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/val_sequences.parquet\n",
    "\n",
    "# Set Environment\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, size, when, count\n",
    "from src.data_preprocessing.data_prep1.data_utils import (\n",
    "    save_parquet, gather_statistics, initialize_environment,\n",
    "    load_config, initialize_logging, initialize_spark, \n",
    "    identify_and_impute_outliers, identify_and_remove_outliers, process_merged_results_sectionals,\n",
    "    identify_missing_and_outliers\n",
    ")\n",
    "\n",
    "try:\n",
    "    spark, jdbc_url, jdbc_properties, queries, parquet_dir, log_file = initialize_environment()\n",
    "    # input(\"Press Enter to continue...\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during initialization: {e}\")\n",
    "    logging.error(f\"An error occurred during initialization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf56395",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 15:29:15,400 - INFO - Environment setup initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set Environment\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, size, when, count\n",
    "from src.data_preprocessing.data_prep1.data_utils import (\n",
    "    save_parquet, gather_statistics, initialize_environment,\n",
    "    load_config, initialize_logging, initialize_spark, \n",
    "    identify_and_impute_outliers, identify_and_remove_outliers, process_merged_results_sectionals,\n",
    "    identify_missing_and_outliers\n",
    ")\n",
    "\n",
    "try:\n",
    "    spark, jdbc_url, jdbc_properties, queries, parquet_dir, log_file = initialize_environment()\n",
    "    # input(\"Press Enter to continue...\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during initialization: {e}\")\n",
    "    logging.error(f\"An error occurred during initialization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f109da6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sequences_path = os.path.join(parquet_dir, \"train_sequences.parquet\")\n",
    "val_sequences_path = os.path.join(parquet_dir, \"val_sequences.parquet\")\n",
    "test_sequences_path = os.path.join(parquet_dir, \"test_sequences.parquet\")\n",
    "train_sequences = spark.read.parquet(train_sequences_path)\n",
    "val_sequences = spark.read.parquet(val_sequences_path)\n",
    "test_sequences = spark.read.parquet(test_sequences_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaecdf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- race_date: date (nullable = true)\n",
      " |-- race_number: integer (nullable = true)\n",
      " |-- horse_id: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- gate_index: integer (nullable = true)\n",
      " |-- course_cd_ohe: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- equip_ohe: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- surface_ohe: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- trk_cond_ohe: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- weather_ohe: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- med_ohe: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- stk_clm_md_ohe: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- turf_mud_mark_ohe: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- race_type_ohe: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- scaled_features: vector (nullable = true)\n",
      " |-- aggregated_struct: struct (nullable = true)\n",
      " |    |-- ohe_flat: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- avg_speed_agg: double (nullable = true)\n",
      " |    |-- max_speed_agg: double (nullable = true)\n",
      " |    |-- final_speed_agg: double (nullable = true)\n",
      " |    |-- avg_accel_agg: double (nullable = true)\n",
      " |    |-- fatigue_agg: double (nullable = true)\n",
      " |    |-- sectional_time_agg: double (nullable = true)\n",
      " |    |-- running_time_agg: double (nullable = true)\n",
      " |    |-- distance_back_agg: double (nullable = true)\n",
      " |    |-- distance_ran_agg: double (nullable = true)\n",
      " |    |-- strides_agg: double (nullable = true)\n",
      " |    |-- max_speed_overall: double (nullable = true)\n",
      " |    |-- min_speed_overall: double (nullable = true)\n",
      " |-- past_races_sequence: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- ohe_flat: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- avg_speed_agg: double (nullable = true)\n",
      " |    |    |-- max_speed_agg: double (nullable = true)\n",
      " |    |    |-- final_speed_agg: double (nullable = true)\n",
      " |    |    |-- avg_accel_agg: double (nullable = true)\n",
      " |    |    |-- fatigue_agg: double (nullable = true)\n",
      " |    |    |-- sectional_time_agg: double (nullable = true)\n",
      " |    |    |-- running_time_agg: double (nullable = true)\n",
      " |    |    |-- distance_back_agg: double (nullable = true)\n",
      " |    |    |-- distance_ran_agg: double (nullable = true)\n",
      " |    |    |-- strides_agg: double (nullable = true)\n",
      " |    |    |-- max_speed_overall: double (nullable = true)\n",
      " |    |    |-- min_speed_overall: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sequences.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "728c1701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "train_sequences_pd = train_sequences.toPandas()\n",
    "val_sequences_pd = val_sequences.toPandas()\n",
    "test_sequences_pd = test_sequences.toPandas()\n",
    "\n",
    "horse_ids_train = train_sequences_pd[\"horse_id\"].values  # Extract horse_id for training\n",
    "horse_ids_val = val_sequences_pd[\"horse_id\"].values  # Extract horse_id for validation\n",
    "horse_ids_test = test_sequences_pd[\"horse_id\"].values  # Extract horse_id for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fca647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|size(past_races_sequence)|\n",
      "+-------------------------+\n",
      "|                       10|\n",
      "+-------------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_sequences.select(F.size(\"past_races_sequence\")).distinct().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e78edfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(label=1, count=5840), Row(label=6, count=3640), Row(label=3, count=5955), Row(label=5, count=4928), Row(label=4, count=5738), Row(label=7, count=5065), Row(label=2, count=5984), Row(label=0, count=5662)]\n"
     ]
    }
   ],
   "source": [
    "label_distribution = train_sequences.groupBy(\"label\").count().collect()\n",
    "print(label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "779dec92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...\n",
       "1    [([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...\n",
       "2    [([0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...\n",
       "3    [([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0...\n",
       "4    [([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0...\n",
       "Name: past_races_sequence, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences_pd[\"past_races_sequence\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04b82223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flatten_sequence(sequence):\n",
    "    \"\"\"\n",
    "    Flattens a sequence of race data into a single NumPy array.\n",
    "    Ensures uniform array shapes for each time step in the sequence.\n",
    "    \"\"\"\n",
    "    ohe_length = 43 + 26 + 10 + 2 + 8 + 7 + 5 + 4 + 4 + 14  # Total OHE length\n",
    "    aggregator_length = len(aggregator_cols)  # Length of aggregator columns\n",
    "\n",
    "    flattened_sequence = []\n",
    "    for step in sequence:\n",
    "        # Extract `ohe_flat` or default to zero array\n",
    "        ohe_flat = step[\"ohe_flat\"] if \"ohe_flat\" in step else [0.0] * ohe_length\n",
    "\n",
    "        # Ensure `ohe_flat` has the correct length\n",
    "        if len(ohe_flat) != ohe_length:\n",
    "            ohe_flat = [0.0] * ohe_length\n",
    "\n",
    "        # Extract aggregator values or default to -999.0\n",
    "        aggregator_values = [step[agg] if agg in step else -999.0 for agg in aggregator_cols]\n",
    "\n",
    "        # Concatenate `ohe_flat` and aggregator values\n",
    "        flattened_step = np.array(ohe_flat + aggregator_values)\n",
    "\n",
    "        # Verify the length of the flattened step\n",
    "        if len(flattened_step) != (ohe_length + aggregator_length):\n",
    "            raise ValueError(f\"Flattened step has inconsistent length: {len(flattened_step)}\")\n",
    "\n",
    "        flattened_sequence.append(flattened_step)\n",
    "\n",
    "    return np.array(flattened_sequence)\n",
    "\n",
    "aggregator_cols = [\n",
    "    \"avg_speed_agg\", \"max_speed_agg\", \"final_speed_agg\", \"avg_accel_agg\", \n",
    "    \"fatigue_agg\", \"sectional_time_agg\", \"running_time_agg\", \"distance_back_agg\", \n",
    "    \"distance_ran_agg\", \"strides_agg\", \"max_speed_overall\", \"min_speed_overall\"\n",
    "]\n",
    "\n",
    "X_train = np.array([flatten_sequence(seq) for seq in train_sequences_pd[\"past_races_sequence\"]])\n",
    "y_train = train_sequences_pd[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0ee064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([flatten_sequence(seq) for seq in train_sequences_pd[\"past_races_sequence\"]])\n",
    "y_train = train_sequences_pd[\"label\"].values\n",
    "\n",
    "X_val = np.array([flatten_sequence(seq) for seq in val_sequences_pd[\"past_races_sequence\"]])\n",
    "y_val = val_sequences_pd[\"label\"].values\n",
    "\n",
    "X_test = np.array([flatten_sequence(seq) for seq in test_sequences_pd[\"past_races_sequence\"]])\n",
    "y_test = test_sequences_pd[\"label\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09a71c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (42812, 10, 135)\n",
      "y_train shape: (42812,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fc17a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    " # Label targets\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10c190a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0| 5662|\n",
      "|    1| 5840|\n",
      "|    2| 5984|\n",
      "|    3| 5955|\n",
      "|    4| 5738|\n",
      "|    5| 4928|\n",
      "|    6| 3640|\n",
      "|    7| 5065|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_dist = train_sequences.groupBy(\"label\").count().orderBy(\"label\")\n",
    "label_dist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "382e23df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0| 1055|\n",
      "|    1| 1115|\n",
      "|    2| 1130|\n",
      "|    3| 1107|\n",
      "|    4| 1073|\n",
      "|    5|  943|\n",
      "|    6|  821|\n",
      "|    7| 1224|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_dist = test_sequences.groupBy(\"label\").count().orderBy(\"label\")\n",
    "label_dist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42173d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (42812, 10, 135), y_train shape: (42812,), horse_ids_train shape: (42812,)\n",
      "X_val shape: (11253, 10, 135), y_val shape: (11253,), horse_ids_val shape: (11253,)\n",
      "X_test shape: (8468, 10, 135), y_test shape: (8468,), horse_ids_test shape: (8468,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}, horse_ids_train shape: {horse_ids_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}, horse_ids_val shape: {horse_ids_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}, horse_ids_test shape: {horse_ids_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdb17aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1735594318.952801 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735594318.952999 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735594318.974621 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735594318.974844 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735594318.974983 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735594318.975111 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735594319.096630 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735594319.096810 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735594319.096946 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735594319.097069 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735594319.097191 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735594319.097314 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735594319.104891 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735594319.105046 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735594319.105180 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735594319.105310 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1735594319.105444 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-30 15:31:59.105569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1004 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "I0000 00:00:1735594319.105988 3079821 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-30 15:31:59.106108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46314 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:e1:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gru_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m Dropout(\u001b[38;5;241m0.2\u001b[39m)(x)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 4) Concatenate GRU output with embedding\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m concat \u001b[38;5;241m=\u001b[39m Concatenate()([\u001b[43mgru_out\u001b[49m, embedding])\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 5) Dense layers + output\u001b[39;00m\n\u001b[1;32m     30\u001b[0m dense_out \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(concat)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gru_out' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Embedding, Concatenate, Dropout, Flatten\n",
    "\n",
    "# Define the input shapes\n",
    "time_steps = X_train.shape[1]\n",
    "features = X_train.shape[2]\n",
    "num_horses = len(np.unique(horse_ids_train))\n",
    "\n",
    "# 1) Input layers\n",
    "input_features = Input(shape=(time_steps, features), name='input_features')\n",
    "input_horse_id = Input(shape=(1,), name='input_horse_id')\n",
    "\n",
    "# 2) Embedding for horse_id\n",
    "num_horses = len(np.unique(horse_ids_train))\n",
    "embedding = Embedding(input_dim=num_horses, output_dim=32)(input_horse_id)\n",
    "embedding = Flatten()(embedding)\n",
    "\n",
    "# 3) GRU layers (replaces LSTM layers)\n",
    "x = GRU(128, return_sequences=True)(input_features)\n",
    "x = Dropout(0.2)(x)\n",
    "x = GRU(64, return_sequences=True)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = GRU(32, return_sequences=False)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# 4) Concatenate GRU output with embedding\n",
    "concat = Concatenate()([x, embedding])\n",
    "\n",
    "# 5) Dense layers + output\n",
    "dense_out = Dense(64, activation='relu')(concat)\n",
    "dense_out = Dropout(0.2)(dense_out)\n",
    "output = Dense(5, activation='softmax')(dense_out)\n",
    "\n",
    "# 6) Build and compile model\n",
    "model_gru = Model(inputs=[input_features, input_horse_id], outputs=output)\n",
    "model_gru.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b3ff92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787fa0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30426d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "train_counts = dict(zip(unique, counts))\n",
    "\n",
    "print(train_counts)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d22444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Train label counts from your distribution\n",
    "train_counts = np.array([580, 621, 604, 594, 1889])\n",
    "total = train_counts.sum()  # 18164\n",
    "n_classes = len(train_counts)  # 5\n",
    "\n",
    "class_weight = {}\n",
    "for i, count_i in enumerate(train_counts):\n",
    "    class_weight[i] = float(total) / (n_classes * count_i)\n",
    "\n",
    "print(class_weight)\n",
    "# Example output:\n",
    "# {0: 1.463870..., 1: 1.416..., 2: 1.426..., 3: 1.446..., 4: 0.45...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee588f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "history = model_gru.fit(\n",
    "    [X_train, horse_ids_train],\n",
    "    y_train,\n",
    "    epochs=50,  \n",
    "    batch_size=4,  # 64,\n",
    "    validation_data=([X_val, horse_ids_val], y_val),\n",
    "    callbacks=[\n",
    "        lr_scheduler, \n",
    "        early_stopping,\n",
    "        model_checkpoint\n",
    "    ],\n",
    "    class_weight=class_weight,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c647b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy = model_gru.evaluate([X_val, horse_ids_val], y_val)\n",
    "print(f'Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24823de",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_gru.predict([X_test, horse_ids_test])\n",
    "print(np.argmax(preds, axis=-1)[:50])  # First 50 predictio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d936d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "#model.save('/path/to/save/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686865bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru.get_layer(\"embedding_1\").get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284c5de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
