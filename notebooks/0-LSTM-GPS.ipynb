{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db68106",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "Plan is to model a horseâ€™s per-race sequence of positions/velocities/accelerations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d976bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment\n",
    "\n",
    "# Setup Environment\n",
    "import time\n",
    "from optuna.importance import MeanDecreaseImpurityImportanceEvaluator\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import optuna.visualization as viz\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import (col, count, row_number, abs, unix_timestamp, mean, \n",
    "                                   when, lit, min as F_min, max as F_max , upper, trim,\n",
    "                                   row_number, mean as F_mean, countDistinct, last, first, when)\n",
    "from src.data_preprocessing.data_prep1.data_utils import initialize_environment\n",
    "from src.data_preprocessing.data_prep1.data_loader import load_data_from_postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff7422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully.\n"
     ]
    }
   ],
   "source": [
    "spark, jdbc_url, jdbc_properties, parquet_dir, log_file = initialize_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c779b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gps_sql_queries():\n",
    "    queries = {\n",
    "        \"gps_horse\": \"\"\"\n",
    "            SELECT g.course_cd, g.race_date,g.race_number,\n",
    "            REGEXP_REPLACE(TRIM(UPPER(saddle_cloth_number)), '\\s+$', '') AS saddle_cloth_number, time_stamp, \n",
    "            longitude, latitude, speed, progress, stride_frequency, g.post_time, location,\n",
    "            re.axciskey, h.horse_id, re.official_fin, h.horse_name\n",
    "            FROM gpspoint g\n",
    "            JOIN results_entries re on g.course_cd = re.course_cd\n",
    "                AND g.race_date = re.race_date\n",
    "                AND g.race_number = re.race_number\n",
    "                AND g.saddle_cloth_number = re.program_num\n",
    "            JOIN horse h on re.axciskey = h.axciskey        \n",
    "            \"\"\",\n",
    "        \"sectionals\": \"\"\"\n",
    "            SELECT REGEXP_REPLACE(TRIM(UPPER(course_cd)), '\\s+$', '') AS course_cd, race_date, \n",
    "            race_number, REGEXP_REPLACE(TRIM(UPPER(saddle_cloth_number)), '\\s+$', '') AS saddle_cloth_number, \n",
    "            gate_name, gate_numeric,\n",
    "                length_to_finish, sectional_time, running_time, distance_back, distance_ran,\n",
    "                number_of_strides, post_time\n",
    "            FROM sectionals\n",
    "            \"\"\"\n",
    "    }\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d1447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = gps_sql_queries()\n",
    "# dfs = load_data_from_postgresql(spark, jdbc_url, jdbc_properties, queries, parquet_dir)\n",
    "#         # Suppose we have a dictionary of queries\n",
    "# for name, df in dfs.items():\n",
    "#     logging.info(f\"DataFrame '{name}' loaded. Schema:\")\n",
    "#     df.printSchema()\n",
    "#     if name == \"gps_horse\":\n",
    "#         gps_horse_df = df\n",
    "#     elif name == \"sectionals\":\n",
    "#         sectionals_df = df    \n",
    "#     else:\n",
    "#         logging.error(f\"Unknown DataFrame name: {name}\")\n",
    "#         continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d54ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start_time = time.time()\n",
    "# gps_horse_df.write.mode(\"overwrite\").parquet(f\"{parquet_dir}/gps_horse_df\")\n",
    "# sectionals_df.write.mode(\"overwrite\").parquet(f\"{parquet_dir}/sectionals_df\")\n",
    "# logging.info(f\"Data written to Parquet in {time.time() - start_time:.2f} seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f6e871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_embedding = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/horse_embedding_data-20250318_2235.parquet\")\n",
    "#sectionals_df = spark.read.parquet(\"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/parquet/sectionals_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f2d5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, lpad, date_format\n",
    "\n",
    "gps_horse_df = gps_horse_df.withColumn(\n",
    "    \"race_id\",\n",
    "    concat_ws(\n",
    "        \"_\",\n",
    "        col(\"course_cd\"),\n",
    "        date_format(col(\"race_date\"), \"yyyyMMdd\"),\n",
    "        lpad(col(\"race_number\").cast(\"string\"), 2, \"0\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd5b0365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386324"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horse_embedding.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a24ce72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- course_cd: string (nullable = true)\n",
      " |-- race_date: date (nullable = true)\n",
      " |-- race_number: double (nullable = true)\n",
      " |-- group_id: string (nullable = true)\n",
      " |-- class_rating: double (nullable = true)\n",
      " |-- horse_id: double (nullable = true)\n",
      " |-- axciskey: string (nullable = true)\n",
      " |-- race_id: string (nullable = true)\n",
      " |-- post_time: string (nullable = true)\n",
      " |-- saddle_cloth_number: string (nullable = true)\n",
      " |-- horse_name: string (nullable = true)\n",
      " |-- official_fin: long (nullable = true)\n",
      " |-- par_time: double (nullable = true)\n",
      " |-- running_time: double (nullable = true)\n",
      " |-- total_distance_ran: double (nullable = true)\n",
      " |-- avgtime_gate1: double (nullable = true)\n",
      " |-- avgtime_gate2: double (nullable = true)\n",
      " |-- avgtime_gate3: double (nullable = true)\n",
      " |-- avgtime_gate4: double (nullable = true)\n",
      " |-- dist_bk_gate1: double (nullable = true)\n",
      " |-- dist_bk_gate2: double (nullable = true)\n",
      " |-- dist_bk_gate3: double (nullable = true)\n",
      " |-- dist_bk_gate4: double (nullable = true)\n",
      " |-- speed_q1: double (nullable = true)\n",
      " |-- speed_q2: double (nullable = true)\n",
      " |-- speed_q3: double (nullable = true)\n",
      " |-- speed_q4: double (nullable = true)\n",
      " |-- speed_var: double (nullable = true)\n",
      " |-- avg_speed_fullrace: double (nullable = true)\n",
      " |-- accel_q1: double (nullable = true)\n",
      " |-- accel_q2: double (nullable = true)\n",
      " |-- accel_q3: double (nullable = true)\n",
      " |-- accel_q4: double (nullable = true)\n",
      " |-- avg_acceleration: double (nullable = true)\n",
      " |-- max_acceleration: double (nullable = true)\n",
      " |-- jerk_q1: double (nullable = true)\n",
      " |-- jerk_q2: double (nullable = true)\n",
      " |-- jerk_q3: double (nullable = true)\n",
      " |-- jerk_q4: double (nullable = true)\n",
      " |-- avg_jerk: double (nullable = true)\n",
      " |-- max_jerk: double (nullable = true)\n",
      " |-- dist_q1: double (nullable = true)\n",
      " |-- dist_q2: double (nullable = true)\n",
      " |-- dist_q3: double (nullable = true)\n",
      " |-- dist_q4: double (nullable = true)\n",
      " |-- total_dist_covered: double (nullable = true)\n",
      " |-- strfreq_q1: double (nullable = true)\n",
      " |-- strfreq_q2: double (nullable = true)\n",
      " |-- strfreq_q3: double (nullable = true)\n",
      " |-- strfreq_q4: double (nullable = true)\n",
      " |-- avg_stride_length: double (nullable = true)\n",
      " |-- net_progress_gain: double (nullable = true)\n",
      " |-- prev_speed_rating: double (nullable = true)\n",
      " |-- previous_class: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- date_of_birth: date (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- equip: string (nullable = true)\n",
      " |-- claimprice: double (nullable = true)\n",
      " |-- previous_distance: double (nullable = true)\n",
      " |-- previous_surface: string (nullable = true)\n",
      " |-- prev_official_fin: double (nullable = true)\n",
      " |-- power: double (nullable = true)\n",
      " |-- med: string (nullable = true)\n",
      " |-- avgspd: double (nullable = true)\n",
      " |-- starts: double (nullable = true)\n",
      " |-- avg_spd_sd: double (nullable = true)\n",
      " |-- ave_cl_sd: double (nullable = true)\n",
      " |-- hi_spd_sd: double (nullable = true)\n",
      " |-- pstyerl: double (nullable = true)\n",
      " |-- purse: double (nullable = true)\n",
      " |-- surface: string (nullable = true)\n",
      " |-- distance_meters: double (nullable = true)\n",
      " |-- trk_cond: string (nullable = true)\n",
      " |-- morn_odds: double (nullable = true)\n",
      " |-- race_type: string (nullable = true)\n",
      " |-- stk_clm_md: string (nullable = true)\n",
      " |-- turf_mud_mark: string (nullable = true)\n",
      " |-- data_flag: string (nullable = true)\n",
      " |-- jock_win_percent: double (nullable = true)\n",
      " |-- jock_itm_percent: double (nullable = true)\n",
      " |-- trainer_win_percent: double (nullable = true)\n",
      " |-- trainer_itm_percent: double (nullable = true)\n",
      " |-- jt_win_percent: double (nullable = true)\n",
      " |-- jt_itm_percent: double (nullable = true)\n",
      " |-- jock_win_track: double (nullable = true)\n",
      " |-- jock_itm_track: double (nullable = true)\n",
      " |-- trainer_win_track: double (nullable = true)\n",
      " |-- trainer_itm_track: double (nullable = true)\n",
      " |-- jt_win_track: double (nullable = true)\n",
      " |-- jt_itm_track: double (nullable = true)\n",
      " |-- sire_itm_percentage: double (nullable = true)\n",
      " |-- sire_roi: double (nullable = true)\n",
      " |-- dam_itm_percentage: double (nullable = true)\n",
      " |-- dam_roi: double (nullable = true)\n",
      " |-- all_starts: double (nullable = true)\n",
      " |-- all_win: double (nullable = true)\n",
      " |-- all_place: double (nullable = true)\n",
      " |-- all_show: double (nullable = true)\n",
      " |-- all_fourth: double (nullable = true)\n",
      " |-- all_earnings: double (nullable = true)\n",
      " |-- horse_itm_percentage: double (nullable = true)\n",
      " |-- cond_starts: double (nullable = true)\n",
      " |-- cond_win: double (nullable = true)\n",
      " |-- cond_place: double (nullable = true)\n",
      " |-- cond_show: double (nullable = true)\n",
      " |-- cond_fourth: double (nullable = true)\n",
      " |-- cond_earnings: double (nullable = true)\n",
      " |-- net_sentiment: double (nullable = true)\n",
      " |-- total_races_5: double (nullable = true)\n",
      " |-- avg_fin_5: double (nullable = true)\n",
      " |-- avg_speed_5: double (nullable = true)\n",
      " |-- best_speed: double (nullable = true)\n",
      " |-- avg_beaten_len_5: double (nullable = true)\n",
      " |-- first_race_date_5: date (nullable = true)\n",
      " |-- most_recent_race_5: date (nullable = true)\n",
      " |-- avg_dist_bk_gate1_5: double (nullable = true)\n",
      " |-- avg_dist_bk_gate2_5: double (nullable = true)\n",
      " |-- avg_dist_bk_gate3_5: double (nullable = true)\n",
      " |-- avg_dist_bk_gate4_5: double (nullable = true)\n",
      " |-- avg_speed_fullrace_5: double (nullable = true)\n",
      " |-- avg_stride_length_5: double (nullable = true)\n",
      " |-- avg_strfreq_q1_5: double (nullable = true)\n",
      " |-- avg_strfreq_q2_5: double (nullable = true)\n",
      " |-- avg_strfreq_q3_5: double (nullable = true)\n",
      " |-- avg_strfreq_q4_5: double (nullable = true)\n",
      " |-- prev_speed: double (nullable = true)\n",
      " |-- speed_improvement: double (nullable = true)\n",
      " |-- prev_race_date: date (nullable = true)\n",
      " |-- days_off: double (nullable = true)\n",
      " |-- layoff_cat: string (nullable = true)\n",
      " |-- avg_workout_rank_3: double (nullable = true)\n",
      " |-- count_workouts_3: double (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- has_gps: long (nullable = true)\n",
      " |-- age_at_race_day: double (nullable = true)\n",
      " |-- relevance: double (nullable = true)\n",
      " |-- top4_label: long (nullable = true)\n",
      " |-- class_offset: double (nullable = true)\n",
      " |-- class_multiplier: double (nullable = true)\n",
      " |-- official_distance: double (nullable = true)\n",
      " |-- base_speed: double (nullable = true)\n",
      " |-- dist_penalty: double (nullable = true)\n",
      " |-- horse_mean_rps: double (nullable = true)\n",
      " |-- horse_std_rps: double (nullable = true)\n",
      " |-- global_speed_score_iq: double (nullable = true)\n",
      " |-- race_count_agg: long (nullable = true)\n",
      " |-- race_avg_speed_agg: double (nullable = true)\n",
      " |-- race_std_speed_agg: double (nullable = true)\n",
      " |-- race_avg_relevance_agg: double (nullable = true)\n",
      " |-- race_std_relevance_agg: double (nullable = true)\n",
      " |-- race_class_count_agg: long (nullable = true)\n",
      " |-- race_class_avg_speed_agg: double (nullable = true)\n",
      " |-- race_class_min_speed_agg: double (nullable = true)\n",
      " |-- race_class_max_speed_agg: double (nullable = true)\n",
      " |-- embed_0: double (nullable = true)\n",
      " |-- embed_1: double (nullable = true)\n",
      " |-- embed_2: double (nullable = true)\n",
      " |-- embed_3: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "horse_embedding.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2e36d",
   "metadata": {},
   "source": [
    "## Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff39033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, to_date\n",
    "\n",
    "# Ensure race_date is in proper date format\n",
    "gps_horse_df = gps_horse_df.withColumn(\"race_date\", to_date(col(\"race_date\")))\n",
    "\n",
    "# Define a Window partitioned by horse_id, ordered by race_date and post_time\n",
    "window_spec = Window.partitionBy(\"horse_id\").orderBy(col(\"race_date\"), col(\"post_time\"))\n",
    "\n",
    "# Assign a row number to ensure strict chronological order\n",
    "gps_horse_df = gps_horse_df.withColumn(\"race_ordinal\", row_number().over(window_spec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea62df",
   "metadata": {},
   "source": [
    "## Padding Sequences to a Fixed Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d1f22f",
   "metadata": {},
   "source": [
    "###  PySpark Code to Count GPS Points per Horse per Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9528e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+------------------+------------------+\n",
      "|summary|        race_id|          horse_id|        seq_length|\n",
      "+-------+---------------+------------------+------------------+\n",
      "|  count|         353237|            353237|            353237|\n",
      "|   mean|           null|250595.63236297443|  105.952827704912|\n",
      "| stddev|           null|334779.99393681716|21.089323837123167|\n",
      "|    min|AQU_20221229_01|                 1|                 8|\n",
      "|    max|TWO_20241215_13|           2275277|               344|\n",
      "+-------+---------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+----------+------+\n",
      "|race_id        |horse_id|seq_length|bucket|\n",
      "+---------------+--------+----------+------+\n",
      "|AQU_20221229_01|6303    |103       |medium|\n",
      "|AQU_20221229_01|6303    |103       |medium|\n",
      "|AQU_20221229_01|6303    |103       |medium|\n",
      "|AQU_20221229_01|6303    |103       |medium|\n",
      "|AQU_20221229_01|6303    |103       |medium|\n",
      "|AQU_20221229_01|6303    |103       |medium|\n",
      "|AQU_20221229_01|6303    |103       |medium|\n",
      "|AQU_20221229_01|6303    |103       |medium|\n",
      "|AQU_20221229_01|6303    |103       |medium|\n",
      "|AQU_20221229_01|6303    |103       |medium|\n",
      "+---------------+--------+----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:========================>                              (11 + 14) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|bucket|   count|\n",
      "+------+--------+\n",
      "|  long|  515559|\n",
      "|medium|22505959|\n",
      "| short|14404941|\n",
      "+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 44:============================================>           (20 + 5) / 25]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Compute sequence lengths per horse per race\n",
    "df_seq_len = gps_horse_df.groupBy(\"race_id\", \"horse_id\").agg(F.count(\"*\").alias(\"seq_length\"))\n",
    "\n",
    "# Show distribution statistics to validate\n",
    "df_seq_len.describe().show()\n",
    "\n",
    "# Step 2: Define length buckets\n",
    "df_buckets = df_seq_len.withColumn(\n",
    "    \"bucket\",\n",
    "    F.when(F.col(\"seq_length\") <= 100, \"short\")\n",
    "     .when(F.col(\"seq_length\") <= 150, \"medium\")\n",
    "     .otherwise(\"long\")\n",
    ")\n",
    "\n",
    "# Step 3: Join back with original data\n",
    "df_binned = gps_horse_df.join(df_buckets, [\"race_id\", \"horse_id\"])\n",
    "\n",
    "# Show sample results\n",
    "df_binned.select(\"race_id\", \"horse_id\", \"seq_length\", \"bucket\").show(10, truncate=False)\n",
    "\n",
    "# Step 4: Get overall distribution of bucket counts\n",
    "df_binned.groupBy(\"bucket\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde81a2d",
   "metadata": {},
   "source": [
    "### ðŸš€ Next Steps: Aggregate GPS Data & Pad Sequences for LSTM\n",
    "\n",
    "Now that the bucket distribution looks good, letâ€™s aggregate the GPS data per horse per race, then pad sequences within each bucket for LSTM training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75364e16",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Step 2: Aggregate GPS Data into Time-Ordered Sequences\n",
    "\n",
    "Collect GPS points as a time-ordered sequence per (horse_id, race_id).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff138340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+------+----------+\n",
      "|horse_id|race_id        |bucket|seq_length|\n",
      "+--------+---------------+------+----------+\n",
      "|19      |SAR_20230713_05|medium|138       |\n",
      "|48      |PEN_20220818_07|short |95        |\n",
      "|120     |PIM_20230922_02|medium|125       |\n",
      "|127     |LRL_20230618_02|short |87        |\n",
      "|154     |TAM_20231220_05|medium|110       |\n",
      "|162     |TGP_20220514_03|short |77        |\n",
      "|181     |LRL_20221028_06|short |95        |\n",
      "|201     |LRL_20231027_10|medium|127       |\n",
      "|203     |PIM_20220923_02|medium|127       |\n",
      "|203     |TAM_20230219_09|medium|124       |\n",
      "+--------+---------------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 1: Define a window partitioned by (horse_id, race_id) and ordered by time_stamp\n",
    "window_spec = Window.partitionBy(\"horse_id\", \"race_id\").orderBy(\"time_stamp\")\n",
    "\n",
    "# Step 2: Aggregate GPS data into an ordered list per horse per race\n",
    "df_agg = df_binned.withColumn(\n",
    "    \"seq\",\n",
    "    F.collect_list(F.struct(\"longitude\", \"latitude\", \"speed\", \"stride_frequency\")).over(window_spec)\n",
    ")\n",
    "\n",
    "# Step 3: Convert to a single row per (horse_id, race_id)\n",
    "df_agg = df_agg.groupBy(\"horse_id\", \"race_id\", \"bucket\").agg(\n",
    "    F.max(\"seq\").alias(\"seq\")  # Ensures each row has a full sequence\n",
    ")\n",
    "\n",
    "# Step 4: Compute sequence lengths for validation\n",
    "df_agg = df_agg.withColumn(\"seq_length\", F.size(\"seq\"))\n",
    "\n",
    "# Step 5: Show sample results\n",
    "df_agg.select(\"horse_id\", \"race_id\", \"bucket\", \"seq_length\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30053d76",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Step 3: Pad Sequences Based on Buckets\n",
    "\n",
    "Each bucket will be padded to its own max sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1895bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "# Define bucket padding sizes\n",
    "BUCKETS = {\n",
    "    \"short\": 100,      # 0-100 points â†’ pad to 100\n",
    "    \"medium\": 150,     # 101-150 points â†’ pad to 150\n",
    "    \"long\": 300        # 151+ points â†’ pad to 300\n",
    "}\n",
    "\n",
    "# Padding UDF\n",
    "def pad_sequence(seq, bucket):\n",
    "    \"\"\"Pads each sequence based on its assigned bucket size.\"\"\"\n",
    "    seq = [list(s) for s in seq] if seq else []\n",
    "    pad_length = BUCKETS.get(bucket, 300)  # Default to max bucket size\n",
    "\n",
    "    if len(seq) < pad_length:\n",
    "        seq.extend([[0.0, 0.0, 0.0, 0.0]] * (pad_length - len(seq)))  # Padding\n",
    "    elif len(seq) > pad_length:\n",
    "        seq = seq[:pad_length]  # Truncate if needed\n",
    "\n",
    "    return seq\n",
    "\n",
    "# Register UDF\n",
    "pad_udf = F.udf(pad_sequence, ArrayType(ArrayType(FloatType())))\n",
    "\n",
    "# Apply padding\n",
    "df_padded = df_agg.withColumn(\"padded_seq\", pad_udf(\"seq\", \"bucket\")).drop(\"seq\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab1e6a7",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Step 4: Save to Parquet for LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_padded.write.mode(\"overwrite\").parquet(f\"{parquet_dir}/lstm_data.parquet\")\n",
    "print(\"âœ… Padded sequences saved for LSTM training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28733c5e",
   "metadata": {},
   "source": [
    "### ðŸš€ Step 5: Preview the Saved Parquet File\n",
    "\n",
    "#### ðŸ“Œ Load & Inspect the Parquet File in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1435e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Parquet file\n",
    "df_check = spark.read.parquet(f\"{parquet_dir}/lstm_data.parquet\")\n",
    "\n",
    "# Show the schema to confirm the structure\n",
    "df_check.printSchema()\n",
    "\n",
    "# Display a sample of the dataset\n",
    "df_check.select(\"horse_id\", \"race_id\", \"seq_length\", \"bucket\", \"padded_seq\").show(5, truncate=False)\n",
    "\n",
    "# Compute statistics on sequence lengths\n",
    "df_check.select(F.min(\"seq_length\"), F.max(\"seq_length\"), F.avg(\"seq_length\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88600dc5",
   "metadata": {},
   "source": [
    "### ðŸš€ Step 6: Load the Parquet File into NumPy for LSTM Training\n",
    "\n",
    "#### ðŸ“Œ Load the Parquet File & Convert to NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941b440",
   "metadata": {},
   "source": [
    "### ðŸ”¥ Step 1: Fix the seq_length Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd87f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Load the Parquet file\n",
    "df = pq.read_table(f\"{parquet_dir}/lstm_data.parquet\").to_pandas()\n",
    "\n",
    "# Fix the sequence format by converting numpy arrays to lists\n",
    "df[\"padded_seq\"] = df[\"padded_seq\"].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
    "\n",
    "# Recalculate sequence lengths\n",
    "df[\"seq_length\"] = df[\"padded_seq\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "\n",
    "# Verify the new sequence lengths\n",
    "print(df[\"seq_length\"].describe())\n",
    "print(df[[\"horse_id\", \"race_id\", \"bucket\", \"seq_length\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057b38c",
   "metadata": {},
   "source": [
    "### ðŸ”¥ Step 2: Handle nan Values in padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104eb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs with 0.0\n",
    "def fix_nans(seq):\n",
    "    \"\"\"Replace NaNs in sequences with 0.0.\"\"\"\n",
    "    return [[0.0 if np.isnan(value) else value for value in point] for point in seq]\n",
    "\n",
    "df[\"padded_seq\"] = df[\"padded_seq\"].apply(lambda x: fix_nans(x) if isinstance(x, list) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6595b807",
   "metadata": {},
   "source": [
    "### ðŸ”¥ Step 3: Convert to NumPy for LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Check if padded_seq is properly formatted as lists of lists\n",
    "df[\"seq_length\"] = df[\"padded_seq\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "\n",
    "# Display unique sequence lengths\n",
    "print(\"Unique sequence lengths:\", df[\"seq_length\"].unique())\n",
    "\n",
    "# Check if any sequences are missing or empty\n",
    "print(\"Empty sequences count:\", df[df[\"seq_length\"] == 0].shape[0])\n",
    "\n",
    "# Sample a few problematic sequences\n",
    "print(df[df[\"seq_length\"] != 300][\"padded_seq\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968bc74d",
   "metadata": {},
   "source": [
    "### ðŸ”¥ Step 2: Fix the Padding in NumPy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44fff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max sequence length (should match your longest padded bucket)\n",
    "MAX_SEQ_LEN = 300  \n",
    "\n",
    "def pad_numpy(seq, target_length=MAX_SEQ_LEN):\n",
    "    \"\"\"Ensures all sequences are the same length.\"\"\"\n",
    "    seq = np.array(seq, dtype=np.float32) if isinstance(seq, list) else np.zeros((0, 4))\n",
    "\n",
    "    # Ensure the sequence is 2D (seq_length, num_features)\n",
    "    if seq.ndim == 1:\n",
    "        seq = seq.reshape(-1, 4)  # Reshape single-dim sequences\n",
    "\n",
    "    pad_size = target_length - len(seq)\n",
    "\n",
    "    if pad_size > 0:\n",
    "        pad = np.zeros((pad_size, seq.shape[1]))  # Create zero padding\n",
    "        seq = np.vstack([seq, pad])\n",
    "    elif pad_size < 0:\n",
    "        seq = seq[:target_length]  # Truncate if too long\n",
    "\n",
    "    return seq\n",
    "\n",
    "# Apply padding fix to every row\n",
    "df[\"padded_seq_fixed\"] = df[\"padded_seq\"].apply(lambda x: pad_numpy(x))\n",
    "\n",
    "# Convert to NumPy array\n",
    "X = np.stack(df[\"padded_seq_fixed\"].values)  # Now stacking works!\n",
    "\n",
    "# Verify shape\n",
    "print(\"âœ… Fixed Dataset Shape:\", X.shape)  # Expected: (num_samples, 300, 4)\n",
    "\n",
    "# Save as NumPy file for LSTM training\n",
    "np.save(f\"{parquet_dir}/lstm_input.npy\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdabf38",
   "metadata": {},
   "source": [
    "### ðŸš€  Build & Train the LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b29342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the processed NumPy array\n",
    "X = np.load(f\"{parquet_dir}/lstm_input.npy\")\n",
    "\n",
    "# Verify dataset shape\n",
    "print(\"âœ… Dataset Shape:\", X.shape)  # Expected: (num_samples, 300, 4)\n",
    "\n",
    "# Define the number of features (longitude, latitude, speed, stride_frequency)\n",
    "num_features = X.shape[2]  # Should be 4\n",
    "seq_length = X.shape[1]  # Should be 300\n",
    "\n",
    "# Create Y (Target Variable) - Example: Predict next speed value\n",
    "Y = np.roll(X[:, :, 2], shift=-1, axis=1)  # Using speed (column index 2) as the target\n",
    "Y[:, -1] = Y[:, -2]  # Fill last column to avoid NaN shift issue\n",
    "\n",
    "# Verify target shape\n",
    "print(\"âœ… Target Shape:\", Y.shape)  # Expected: (num_samples, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3708d23",
   "metadata": {},
   "source": [
    "#### ðŸ“Œ Step 2: Define the LSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9844de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List available GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Available GPUs:\", gpus)\n",
    "\n",
    "# Enable memory growth (prevents OOM errors)\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"âœ… GPU memory growth enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc18c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# âœ… List GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs:\", gpus)\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)  # Prevent full allocation\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "                gpu,\n",
    "                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=34000)]  # Limit to 34GB\n",
    "            )\n",
    "        print(\"âœ… GPU memory growth enabled with limit\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error setting memory config: {e}\")\n",
    "\n",
    "# âœ… Use MirroredStrategy to enable multi-GPU training\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(f\"âœ… Running on {strategy.num_replicas_in_sync} GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a8bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# âœ… List GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs:\", gpus)\n",
    "\n",
    "# âœ… Enable memory growth (prevents full allocation)\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# âœ… Use MirroredStrategy to enable multi-GPU training\n",
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/GPU:0\", \"/GPU:1\"])\n",
    "\n",
    "print(f\"âœ… Running on {strategy.num_replicas_in_sync} GPUs\")\n",
    "\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Masking(mask_value=0.0, input_shape=(seq_length, num_features)),\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True, activation=\"tanh\"),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=False, activation=\"tanh\"),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1)  # Predicting next speed value\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[\"mae\"])\n",
    "\n",
    "print(\"âœ… Model compiled with MirroredStrategy using NVLink.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b629cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# âœ… Enable multi-GPU training using MirroredStrategy\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(f\"âœ… Running on {strategy.num_replicas_in_sync} GPUs\")\n",
    "\n",
    "# Define the LSTM model within the strategy scope\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        Masking(mask_value=0.0, input_shape=(seq_length, num_features)),\n",
    "        LSTM(128, return_sequences=True, activation=\"tanh\"),\n",
    "        Dropout(0.3),\n",
    "        LSTM(64, return_sequences=False, activation=\"tanh\"),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(1)  # Predicting the next speed value\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001), metrics=[\"mae\"])\n",
    "\n",
    "print(\"âœ… Model compiled with MirroredStrategy using NVLink.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153366f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Enable GPU memory growth to prevent CUDA OOM issues\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)  # Prevents full allocation\n",
    "        print(\"âœ… GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error setting memory growth: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693914d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce batch size from 32 to 16 or lower\n",
    "batch_size = 16  \n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    Masking(mask_value=0.0, input_shape=(seq_length, num_features)),  # Ignore padded values\n",
    "    LSTM(64, return_sequences=True, activation=\"tanh\"),  # Reduce from 128 to 64\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, return_sequences=False, activation=\"tanh\"),  # Reduce from 64 to 32\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation=\"relu\"),\n",
    "    Dense(1)  # Predict next speed value\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001), metrics=[\"mae\"])\n",
    "\n",
    "# Show model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d5e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
