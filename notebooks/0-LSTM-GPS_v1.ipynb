{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db68106",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "Plan is to model a horse’s per-race sequence of positions/velocities/accelerations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d976bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "os.environ.pop(\"CUDA_VISIBLE_DEVICES\", None)\n",
    "\n",
    "# Setup Environment\n",
    "import time\n",
    "from optuna.importance import MeanDecreaseImpurityImportanceEvaluator\n",
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import optuna.visualization as viz\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import (col, count, row_number, abs, unix_timestamp, mean, \n",
    "                                   when, lit, min as F_min, max as F_max , upper, trim,\n",
    "                                   row_number, mean as F_mean, countDistinct, last, first, when)\n",
    "from src.data_preprocessing.data_prep1.data_utils import initialize_environment\n",
    "from src.data_preprocessing.data_prep1.data_loader import load_data_from_postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark, jdbc_url, jdbc_properties, parquet_dir, log_file = initialize_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c779b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gps_sql_queries():\n",
    "    queries = {\n",
    "        \"gps_horse\": \"\"\"\n",
    "            SELECT g.course_cd, g.race_date,g.race_number,\n",
    "            REGEXP_REPLACE(TRIM(UPPER(saddle_cloth_number)), '\\s+$', '') AS saddle_cloth_number, time_stamp, \n",
    "            longitude, latitude, speed, progress, stride_frequency, g.post_time, location,\n",
    "            re.axciskey, h.horse_id, re.official_fin, h.horse_name\n",
    "            FROM gpspoint g\n",
    "            JOIN results_entries re on g.course_cd = re.course_cd\n",
    "                AND g.race_date = re.race_date\n",
    "                AND g.race_number = re.race_number\n",
    "                AND g.saddle_cloth_number = re.program_num\n",
    "            JOIN horse h on re.axciskey = h.axciskey\n",
    "            WHERE speed is not null\n",
    "            AND progress is not null\n",
    "            AND stride_frequency is not null\n",
    "            \"\"\",\n",
    "        \"sectionals\": \"\"\"\n",
    "            SELECT s.course_cd, s.race_date, s.race_number, \n",
    "            REGEXP_REPLACE(TRIM(UPPER(saddle_cloth_number)), '\\s+$', '') AS saddle_cloth_number, \n",
    "            s.gate_name, s.gate_numeric, s.length_to_finish, s.sectional_time, s.running_time, \n",
    "            s.distance_back, s.distance_ran, s.number_of_strides, s.post_time, re.official_fin\n",
    "            FROM sectionals s\n",
    "            JOIN results_entries re on s.course_cd = re.course_cd\n",
    "                AND s.race_date = re.race_date\n",
    "                AND s.race_number = re.race_number\n",
    "                AND s.saddle_cloth_number = re.program_num\n",
    "            JOIN horse h on re.axciskey = h.axciskey \n",
    "            WHERE length_to_finish is not null\n",
    "            AND sectional_time is not null\n",
    "            AND running_time is not null\n",
    "            AND distance_back is not null\n",
    "            AND distance_ran is not null\n",
    "            AND distance_ran is not null\n",
    "            \"\"\"\n",
    "    }\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d1447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = gps_sql_queries()\n",
    "dfs = load_data_from_postgresql(spark, jdbc_url, jdbc_properties, queries, parquet_dir)\n",
    "        # Suppose we have a dictionary of queries\n",
    "for name, df in dfs.items():\n",
    "    logging.info(f\"DataFrame '{name}' loaded. Schema:\")\n",
    "    df.printSchema()\n",
    "    if name == \"gps_horse\":\n",
    "        gps_horse_df = df\n",
    "    elif name == \"sectionals\":\n",
    "        sectionals_df = df    \n",
    "    else:\n",
    "        logging.error(f\"Unknown DataFrame name: {name}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d54ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "gps_horse_df.write.mode(\"overwrite\").parquet(f\"{parquet_dir}/gps_horse_df\")\n",
    "sectionals_df.write.mode(\"overwrite\").parquet(f\"{parquet_dir}/sectionals_df\")\n",
    "logging.info(f\"Data written to Parquet in {time.time() - start_time:.2f} seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, lpad, date_format\n",
    "\n",
    "gps_horse_df = gps_horse_df.withColumn(\n",
    "    \"race_id\",\n",
    "    concat_ws(\n",
    "        \"_\",\n",
    "        col(\"course_cd\"),\n",
    "        date_format(col(\"race_date\"), \"yyyyMMdd\"),\n",
    "        lpad(col(\"race_number\").cast(\"string\"), 2, \"0\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2e36d",
   "metadata": {},
   "source": [
    "## Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff39033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, to_date\n",
    "\n",
    "# Ensure race_date is in proper date format\n",
    "gps_horse_df = gps_horse_df.withColumn(\"race_date\", to_date(col(\"race_date\")))\n",
    "\n",
    "# Define a Window partitioned by horse_id, ordered by race_date and post_time\n",
    "window_spec = Window.partitionBy(\"horse_id\").orderBy(col(\"race_date\"), col(\"post_time\"))\n",
    "\n",
    "# Assign a row number to ensure strict chronological order\n",
    "gps_horse_df = gps_horse_df.withColumn(\"race_ordinal\", row_number().over(window_spec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea62df",
   "metadata": {},
   "source": [
    "## Padding Sequences to a Fixed Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31f4d5",
   "metadata": {},
   "source": [
    "###  Step 1: Select Relevant Columns & Order by Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7950f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [\"speed\", \"progress\", \"stride_frequency\"]  # Add or adjust as needed\n",
    "\n",
    "# Filter nulls (optional, depending on your quality)\n",
    "gps_horse_df_filtered = gps_horse_df.dropna(subset=features + [\"race_ordinal\", \"horse_id\"])\n",
    "\n",
    "# Group and collect sequences\n",
    "from pyspark.sql.functions import collect_list, struct\n",
    "\n",
    "sequence_df = gps_horse_df_filtered \\\n",
    "    .select(\"horse_id\", \"race_ordinal\", *features) \\\n",
    "    .orderBy(\"horse_id\", \"race_ordinal\") \\\n",
    "    .withColumn(\"features\", struct(*[col(c) for c in features])) \\\n",
    "    .groupBy(\"horse_id\") \\\n",
    "    .agg(collect_list(\"features\").alias(\"sequence\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d1f22f",
   "metadata": {},
   "source": [
    "###  PySpark Code to Count GPS Points per Horse per Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9528e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Compute sequence lengths per horse per race\n",
    "df_seq_len = gps_horse_df.groupBy(\"race_id\", \"horse_id\").agg(F.count(\"*\").alias(\"seq_length\"))\n",
    "\n",
    "# Show distribution statistics to validate\n",
    "df_seq_len.describe().show()\n",
    "\n",
    "# Step 2: Define length buckets\n",
    "df_buckets = df_seq_len.withColumn(\n",
    "    \"bucket\",\n",
    "    F.when(F.col(\"seq_length\") <= 100, \"short\")\n",
    "     .when(F.col(\"seq_length\") <= 150, \"medium\")\n",
    "     .otherwise(\"long\")\n",
    ")\n",
    "\n",
    "# Step 3: Join back with original data\n",
    "df_binned = gps_horse_df.join(df_buckets, [\"race_id\", \"horse_id\"])\n",
    "\n",
    "# Show sample results\n",
    "df_binned.select(\"race_id\", \"horse_id\", \"seq_length\", \"bucket\").show(10, truncate=False)\n",
    "\n",
    "# Step 4: Get overall distribution of bucket counts\n",
    "df_binned.groupBy(\"bucket\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9b482",
   "metadata": {},
   "source": [
    "###  Filter by Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f6b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df = df_binned.filter(F.col(\"bucket\") == \"short\")\n",
    "medium_df = df_binned.filter(F.col(\"bucket\") == \"medium\")\n",
    "long_df = df_binned.filter(F.col(\"bucket\") == \"long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd773956",
   "metadata": {},
   "source": [
    "### Convert to Sequence Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct, collect_list\n",
    "\n",
    "def make_sequences(df, features):\n",
    "    return df.select(\"race_id\", \"horse_id\", \"race_ordinal\", *features) \\\n",
    "        .orderBy(\"race_id\", \"horse_id\", \"race_ordinal\") \\\n",
    "        .withColumn(\"features\", struct(*[col(f) for f in features])) \\\n",
    "        .groupBy(\"race_id\", \"horse_id\") \\\n",
    "        .agg(collect_list(\"features\").alias(\"sequence\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcfedb6",
   "metadata": {},
   "source": [
    "#### Apply this to each bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2b755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"speed\", \"progress\", \"stride_frequency\"]\n",
    "short_seq = make_sequences(short_df, features)\n",
    "medium_seq = make_sequences(medium_df, features)\n",
    "long_seq = make_sequences(long_df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a17837",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_seq.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36140c0f",
   "metadata": {},
   "source": [
    "#### Convert to Pandas + Pad\n",
    "\n",
    "Convert to Pandas and pad each list to a fixed length:\n",
    "\n",
    "\t•\tshort → pad to 100\n",
    "\t•\tmedium → pad to 150\n",
    "\t•\tlong → pad to max (or truncate at 200–250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e201e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in official_fin per (race_id, horse_id)\n",
    "short_seq = short_seq.join(\n",
    "    short_df.select(\"race_id\", \"horse_id\", \"official_fin\").dropDuplicates([\"race_id\", \"horse_id\"]),\n",
    "    on=[\"race_id\", \"horse_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "medium_seq = medium_seq.join(\n",
    "    medium_df.select(\"race_id\", \"horse_id\", \"official_fin\").dropDuplicates([\"race_id\", \"horse_id\"]),\n",
    "    on=[\"race_id\", \"horse_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "long_seq = long_seq.join(\n",
    "    long_df.select(\"race_id\", \"horse_id\", \"official_fin\").dropDuplicates([\"race_id\", \"horse_id\"]),\n",
    "    on=[\"race_id\", \"horse_id\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecff4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(seq, target_len):\n",
    "    padded = seq[:target_len]\n",
    "    pad_len = target_len - len(padded)\n",
    "    pad_val = {\"speed\": 0.0, \"progress\": 0.0, \"stride_frequency\": 0.0}\n",
    "    return padded + [pad_val] * pad_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b206084",
   "metadata": {},
   "source": [
    "And turn into NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a832515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_pd = short_seq.toPandas()\n",
    "short_pd[\"padded_seq\"] = short_pd[\"sequence\"].apply(lambda x: pad_sequence(x, 100))\n",
    "X_short = np.array([[ [d[\"speed\"], d[\"progress\"], d[\"stride_frequency\"]] for d in seq] for seq in short_pd[\"padded_seq\"]])\n",
    "\n",
    "medium_pd = medium_seq.toPandas()\n",
    "medium_pd[\"padded_seq\"] = medium_pd[\"sequence\"].apply(lambda x: pad_sequence(x, 100))\n",
    "X_medium = np.array([[ [d[\"speed\"], d[\"progress\"], d[\"stride_frequency\"]] for d in seq] for seq in medium_pd[\"padded_seq\"]])\n",
    "\n",
    "long_pd = long_seq.toPandas()\n",
    "long_pd[\"padded_seq\"] = long_pd[\"sequence\"].apply(lambda x: pad_sequence(x, 100))\n",
    "X_long = np.array([[ [d[\"speed\"], d[\"progress\"], d[\"stride_frequency\"]] for d in seq] for seq in long_pd[\"padded_seq\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25ecbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_pd.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f640df92",
   "metadata": {},
   "source": [
    "✅ Shape Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b959fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_short.shape)   # (num_samples, 100, 3)\n",
    "print(X_medium.shape)  # (num_samples, 100, 3)\n",
    "print(X_long.shape)    # (num_samples, 100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aafc88d",
   "metadata": {},
   "source": [
    "### 🧠 Step-by-Step: Create PyTorch Dataset and DataLoader from X_short, X_medium, and X_long\n",
    "\n",
    "Assuming I’ll eventually associate each sequence with a label (e.g., official_finish, win, etc.), the Dataset class will accept both X and y.\n",
    "\n",
    "⸻\n",
    "\n",
    "✅ Step 1: Install PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfaf9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d35eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HorseRaceDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32) if y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        else:\n",
    "            return self.X[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d7d65a",
   "metadata": {},
   "source": [
    "✅ Step 3: Prepare Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec84e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_short = short_pd[\"official_fin\"].apply(lambda x: 1 if x == 1 else 0).values  # Example: binary winner\n",
    "y_medium = medium_pd[\"official_fin\"].apply(lambda x: 1 if x == 1 else 0).values\n",
    "y_long = long_pd[\"official_fin\"].apply(lambda x: 1 if x == 1 else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0316ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_short = X_short.astype(np.float32)\n",
    "X_medium = X_medium.astype(np.float32)\n",
    "X_long = X_long.astype(np.float32)\n",
    "\n",
    "y_short = y_short.astype(np.float32)\n",
    "y_medium = y_medium.astype(np.float32)\n",
    "y_long = y_long.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b5a7f2",
   "metadata": {},
   "source": [
    "### Normalize in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(tensor_3d, feature_idx):\n",
    "    feature = tensor_3d[:, :, feature_idx]\n",
    "    mean = np.mean(feature)\n",
    "    std = np.std(feature)\n",
    "    tensor_3d[:, :, feature_idx] = (feature - mean) / (std + 1e-8)\n",
    "    return tensor_3d\n",
    "\n",
    "for i in range(3):  # 3 features\n",
    "    X_short = normalize_feature(X_short, i)\n",
    "    X_medium = normalize_feature(X_medium, i)\n",
    "    X_long = normalize_feature(X_long, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee2592",
   "metadata": {},
   "source": [
    "✅ Step 4: Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0ae3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_short = HorseRaceDataset(X_short.astype(np.float32), y_short)\n",
    "train_dataset_medium = HorseRaceDataset(X_medium.astype(np.float32), y_medium)\n",
    "train_dataset_long = HorseRaceDataset(X_long.astype(np.float32), y_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e442dd3e",
   "metadata": {},
   "source": [
    "✅ Step 5: Wrap in DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a51948",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_short = DataLoader(train_dataset_short, batch_size=64, shuffle=True, pin_memory=True)\n",
    "train_loader_medium = DataLoader(train_dataset_medium, batch_size=64, shuffle=True, pin_memory=True)\n",
    "train_loader_long = DataLoader(train_dataset_long, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4759366",
   "metadata": {},
   "source": [
    "✅ Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_X, batch_y in train_loader_long:\n",
    "    print(\"X batch shape:\", batch_X.shape)  # torch.Size([64, 100, 3])\n",
    "    print(\"y batch shape:\", batch_y.shape)  # torch.Size([64])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_short.dtype, X_short.shape)  # float32 (164445, 100, 3)\n",
    "print(y_short.dtype, y_short.shape)  # float32 (164445,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_short = np.clip(X_short, -10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b5362a",
   "metadata": {},
   "source": [
    "# 🧠 Basic LSTM Model for Sequence Classification\n",
    "\n",
    "This model assumes:\n",
    "\n",
    "\t•\tInput shape: (batch_size, sequence_length=100, num_features=3)\n",
    "\t•\tOutput: binary classification (e.g. win = 1, not win = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df7dd1",
   "metadata": {},
   "source": [
    "### ✅ HorseRaceLSTM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HorseRaceLSTM(nn.Module):\n",
    "    def __init__(self, input_size=3, hidden_size=64, num_layers=1, dropout=0.2):\n",
    "        super(HorseRaceLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # no sigmoid!\n",
    "        return out.squeeze(1)         # return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0222bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"Device name 0:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df6bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d41b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = HorseRaceLSTM(input_size=3, hidden_size=64, num_layers=2)\n",
    "model.to(device)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b147ef",
   "metadata": {},
   "source": [
    "### 🧪 Mini Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215190e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Post-normalization check:\")\n",
    "print(\"Min:\", X_short.min(), \"Max:\", X_short.max())\n",
    "print(\"Any NaNs?\", np.isnan(X_short).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c80a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        print(\"X_batch device:\", X_batch.device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad = param.grad\n",
    "                print(f\"{name}: mean={grad.mean().item():.6f}, std={grad.std().item():.6f}\")\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        assert y_batch.min() >= 0.0 and y_batch.max() <= 1.0\n",
    "\n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5157510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pynvml matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85a46cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    loss = train_one_epoch(model, train_loader_short, optimizer, loss_fn, device)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30449154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, dataloader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > threshold).float()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2444deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, train_loader_short, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13363a86",
   "metadata": {},
   "source": [
    "Awesome — your clarity here opens up some very strategic use cases. Let’s align your LSTM with your goal: predicting relative horse performance (pre-race) and ranking horses competitively, especially in the top 3–4.\n",
    "\n",
    "⸻\n",
    "\n",
    "🧠 Your Current Insight:\n",
    "\n",
    "You already nailed it:\n",
    "\n",
    "“One option is to get scores and put them in as features for my CatBoost YetiRank model.”\n",
    "\n",
    "That’s an excellent direction. But here are more ways you can apply LSTM effectively in this race prediction pipeline — all using pre-race data.\n",
    "\n",
    "⸻\n",
    "\n",
    "🎯 Option 1: Use LSTM as a “Speed Projection Score Generator”\n",
    "\n",
    "What it does:\n",
    "\t•\tFeed in each horse’s last N races (as sequences: speed, stride freq, progress, surface, etc.)\n",
    "\t•\tModel outputs a projected performance score for today’s race.\n",
    "\n",
    "How to use it:\n",
    "\t•\tRank all horses in the race based on this projected score.\n",
    "\t•\tFeed this score into:\n",
    "\t•\tA CatBoost ranking model\n",
    "\t•\tA stacked ensemble model\n",
    "\t•\tOr use it standalone as a “FoxSpeedScore”\n",
    "\n",
    "⸻\n",
    "\n",
    "🧠 Option 2: Train LSTM to Rank Instead of Classify\n",
    "\n",
    "How:\n",
    "\n",
    "Train your LSTM to output a continuous score (like “expected finish time,” or normalized “rank” from 0 to 1).\n",
    "\n",
    "Then:\n",
    "\t•\tSort horses in a race by this score\n",
    "\t•\tTake the top 3–4 as your predicted finish order\n",
    "\n",
    "You can also:\n",
    "\t•\tUse pairwise ranking loss (like in YetiRank or RankNet)\n",
    "\t•\tOr assign labels like:\n",
    "\n",
    "label = 1.0 if finish_position == 1 else\n",
    "        0.75 if finish_position == 2 else\n",
    "        0.5 if finish_position == 3 else\n",
    "        0.25 if finish_position == 4 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "🔄 Option 3: LSTM as Embedding Generator (for Meta Models)\n",
    "\n",
    "Train the LSTM to output a vector embedding for each horse’s recent form (from sequences of past races).\n",
    "\n",
    "Then:\n",
    "\t•\tConcatenate that embedding with other race-day features (track, surface, odds, trainer stats)\n",
    "\t•\tFeed it into your CatBoost or final model\n",
    "\n",
    "This gives your meta-model temporal awareness — something tree-based models lack natively.\n",
    "\n",
    "⸻\n",
    "\n",
    "🔍 Option 4: Predict “Speed Projection Curve”\n",
    "\n",
    "Instead of just a win probability, predict the horse’s expected:\n",
    "\t•\tEarly pace (first call)\n",
    "\t•\tMid-race pace (second call)\n",
    "\t•\tFinal pace (finish)\n",
    "\n",
    "From pre-race data, this gives you a 3-point projected speed profile.\n",
    "\n",
    "Feed those into your betting strategy or exotic models (like for exactas, trifectas).\n",
    "\n",
    "⸻\n",
    "\n",
    "🔧 Option 5: Sequence-based Feature Augmentation\n",
    "\n",
    "You can extract LSTM-informed features like:\n",
    "\t•\t“Last-5 Race Speed Trend” (up or down)\n",
    "\t•\t“Form Stability Score” (std of last speeds)\n",
    "\t•\t“Fatigue Recovery” (from stride frequency drop-off)\n",
    "\n",
    "Use those as handcrafted features in your CatBoost or XGBoost ensemble.\n",
    "\n",
    "⸻\n",
    "\n",
    "🧠 Recap: How to Use LSTM in Your Pipeline\n",
    "\n",
    "Use Case\tLSTM Role\tHow to Use It\n",
    "Score projection\tPredict a win potential score\tFeed into CatBoost ranking or rank directly\n",
    "Ranking (not classification)\tPredict relative order or rank\tTrain with ordinal or continuous targets\n",
    "Feature augmentation\tTemporal-aware feature creator\tFeed to final model as extra inputs\n",
    "Embedding for fusion model\tSequence-to-vector encoder\tConcatenate with static features\n",
    "Sectional prediction\tProject in-race speed curve\tHelp strategy for exotic bets\n",
    "\n",
    "\n",
    "\n",
    "⸻\n",
    "\n",
    "Want help restructuring your LSTM to output rank scores, or building embeddings to plug into CatBoost? Or shall we start designing the next phase of the ensemble architecture?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d6f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/home/exx/myCode/horse-racing/FoxRiverAIRacing/data/models/LSTM/horse_lstm_short.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dead31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HorseRaceLSTM(input_size=3, hidden_size=64, num_layers=2)\n",
    "model.load_state_dict(torch.load(\"horse_lstm_short.pt\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
